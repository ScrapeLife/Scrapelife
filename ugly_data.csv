,target,readme
0,JavaScript,"




freeCodeCamp.org's open-source codebase and curriculum
freeCodeCamp.org is a friendly community where you can learn to code for free. It is run by a donor-supported 501(c)(3) nonprofit to help  millions of busy adults transition into tech. Our community has already helped more than 10,000 people get their first developer job.
Our full-stack web development curriculum is completely free and self-paced. We have thousands of interactive coding challenges to help you expand your skills.
Table of Contents

Certifications
The Learning Platform
Reporting Bugs and Issues
Reporting Security Issues
Contributing
Platform, Build and Deployment Status
License

Certifications
freeCodeCamp.org offers several free developer certifications. Each of these certifications involves building 5 required web app projects, along with hundreds of optional coding challenges to help you prepare for those projects. We estimate that each certification will take a beginning programmer around 300 hours to earn.
Each of these 30 projects in the freeCodeCamp.org curriculum has its own agile user stories and automated tests. These help you build up your project incrementally and ensure you've fulfilled all the user stories before you submit it.
You can pull in these test suites through freeCodeCamp's CDN. This means you can build these projects on websites like CodePen and Glitch - or even on your local computer's development environment.
Once you’ve earned a certification, you will always have it. You will always be able to link to it from your LinkedIn or résumé. And when your prospective employers or freelance clients click that link, they’ll see a verified certification specific to you.
The one exception to this is if we discover violations of our Academic Honesty Policy. When we catch people unambiguously plagiarizing (submitting other people's code or projects as their own without citation), we do what all rigorous institutions of learning should do - we revoke their certifications and ban those people.
Here are our six core certifications:
1. Responsive Web Design Certification

Basic HTML and HTML5
Basic CSS
Applied Visual Design
Applied Accessibility
Responsive Web Design Principles
CSS Flexbox
CSS Grid


Projects: Tribute Page, Survey Form, Product Landing Page, Technical Documentation Page, Personal Portfolio Webpage

2. JavaScript Algorithms and Data Structures Certification

Basic JavaScript
ES6
Regular Expressions
Debugging
Basic Data Structures
Algorithm Scripting
Object-Oriented Programming
Functional Programming
Intermediate Algorithm Scripting


Projects: Palindrome Checker, Roman Numeral Converter, Caesar's Cipher, Telephone Number Validator, Cash Register

3. Front End Libraries Certification

Bootstrap
jQuery
Sass
React
Redux
React and Redux


Projects: Random Quote Machine, Markdown Previewer, Drum Machine, JavaScript Calculator, Pomodoro Clock

4. Data Visualization Certification

Data Visualization with D3
JSON APIs and Ajax


Projects: Bar Chart, Scatterplot Graph, Heat Map, Choropleth Map, Treemap Diagram

5. APIs and Microservices Certification

Managing Packages with Npm
Basic Node and Express
MongoDB and Mongoose


Projects: Timestamp Microservice, Request Header Parser, URL Shortener, Exercise Tracker, File Metadata Microservice

6. Information Security and Quality Assurance Certification

Information Security with HelmetJS
Quality Assurance and Testing with Chai
Advanced Node and Express


Projects: Metric-Imperial Converter, Issue Tracker, Personal Library, Stock Price Checker, Anonymous Message Board

Full Stack Development Certification
Once you have earned all 6 of these certifications, you'll be able to claim your freeCodeCamp.org Full Stack Development Certification. This final distinction signifies that you’ve completed around 1,800 hours of coding with a wide range of web development tools.
Legacy Certifications
We also have 3 legacy certifications from our 2015 curriculum, which are still available. All of the required projects for these legacy certifications will remain available on freeCodeCamp.org.

Legacy Front End Development Certification
Legacy Data Visualization Certification
Legacy Back End Development Certification

The Learning Platform
This code is running live at freeCodeCamp.org.
Our community also has:

A forum where you can usually get programming help or project feedback within hours.
A YouTube channel with free courses on Python, SQL, Android, and a wide variety of other technologies.
A podcast with technology insights and inspiring stories from developers.
A Developer News publication, a free, open source, no-ads place to cross-post your blog articles.


Join our community here.

Reporting Bugs and Issues
If you think you've found a bug, first read the how to report a bug article and follow its instructions.
If you're confident it's a new bug and have confirmed that someone else is facing the same issue, go ahead and create a new GitHub issue. Be sure to include as much information as possible so we can reproduce the bug.
Reporting Security Issues
If you think you have found a vulnerability, please report responsibly. Don't create GitHub issues for security issues. Instead, please send an email to security@freecodecamp.org and we'll look into it immediately.
Contributing

Please follow these steps to contribute.

Platform, Build and Deployment Status
The general platform status for all our applications is available at status.freecodecamp.org. The build and deployment status for the code is available in our DevOps Guide.
License
Copyright © 2019 freeCodeCamp.org
The content of this repository is bound by the following licenses:

The computer software is licensed under the BSD-3-Clause license.
The learning resources in the /curriculum directory including their subdirectories thereon are licensed under the CC-BY-SA-4.0 license.

"
1,JavaScript,"










Supporting Vue.js
Vue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome backers. If you'd like to join them, please consider:

Become a backer or sponsor on Patreon.
Become a backer or sponsor on Open Collective.
One-time donation via PayPal or crypto-currencies.

What's the difference between Patreon and OpenCollective?
Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.
Special Sponsors





Platinum Sponsors


























Platinum Sponsors (China)











Gold Sponsors




































































































































































Sponsors via Open Collective
Platinum


Gold






Introduction
Vue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.
Browser Compatibility
Vue.js supports all browsers that are ES5-compliant (IE8 and below are not supported).
Ecosystem



Project
Status
Description




vue-router

Single-page application routing


vuex

Large-scale state management


vue-cli

Project scaffolding


vue-loader

Single File Component (*.vue file) loader for webpack


vue-server-renderer

Server-side rendering support


vue-class-component

TypeScript decorator for a class-based API


vue-rx

RxJS integration


vue-devtools

Browser DevTools extension



Documentation
To check out live examples and docs, visit vuejs.org.
Questions
For questions and support please use the official forum or community chat. The issue list of this repo is exclusively for bug reports and feature requests.
Issues
Please make sure to read the Issue Reporting Checklist before opening an issue. Issues not conforming to the guidelines may be closed immediately.
Changelog
Detailed changes for each release are documented in the release notes.
Stay In Touch

Twitter
Blog
Job Board

Contribution
Please make sure to read the Contributing Guide before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to this curated list!
Thank you to all the people who already contributed to Vue!

License
MIT
Copyright (c) 2013-present, Yuxi (Evan) You
"
2,JavaScript,"React ·    
React is a JavaScript library for building user interfaces.

Declarative: React makes it painless to create interactive UIs. Design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes. Declarative views make your code more predictable, simpler to understand, and easier to debug.
Component-Based: Build encapsulated components that manage their own state, then compose them to make complex UIs. Since component logic is written in JavaScript instead of templates, you can easily pass rich data through your app and keep state out of the DOM.
Learn Once, Write Anywhere: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code. React can also render on the server using Node and power mobile apps using React Native.

Learn how to use React in your own project.
Installation
React has been designed for gradual adoption from the start, and you can use as little or as much React as you need:

Use Online Playgrounds to get a taste of React.
Add React to a Website as a <script> tag in one minute.
Create a New React App if you're looking for a powerful JavaScript toolchain.

You can use React as a <script> tag from a CDN, or as a react package on npm.
Documentation
You can find the React documentation on the website.
Check out the Getting Started page for a quick overview.
The documentation is divided into several sections:

Tutorial
Main Concepts
Advanced Guides
API Reference
Where to Get Support
Contributing Guide

You can improve it by sending pull requests to this repository.
Examples
We have several examples on the website. Here is the first one to get you started:
function HelloMessage({ name }) {
  return <div>Hello {name}</div>;
}

ReactDOM.render(
  <HelloMessage name=""Taylor"" />,
  document.getElementById('container')
);
This example will render ""Hello Taylor"" into a container on the page.
You'll notice that we used an HTML-like syntax; we call it JSX. JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a <script> tag, read this section on integrating JSX; otherwise, the recommended JavaScript toolchains handle it automatically.
Contributing
The main purpose of this repository is to continue to evolve React core, making it faster and easier to use. Development of React happens in the open on GitHub, and we are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can take part in improving React.
Code of Conduct
Facebook has adopted a Code of Conduct that we expect project participants to adhere to. Please read the full text so that you can understand what actions will and will not be tolerated.
Contributing Guide
Read our contributing guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React.
Good First Issues
To help you get your feet wet and get you familiar with our contribution process, we have a list of good first issues that contain bugs which have a relatively limited scope. This is a great place to get started.
License
React is MIT licensed.
"
3,JavaScript,"




Bootstrap

  Sleek, intuitive, and powerful front-end framework for faster and easier web development.
  
Explore Bootstrap docs »


Report bug
  ·
  Request feature
  ·
  Themes
  ·
  Blog

Table of contents

Quick start
Status
What's included
Bugs and feature requests
Documentation
Contributing
Community
Versioning
Creators
Thanks
Copyright and license

Quick start
Several quick start options are available:

Download the latest release.
Clone the repo: git clone https://github.com/twbs/bootstrap.git
Install with npm: npm install bootstrap
Install with yarn: yarn add bootstrap@4.3.1
Install with Composer: composer require twbs/bootstrap:4.3.1
Install with NuGet: CSS: Install-Package bootstrap Sass: Install-Package bootstrap.sass

Read the Getting started page for information on the framework contents, templates and examples, and more.
Status














What's included
Within the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:
bootstrap/
└── dist/
    ├── css/
    │   ├── bootstrap-grid.css
    │   ├── bootstrap-grid.css.map
    │   ├── bootstrap-grid.min.css
    │   ├── bootstrap-grid.min.css.map
    │   ├── bootstrap-reboot.css
    │   ├── bootstrap-reboot.css.map
    │   ├── bootstrap-reboot.min.css
    │   ├── bootstrap-reboot.min.css.map
    │   ├── bootstrap-utilities.css
    │   ├── bootstrap-utilities.css.map
    │   ├── bootstrap-utilities.min.css
    │   ├── bootstrap-utilities.min.css.map
    │   ├── bootstrap.css
    │   ├── bootstrap.css.map
    │   ├── bootstrap.min.css
    │   └── bootstrap.min.css.map
    └── js/
        ├── bootstrap.bundle.js
        ├── bootstrap.bundle.js.map
        ├── bootstrap.bundle.min.js
        ├── bootstrap.bundle.min.js.map
        ├── bootstrap.esm.js
        ├── bootstrap.esm.js.map
        ├── bootstrap.esm.min.js
        ├── bootstrap.esm.min.js.map
        ├── bootstrap.js
        ├── bootstrap.js.map
        ├── bootstrap.min.js
        └── bootstrap.min.js.map

We provide compiled CSS and JS (bootstrap.*), as well as compiled and minified CSS and JS (bootstrap.min.*). source maps (bootstrap.*.map) are available for use with certain browsers' developer tools. Bundled JS files (bootstrap.bundle.js and minified bootstrap.bundle.min.js) include Popper.
Bugs and feature requests
Have a bug or a feature request? Please first read the issue guidelines and search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue.
Documentation
Bootstrap's documentation, included in this repo in the root directory, is built with Hugo and publicly hosted on GitHub Pages at https://getbootstrap.com/. The docs may also be run locally.
Documentation search is powered by Algolia's DocSearch. Working on our search? Be sure to set debug: true in site/assets/js/src/search.js file.
Running documentation locally

Run npm install to install the Node.js dependencies, including Hugo (the site builder).
Run npm run test (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.
From the root /bootstrap directory, run npm run docs-serve in the command line.
Open http://localhost:9001/ in your browser, and voilà.

Learn more about using Hugo by reading its documentation.
Documentation for previous releases
You can find all our previous releases docs on https://getbootstrap.com/docs/versions/.
Previous releases and their documentation are also available for download.
Contributing
Please read through our contributing guidelines. Included are directions for opening issues, coding standards, and notes on development.
Moreover, if your pull request contains JavaScript patches or features, you must include relevant unit tests. All HTML and CSS should conform to the Code Guide, maintained by Mark Otto.
Editor preferences are available in the editor config for easy use in common text editors. Read more and download plugins at https://editorconfig.org/.
Community
Get updates on Bootstrap's development and chat with the project maintainers and community members.

Follow @getbootstrap on Twitter.
Read and subscribe to The Official Bootstrap Blog.
Join the official Slack room.
Chat with fellow Bootstrappers in IRC. On the irc.freenode.net server, in the ##bootstrap channel.
Implementation help may be found at Stack Overflow (tagged bootstrap-4).
Developers should use the keyword bootstrap on packages which modify or add to the functionality of Bootstrap when distributing through npm or similar delivery mechanisms for maximum discoverability.

Versioning
For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under the Semantic Versioning guidelines. Sometimes we screw up, but we adhere to those rules whenever possible.
See the Releases section of our GitHub project for changelogs for each release version of Bootstrap. Release announcement posts on the official Bootstrap blog contain summaries of the most noteworthy changes made in each release.
Creators
Mark Otto

https://twitter.com/mdo
https://github.com/mdo

Jacob Thornton

https://twitter.com/fat
https://github.com/fat

Thanks



Thanks to BrowserStack for providing the infrastructure that allows us to test in real browsers!
Backers
Thank you to all our backers! 🙏 [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










Copyright and license
Code and documentation copyright 2011-2019 the Bootstrap Authors and Twitter, Inc. Code released under the MIT License. Docs released under Creative Commons.
"
4,JavaScript,"Airbnb JavaScript Style Guide() {
A mostly reasonable approach to JavaScript

Note: this guide assumes you are using Babel, and requires that you use babel-preset-airbnb or the equivalent. It also assumes you are installing shims/polyfills in your app, with airbnb-browser-shims or the equivalent.




This guide is available in other languages too. See Translation
Other Style Guides

ES5 (Deprecated)
React
CSS-in-JavaScript
CSS & Sass
Ruby

Table of Contents

Types
References
Objects
Arrays
Destructuring
Strings
Functions
Arrow Functions
Classes & Constructors
Modules
Iterators and Generators
Properties
Variables
Hoisting
Comparison Operators & Equality
Blocks
Control Statements
Comments
Whitespace
Commas
Semicolons
Type Casting & Coercion
Naming Conventions
Accessors
Events
jQuery
ECMAScript 5 Compatibility
ECMAScript 6+ (ES 2015+) Styles
Standard Library
Testing
Performance
Resources
In the Wild
Translation
The JavaScript Style Guide Guide
Chat With Us About JavaScript
Contributors
License
Amendments

Types



1.1 Primitives: When you access a primitive type you work directly on its value.

string
number
boolean
null
undefined
symbol

const foo = 1;
let bar = foo;

bar = 9;

console.log(foo, bar); // => 1, 9

Symbols cannot be faithfully polyfilled, so they should not be used when targeting browsers/environments that don’t support them natively.






1.2 Complex: When you access a complex type you work on a reference to its value.

object
array
function

const foo = [1, 2];
const bar = foo;

bar[0] = 9;

console.log(foo[0], bar[0]); // => 9, 9


⬆ back to top
References



2.1 Use const for all of your references; avoid using var. eslint: prefer-const, no-const-assign

Why? This ensures that you can’t reassign your references, which can lead to bugs and difficult to comprehend code.

// bad
var a = 1;
var b = 2;

// good
const a = 1;
const b = 2;





2.2 If you must reassign references, use let instead of var. eslint: no-var

Why? let is block-scoped rather than function-scoped like var.

// bad
var count = 1;
if (true) {
  count += 1;
}

// good, use the let.
let count = 1;
if (true) {
  count += 1;
}





2.3 Note that both let and const are block-scoped.
// const and let only exist in the blocks they are defined in.
{
  let a = 1;
  const b = 1;
}
console.log(a); // ReferenceError
console.log(b); // ReferenceError


⬆ back to top
Objects



3.1 Use the literal syntax for object creation. eslint: no-new-object
// bad
const item = new Object();

// good
const item = {};





3.2 Use computed property names when creating objects with dynamic property names.

Why? They allow you to define all the properties of an object in one place.

function getKey(k) {
  return `a key named ${k}`;
}

// bad
const obj = {
  id: 5,
  name: 'San Francisco',
};
obj[getKey('enabled')] = true;

// good
const obj = {
  id: 5,
  name: 'San Francisco',
  [getKey('enabled')]: true,
};





3.3 Use object method shorthand. eslint: object-shorthand
// bad
const atom = {
  value: 1,

  addValue: function (value) {
    return atom.value + value;
  },
};

// good
const atom = {
  value: 1,

  addValue(value) {
    return atom.value + value;
  },
};





3.4 Use property value shorthand. eslint: object-shorthand

Why? It is shorter and descriptive.

const lukeSkywalker = 'Luke Skywalker';

// bad
const obj = {
  lukeSkywalker: lukeSkywalker,
};

// good
const obj = {
  lukeSkywalker,
};





3.5 Group your shorthand properties at the beginning of your object declaration.

Why? It’s easier to tell which properties are using the shorthand.

const anakinSkywalker = 'Anakin Skywalker';
const lukeSkywalker = 'Luke Skywalker';

// bad
const obj = {
  episodeOne: 1,
  twoJediWalkIntoACantina: 2,
  lukeSkywalker,
  episodeThree: 3,
  mayTheFourth: 4,
  anakinSkywalker,
};

// good
const obj = {
  lukeSkywalker,
  anakinSkywalker,
  episodeOne: 1,
  twoJediWalkIntoACantina: 2,
  episodeThree: 3,
  mayTheFourth: 4,
};





3.6 Only quote properties that are invalid identifiers. eslint: quote-props

Why? In general we consider it subjectively easier to read. It improves syntax highlighting, and is also more easily optimized by many JS engines.

// bad
const bad = {
  'foo': 3,
  'bar': 4,
  'data-blah': 5,
};

// good
const good = {
  foo: 3,
  bar: 4,
  'data-blah': 5,
};





3.7 Do not call Object.prototype methods directly, such as hasOwnProperty, propertyIsEnumerable, and isPrototypeOf. eslint: no-prototype-builtins

Why? These methods may be shadowed by properties on the object in question - consider { hasOwnProperty: false } - or, the object may be a null object (Object.create(null)).

// bad
console.log(object.hasOwnProperty(key));

// good
console.log(Object.prototype.hasOwnProperty.call(object, key));

// best
const has = Object.prototype.hasOwnProperty; // cache the lookup once, in module scope.
console.log(has.call(object, key));
/* or */
import has from 'has'; // https://www.npmjs.com/package/has
console.log(has(object, key));





3.8 Prefer the object spread operator over Object.assign to shallow-copy objects. Use the object rest operator to get a new object with certain properties omitted.
// very bad
const original = { a: 1, b: 2 };
const copy = Object.assign(original, { c: 3 }); // this mutates `original` ಠ_ಠ
delete copy.a; // so does this

// bad
const original = { a: 1, b: 2 };
const copy = Object.assign({}, original, { c: 3 }); // copy => { a: 1, b: 2, c: 3 }

// good
const original = { a: 1, b: 2 };
const copy = { ...original, c: 3 }; // copy => { a: 1, b: 2, c: 3 }

const { a, ...noA } = copy; // noA => { b: 2, c: 3 }


⬆ back to top
Arrays



4.1 Use the literal syntax for array creation. eslint: no-array-constructor
// bad
const items = new Array();

// good
const items = [];





4.2 Use Array#push instead of direct assignment to add items to an array.
const someStack = [];

// bad
someStack[someStack.length] = 'abracadabra';

// good
someStack.push('abracadabra');





4.3 Use array spreads ... to copy arrays.
// bad
const len = items.length;
const itemsCopy = [];
let i;

for (i = 0; i < len; i += 1) {
  itemsCopy[i] = items[i];
}

// good
const itemsCopy = [...items];






4.4 To convert an iterable object to an array, use spreads ... instead of Array.from.
const foo = document.querySelectorAll('.foo');

// good
const nodes = Array.from(foo);

// best
const nodes = [...foo];





4.5 Use Array.from for converting an array-like object to an array.
const arrLike = { 0: 'foo', 1: 'bar', 2: 'baz', length: 3 };

// bad
const arr = Array.prototype.slice.call(arrLike);

// good
const arr = Array.from(arrLike);





4.6 Use Array.from instead of spread ... for mapping over iterables, because it avoids creating an intermediate array.
// bad
const baz = [...foo].map(bar);

// good
const baz = Array.from(foo, bar);





4.7 Use return statements in array method callbacks. It’s ok to omit the return if the function body consists of a single statement returning an expression without side effects, following 8.2. eslint: array-callback-return
// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => x + 1);

// bad - no returned value means `acc` becomes undefined after the first iteration
[[0, 1], [2, 3], [4, 5]].reduce((acc, item, index) => {
  const flatten = acc.concat(item);
});

// good
[[0, 1], [2, 3], [4, 5]].reduce((acc, item, index) => {
  const flatten = acc.concat(item);
  return flatten;
});

// bad
inbox.filter((msg) => {
  const { subject, author } = msg;
  if (subject === 'Mockingbird') {
    return author === 'Harper Lee';
  } else {
    return false;
  }
});

// good
inbox.filter((msg) => {
  const { subject, author } = msg;
  if (subject === 'Mockingbird') {
    return author === 'Harper Lee';
  }

  return false;
});





4.8 Use line breaks after open and before close array brackets if an array has multiple lines
// bad
const arr = [
  [0, 1], [2, 3], [4, 5],
];

const objectInArray = [{
  id: 1,
}, {
  id: 2,
}];

const numberInArray = [
  1, 2,
];

// good
const arr = [[0, 1], [2, 3], [4, 5]];

const objectInArray = [
  {
    id: 1,
  },
  {
    id: 2,
  },
];

const numberInArray = [
  1,
  2,
];


⬆ back to top
Destructuring



5.1 Use object destructuring when accessing and using multiple properties of an object. eslint: prefer-destructuring

Why? Destructuring saves you from creating temporary references for those properties.

// bad
function getFullName(user) {
  const firstName = user.firstName;
  const lastName = user.lastName;

  return `${firstName} ${lastName}`;
}

// good
function getFullName(user) {
  const { firstName, lastName } = user;
  return `${firstName} ${lastName}`;
}

// best
function getFullName({ firstName, lastName }) {
  return `${firstName} ${lastName}`;
}





5.2 Use array destructuring. eslint: prefer-destructuring
const arr = [1, 2, 3, 4];

// bad
const first = arr[0];
const second = arr[1];

// good
const [first, second] = arr;





5.3 Use object destructuring for multiple return values, not array destructuring.

Why? You can add new properties over time or change the order of things without breaking call sites.

// bad
function processInput(input) {
  // then a miracle occurs
  return [left, right, top, bottom];
}

// the caller needs to think about the order of return data
const [left, __, top] = processInput(input);

// good
function processInput(input) {
  // then a miracle occurs
  return { left, right, top, bottom };
}

// the caller selects only the data they need
const { left, top } = processInput(input);


⬆ back to top
Strings



6.1 Use single quotes '' for strings. eslint: quotes
// bad
const name = ""Capt. Janeway"";

// bad - template literals should contain interpolation or newlines
const name = `Capt. Janeway`;

// good
const name = 'Capt. Janeway';





6.2 Strings that cause the line to go over 100 characters should not be written across multiple lines using string concatenation.

Why? Broken strings are painful to work with and make code less searchable.

// bad
const errorMessage = 'This is a super long error that was thrown because \
of Batman. When you stop to think about how Batman had anything to do \
with this, you would get nowhere \
fast.';

// bad
const errorMessage = 'This is a super long error that was thrown because ' +
  'of Batman. When you stop to think about how Batman had anything to do ' +
  'with this, you would get nowhere fast.';

// good
const errorMessage = 'This is a super long error that was thrown because of Batman. When you stop to think about how Batman had anything to do with this, you would get nowhere fast.';





6.3 When programmatically building up strings, use template strings instead of concatenation. eslint: prefer-template template-curly-spacing

Why? Template strings give you a readable, concise syntax with proper newlines and string interpolation features.

// bad
function sayHi(name) {
  return 'How are you, ' + name + '?';
}

// bad
function sayHi(name) {
  return ['How are you, ', name, '?'].join();
}

// bad
function sayHi(name) {
  return `How are you, ${ name }?`;
}

// good
function sayHi(name) {
  return `How are you, ${name}?`;
}




6.4 Never use eval() on a string, it opens too many vulnerabilities. eslint: no-eval




6.5 Do not unnecessarily escape characters in strings. eslint: no-useless-escape

Why? Backslashes harm readability, thus they should only be present when necessary.

// bad
const foo = '\'this\' \i\s \""quoted\""';

// good
const foo = '\'this\' is ""quoted""';
const foo = `my name is '${name}'`;


⬆ back to top
Functions



7.1 Use named function expressions instead of function declarations. eslint: func-style

Why? Function declarations are hoisted, which means that it’s easy - too easy - to reference the function before it is defined in the file. This harms readability and maintainability. If you find that a function’s definition is large or complex enough that it is interfering with understanding the rest of the file, then perhaps it’s time to extract it to its own module! Don’t forget to explicitly name the expression, regardless of whether or not the name is inferred from the containing variable (which is often the case in modern browsers or when using compilers such as Babel). This eliminates any assumptions made about the Error’s call stack. (Discussion)

// bad
function foo() {
  // ...
}

// bad
const foo = function () {
  // ...
};

// good
// lexical name distinguished from the variable-referenced invocation(s)
const short = function longUniqueMoreDescriptiveLexicalFoo() {
  // ...
};





7.2 Wrap immediately invoked function expressions in parentheses. eslint: wrap-iife

Why? An immediately invoked function expression is a single unit - wrapping both it, and its invocation parens, in parens, cleanly expresses this. Note that in a world with modules everywhere, you almost never need an IIFE.

// immediately-invoked function expression (IIFE)
(function () {
  console.log('Welcome to the Internet. Please follow me.');
}());




7.3 Never declare a function in a non-function block (if, while, etc). Assign the function to a variable instead. Browsers will allow you to do it, but they all interpret it differently, which is bad news bears. eslint: no-loop-func




7.4 Note: ECMA-262 defines a block as a list of statements. A function declaration is not a statement.
// bad
if (currentUser) {
  function test() {
    console.log('Nope.');
  }
}

// good
let test;
if (currentUser) {
  test = () => {
    console.log('Yup.');
  };
}





7.5 Never name a parameter arguments. This will take precedence over the arguments object that is given to every function scope.
// bad
function foo(name, options, arguments) {
  // ...
}

// good
function foo(name, options, args) {
  // ...
}





7.6 Never use arguments, opt to use rest syntax ... instead. eslint: prefer-rest-params

Why? ... is explicit about which arguments you want pulled. Plus, rest arguments are a real Array, and not merely Array-like like arguments.

// bad
function concatenateAll() {
  const args = Array.prototype.slice.call(arguments);
  return args.join('');
}

// good
function concatenateAll(...args) {
  return args.join('');
}





7.7 Use default parameter syntax rather than mutating function arguments.
// really bad
function handleThings(opts) {
  // No! We shouldn’t mutate function arguments.
  // Double bad: if opts is falsy it'll be set to an object which may
  // be what you want but it can introduce subtle bugs.
  opts = opts || {};
  // ...
}

// still bad
function handleThings(opts) {
  if (opts === void 0) {
    opts = {};
  }
  // ...
}

// good
function handleThings(opts = {}) {
  // ...
}





7.8 Avoid side effects with default parameters.

Why? They are confusing to reason about.

var b = 1;
// bad
function count(a = b++) {
  console.log(a);
}
count();  // 1
count();  // 2
count(3); // 3
count();  // 3





7.9 Always put default parameters last.
// bad
function handleThings(opts = {}, name) {
  // ...
}

// good
function handleThings(name, opts = {}) {
  // ...
}





7.10 Never use the Function constructor to create a new function. eslint: no-new-func

Why? Creating a function in this way evaluates a string similarly to eval(), which opens vulnerabilities.

// bad
var add = new Function('a', 'b', 'return a + b');

// still bad
var subtract = Function('a', 'b', 'return a - b');





7.11 Spacing in a function signature. eslint: space-before-function-paren space-before-blocks

Why? Consistency is good, and you shouldn’t have to add or remove a space when adding or removing a name.

// bad
const f = function(){};
const g = function (){};
const h = function() {};

// good
const x = function () {};
const y = function a() {};





7.12 Never mutate parameters. eslint: no-param-reassign

Why? Manipulating objects passed in as parameters can cause unwanted variable side effects in the original caller.

// bad
function f1(obj) {
  obj.key = 1;
}

// good
function f2(obj) {
  const key = Object.prototype.hasOwnProperty.call(obj, 'key') ? obj.key : 1;
}





7.13 Never reassign parameters. eslint: no-param-reassign

Why? Reassigning parameters can lead to unexpected behavior, especially when accessing the arguments object. It can also cause optimization issues, especially in V8.

// bad
function f1(a) {
  a = 1;
  // ...
}

function f2(a) {
  if (!a) { a = 1; }
  // ...
}

// good
function f3(a) {
  const b = a || 1;
  // ...
}

function f4(a = 1) {
  // ...
}





7.14 Prefer the use of the spread operator ... to call variadic functions. eslint: prefer-spread

Why? It’s cleaner, you don’t need to supply a context, and you can not easily compose new with apply.

// bad
const x = [1, 2, 3, 4, 5];
console.log.apply(console, x);

// good
const x = [1, 2, 3, 4, 5];
console.log(...x);

// bad
new (Function.prototype.bind.apply(Date, [null, 2016, 8, 5]));

// good
new Date(...[2016, 8, 5]);





7.15 Functions with multiline signatures, or invocations, should be indented just like every other multiline list in this guide: with each item on a line by itself, with a trailing comma on the last item. eslint: function-paren-newline
// bad
function foo(bar,
             baz,
             quux) {
  // ...
}

// good
function foo(
  bar,
  baz,
  quux,
) {
  // ...
}

// bad
console.log(foo,
  bar,
  baz);

// good
console.log(
  foo,
  bar,
  baz,
);


⬆ back to top
Arrow Functions



8.1 When you must use an anonymous function (as when passing an inline callback), use arrow function notation. eslint: prefer-arrow-callback, arrow-spacing

Why? It creates a version of the function that executes in the context of this, which is usually what you want, and is a more concise syntax.


Why not? If you have a fairly complicated function, you might move that logic out into its own named function expression.

// bad
[1, 2, 3].map(function (x) {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});





8.2 If the function body consists of a single statement returning an expression without side effects, omit the braces and use the implicit return. Otherwise, keep the braces and use a return statement. eslint: arrow-parens, arrow-body-style

Why? Syntactic sugar. It reads well when multiple functions are chained together.

// bad
[1, 2, 3].map((number) => {
  const nextNumber = number + 1;
  `A string containing the ${nextNumber}.`;
});

// good
[1, 2, 3].map((number) => `A string containing the ${number + 1}.`);

// good
[1, 2, 3].map((number) => {
  const nextNumber = number + 1;
  return `A string containing the ${nextNumber}.`;
});

// good
[1, 2, 3].map((number, index) => ({
  [index]: number,
}));

// No implicit return with side effects
function foo(callback) {
  const val = callback();
  if (val === true) {
    // Do something if callback returns true
  }
}

let bool = false;

// bad
foo(() => bool = true);

// good
foo(() => {
  bool = true;
});





8.3 In case the expression spans over multiple lines, wrap it in parentheses for better readability.

Why? It shows clearly where the function starts and ends.

// bad
['get', 'post', 'put'].map((httpMethod) => Object.prototype.hasOwnProperty.call(
    httpMagicObjectWithAVeryLongName,
    httpMethod,
  )
);

// good
['get', 'post', 'put'].map((httpMethod) => (
  Object.prototype.hasOwnProperty.call(
    httpMagicObjectWithAVeryLongName,
    httpMethod,
  )
));





8.4 Always include parentheses around arguments for clarity and consistency. eslint: arrow-parens

Why? Minimizes diff churn when adding or removing arguments.

// bad
[1, 2, 3].map(x => x * x);

// good
[1, 2, 3].map((x) => x * x);

// bad
[1, 2, 3].map(number => (
  `A long string with the ${number}. It’s so long that we don’t want it to take up space on the .map line!`
));

// good
[1, 2, 3].map((number) => (
  `A long string with the ${number}. It’s so long that we don’t want it to take up space on the .map line!`
));

// bad
[1, 2, 3].map(x => {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});





8.5 Avoid confusing arrow function syntax (=>) with comparison operators (<=, >=). eslint: no-confusing-arrow
// bad
const itemHeight = (item) => item.height <= 256 ? item.largeSize : item.smallSize;

// bad
const itemHeight = (item) => item.height >= 256 ? item.largeSize : item.smallSize;

// good
const itemHeight = (item) => (item.height <= 256 ? item.largeSize : item.smallSize);

// good
const itemHeight = (item) => {
  const { height, largeSize, smallSize } = item;
  return height <= 256 ? largeSize : smallSize;
};





8.6 Enforce the location of arrow function bodies with implicit returns. eslint: implicit-arrow-linebreak
// bad
(foo) =>
  bar;

(foo) =>
  (bar);

// good
(foo) => bar;
(foo) => (bar);
(foo) => (
   bar
)


⬆ back to top
Classes & Constructors



9.1 Always use class. Avoid manipulating prototype directly.

Why? class syntax is more concise and easier to reason about.

// bad
function Queue(contents = []) {
  this.queue = [...contents];
}
Queue.prototype.pop = function () {
  const value = this.queue[0];
  this.queue.splice(0, 1);
  return value;
};

// good
class Queue {
  constructor(contents = []) {
    this.queue = [...contents];
  }
  pop() {
    const value = this.queue[0];
    this.queue.splice(0, 1);
    return value;
  }
}





9.2 Use extends for inheritance.

Why? It is a built-in way to inherit prototype functionality without breaking instanceof.

// bad
const inherits = require('inherits');
function PeekableQueue(contents) {
  Queue.apply(this, contents);
}
inherits(PeekableQueue, Queue);
PeekableQueue.prototype.peek = function () {
  return this.queue[0];
};

// good
class PeekableQueue extends Queue {
  peek() {
    return this.queue[0];
  }
}





9.3 Methods can return this to help with method chaining.
// bad
Jedi.prototype.jump = function () {
  this.jumping = true;
  return true;
};

Jedi.prototype.setHeight = function (height) {
  this.height = height;
};

const luke = new Jedi();
luke.jump(); // => true
luke.setHeight(20); // => undefined

// good
class Jedi {
  jump() {
    this.jumping = true;
    return this;
  }

  setHeight(height) {
    this.height = height;
    return this;
  }
}

const luke = new Jedi();

luke.jump()
  .setHeight(20);





9.4 It’s okay to write a custom toString() method, just make sure it works successfully and causes no side effects.
class Jedi {
  constructor(options = {}) {
    this.name = options.name || 'no name';
  }

  getName() {
    return this.name;
  }

  toString() {
    return `Jedi - ${this.getName()}`;
  }
}





9.5 Classes have a default constructor if one is not specified. An empty constructor function or one that just delegates to a parent class is unnecessary. eslint: no-useless-constructor
// bad
class Jedi {
  constructor() {}

  getName() {
    return this.name;
  }
}

// bad
class Rey extends Jedi {
  constructor(...args) {
    super(...args);
  }
}

// good
class Rey extends Jedi {
  constructor(...args) {
    super(...args);
    this.name = 'Rey';
  }
}





9.6 Avoid duplicate class members. eslint: no-dupe-class-members

Why? Duplicate class member declarations will silently prefer the last one - having duplicates is almost certainly a bug.

// bad
class Foo {
  bar() { return 1; }
  bar() { return 2; }
}

// good
class Foo {
  bar() { return 1; }
}

// good
class Foo {
  bar() { return 2; }
}





9.7 Class methods should use this or be made into a static method unless an external library or framework requires to use specific non-static methods. Being an instance method should indicate that it behaves differently based on properties of the receiver. eslint: class-methods-use-this
// bad
class Foo {
  bar() {
    console.log('bar');
  }
}

// good - this is used
class Foo {
  bar() {
    console.log(this.bar);
  }
}

// good - constructor is exempt
class Foo {
  constructor() {
    // ...
  }
}

// good - static methods aren't expected to use this
class Foo {
  static bar() {
    console.log('bar');
  }
}


⬆ back to top
Modules



10.1 Always use modules (import/export) over a non-standard module system. You can always transpile to your preferred module system.

Why? Modules are the future, let’s start using the future now.

// bad
const AirbnbStyleGuide = require('./AirbnbStyleGuide');
module.exports = AirbnbStyleGuide.es6;

// ok
import AirbnbStyleGuide from './AirbnbStyleGuide';
export default AirbnbStyleGuide.es6;

// best
import { es6 } from './AirbnbStyleGuide';
export default es6;





10.2 Do not use wildcard imports.

Why? This makes sure you have a single default export.

// bad
import * as AirbnbStyleGuide from './AirbnbStyleGuide';

// good
import AirbnbStyleGuide from './AirbnbStyleGuide';





10.3 And do not export directly from an import.

Why? Although the one-liner is concise, having one clear way to import and one clear way to export makes things consistent.

// bad
// filename es6.js
export { es6 as default } from './AirbnbStyleGuide';

// good
// filename es6.js
import { es6 } from './AirbnbStyleGuide';
export default es6;





10.4 Only import from a path in one place.
eslint: no-duplicate-imports

Why? Having multiple lines that import from the same path can make code harder to maintain.

// bad
import foo from 'foo';
// … some other imports … //
import { named1, named2 } from 'foo';

// good
import foo, { named1, named2 } from 'foo';

// good
import foo, {
  named1,
  named2,
} from 'foo';





10.5 Do not export mutable bindings.
eslint: import/no-mutable-exports

Why? Mutation should be avoided in general, but in particular when exporting mutable bindings. While this technique may be needed for some special cases, in general, only constant references should be exported.

// bad
let foo = 3;
export { foo };

// good
const foo = 3;
export { foo };





10.6 In modules with a single export, prefer default export over named export.
eslint: import/prefer-default-export

Why? To encourage more files that only ever export one thing, which is better for readability and maintainability.

// bad
export function foo() {}

// good
export default function foo() {}





10.7 Put all imports above non-import statements.
eslint: import/first

Why? Since imports are hoisted, keeping them all at the top prevents surprising behavior.

// bad
import foo from 'foo';
foo.init();

import bar from 'bar';

// good
import foo from 'foo';
import bar from 'bar';

foo.init();





10.8 Multiline imports should be indented just like multiline array and object literals.

Why? The curly braces follow the same indentation rules as every other curly brace block in the style guide, as do the trailing commas.

// bad
import {longNameA, longNameB, longNameC, longNameD, longNameE} from 'path';

// good
import {
  longNameA,
  longNameB,
  longNameC,
  longNameD,
  longNameE,
} from 'path';





10.9 Disallow Webpack loader syntax in module import statements.
eslint: import/no-webpack-loader-syntax

Why? Since using Webpack syntax in the imports couples the code to a module bundler. Prefer using the loader syntax in webpack.config.js.

// bad
import fooSass from 'css!sass!foo.scss';
import barCss from 'style!css!bar.css';

// good
import fooSass from 'foo.scss';
import barCss from 'bar.css';


⬆ back to top
Iterators and Generators



11.1 Don’t use iterators. Prefer JavaScript’s higher-order functions instead of loops like for-in or for-of. eslint: no-iterator no-restricted-syntax

Why? This enforces our immutable rule. Dealing with pure functions that return values is easier to reason about than side effects.


Use map() / every() / filter() / find() / findIndex() / reduce() / some() / ... to iterate over arrays, and Object.keys() / Object.values() / Object.entries() to produce arrays so you can iterate over objects.

const numbers = [1, 2, 3, 4, 5];

// bad
let sum = 0;
for (let num of numbers) {
  sum += num;
}
sum === 15;

// good
let sum = 0;
numbers.forEach((num) => {
  sum += num;
});
sum === 15;

// best (use the functional force)
const sum = numbers.reduce((total, num) => total + num, 0);
sum === 15;

// bad
const increasedByOne = [];
for (let i = 0; i < numbers.length; i++) {
  increasedByOne.push(numbers[i] + 1);
}

// good
const increasedByOne = [];
numbers.forEach((num) => {
  increasedByOne.push(num + 1);
});

// best (keeping it functional)
const increasedByOne = numbers.map((num) => num + 1);





11.2 Don’t use generators for now.

Why? They don’t transpile well to ES5.






11.3 If you must use generators, or if you disregard our advice, make sure their function signature is spaced properly. eslint: generator-star-spacing

Why? function and * are part of the same conceptual keyword - * is not a modifier for function, function* is a unique construct, different from function.

// bad
function * foo() {
  // ...
}

// bad
const bar = function * () {
  // ...
};

// bad
const baz = function *() {
  // ...
};

// bad
const quux = function*() {
  // ...
};

// bad
function*foo() {
  // ...
}

// bad
function *foo() {
  // ...
}

// very bad
function
*
foo() {
  // ...
}

// very bad
const wat = function
*
() {
  // ...
};

// good
function* foo() {
  // ...
}

// good
const foo = function* () {
  // ...
};


⬆ back to top
Properties



12.1 Use dot notation when accessing properties. eslint: dot-notation
const luke = {
  jedi: true,
  age: 28,
};

// bad
const isJedi = luke['jedi'];

// good
const isJedi = luke.jedi;





12.2 Use bracket notation [] when accessing properties with a variable.
const luke = {
  jedi: true,
  age: 28,
};

function getProp(prop) {
  return luke[prop];
}

const isJedi = getProp('jedi');





12.3 Use exponentiation operator ** when calculating exponentiations. eslint: no-restricted-properties.
// bad
const binary = Math.pow(2, 10);

// good
const binary = 2 ** 10;


⬆ back to top
Variables



13.1 Always use const or let to declare variables. Not doing so will result in global variables. We want to avoid polluting the global namespace. Captain Planet warned us of that. eslint: no-undef prefer-const
// bad
superPower = new SuperPower();

// good
const superPower = new SuperPower();





13.2 Use one const or let declaration per variable or assignment. eslint: one-var

Why? It’s easier to add new variable declarations this way, and you never have to worry about swapping out a ; for a , or introducing punctuation-only diffs. You can also step through each declaration with the debugger, instead of jumping through all of them at once.

// bad
const items = getItems(),
    goSportsTeam = true,
    dragonball = 'z';

// bad
// (compare to above, and try to spot the mistake)
const items = getItems(),
    goSportsTeam = true;
    dragonball = 'z';

// good
const items = getItems();
const goSportsTeam = true;
const dragonball = 'z';





13.3 Group all your consts and then group all your lets.

Why? This is helpful when later on you might need to assign a variable depending on one of the previous assigned variables.

// bad
let i, len, dragonball,
    items = getItems(),
    goSportsTeam = true;

// bad
let i;
const items = getItems();
let dragonball;
const goSportsTeam = true;
let len;

// good
const goSportsTeam = true;
const items = getItems();
let dragonball;
let i;
let length;





13.4 Assign variables where you need them, but place them in a reasonable place.

Why? let and const are block scoped and not function scoped.

// bad - unnecessary function call
function checkName(hasName) {
  const name = getName();

  if (hasName === 'test') {
    return false;
  }

  if (name === 'test') {
    this.setName('');
    return false;
  }

  return name;
}

// good
function checkName(hasName) {
  if (hasName === 'test') {
    return false;
  }

  const name = getName();

  if (name === 'test') {
    this.setName('');
    return false;
  }

  return name;
}





13.5 Don’t chain variable assignments. eslint: no-multi-assign

Why? Chaining variable assignments creates implicit global variables.

// bad
(function example() {
  // JavaScript interprets this as
  // let a = ( b = ( c = 1 ) );
  // The let keyword only applies to variable a; variables b and c become
  // global variables.
  let a = b = c = 1;
}());

console.log(a); // throws ReferenceError
console.log(b); // 1
console.log(c); // 1

// good
(function example() {
  let a = 1;
  let b = a;
  let c = a;
}());

console.log(a); // throws ReferenceError
console.log(b); // throws ReferenceError
console.log(c); // throws ReferenceError

// the same applies for `const`





13.6 Avoid using unary increments and decrements (++, --). eslint no-plusplus

Why? Per the eslint documentation, unary increment and decrement statements are subject to automatic semicolon insertion and can cause silent errors with incrementing or decrementing values within an application. It is also more expressive to mutate your values with statements like num += 1 instead of num++ or num ++. Disallowing unary increment and decrement statements also prevents you from pre-incrementing/pre-decrementing values unintentionally which can also cause unexpected behavior in your programs.

// bad

const array = [1, 2, 3];
let num = 1;
num++;
--num;

let sum = 0;
let truthyCount = 0;
for (let i = 0; i < array.length; i++) {
  let value = array[i];
  sum += value;
  if (value) {
    truthyCount++;
  }
}

// good

const array = [1, 2, 3];
let num = 1;
num += 1;
num -= 1;

const sum = array.reduce((a, b) => a + b, 0);
const truthyCount = array.filter(Boolean).length;





13.7 Avoid linebreaks before or after = in an assignment. If your assignment violates max-len, surround the value in parens. eslint operator-linebreak.

Why? Linebreaks surrounding = can obfuscate the value of an assignment.

// bad
const foo =
  superLongLongLongLongLongLongLongLongFunctionName();

// bad
const foo
  = 'superLongLongLongLongLongLongLongLongString';

// good
const foo = (
  superLongLongLongLongLongLongLongLongFunctionName()
);

// good
const foo = 'superLongLongLongLongLongLongLongLongString';





13.8 Disallow unused variables. eslint: no-unused-vars

Why? Variables that are declared and not used anywhere in the code are most likely an error due to incomplete refactoring. Such variables take up space in the code and can lead to confusion by readers.

// bad

var some_unused_var = 42;

// Write-only variables are not considered as used.
var y = 10;
y = 5;

// A read for a modification of itself is not considered as used.
var z = 0;
z = z + 1;

// Unused function arguments.
function getX(x, y) {
    return x;
}

// good

function getXPlusY(x, y) {
  return x + y;
}

var x = 1;
var y = a + 2;

alert(getXPlusY(x, y));

// 'type' is ignored even if unused because it has a rest property sibling.
// This is a form of extracting an object that omits the specified keys.
var { type, ...coords } = data;
// 'coords' is now the 'data' object without its 'type' property.


⬆ back to top
Hoisting



14.1 var declarations get hoisted to the top of their closest enclosing function scope, their assignment does not. const and let declarations are blessed with a new concept called Temporal Dead Zones (TDZ). It’s important to know why typeof is no longer safe.
// we know this wouldn’t work (assuming there
// is no notDefined global variable)
function example() {
  console.log(notDefined); // => throws a ReferenceError
}

// creating a variable declaration after you
// reference the variable will work due to
// variable hoisting. Note: the assignment
// value of `true` is not hoisted.
function example() {
  console.log(declaredButNotAssigned); // => undefined
  var declaredButNotAssigned = true;
}

// the interpreter is hoisting the variable
// declaration to the top of the scope,
// which means our example could be rewritten as:
function example() {
  let declaredButNotAssigned;
  console.log(declaredButNotAssigned); // => undefined
  declaredButNotAssigned = true;
}

// using const and let
function example() {
  console.log(declaredButNotAssigned); // => throws a ReferenceError
  console.log(typeof declaredButNotAssigned); // => throws a ReferenceError
  const declaredButNotAssigned = true;
}





14.2 Anonymous function expressions hoist their variable name, but not the function assignment.
function example() {
  console.log(anonymous); // => undefined

  anonymous(); // => TypeError anonymous is not a function

  var anonymous = function () {
    console.log('anonymous function expression');
  };
}





14.3 Named function expressions hoist the variable name, not the function name or the function body.
function example() {
  console.log(named); // => undefined

  named(); // => TypeError named is not a function

  superPower(); // => ReferenceError superPower is not defined

  var named = function superPower() {
    console.log('Flying');
  };
}

// the same is true when the function name
// is the same as the variable name.
function example() {
  console.log(named); // => undefined

  named(); // => TypeError named is not a function

  var named = function named() {
    console.log('named');
  };
}





14.4 Function declarations hoist their name and the function body.
function example() {
  superPower(); // => Flying

  function superPower() {
    console.log('Flying');
  }
}


For more information refer to JavaScript Scoping & Hoisting by Ben Cherry.


⬆ back to top
Comparison Operators & Equality


15.1 Use === and !== over == and !=. eslint: eqeqeq




15.2 Conditional statements such as the if statement evaluate their expression using coercion with the ToBoolean abstract method and always follow these simple rules:

Objects evaluate to true
Undefined evaluates to false
Null evaluates to false
Booleans evaluate to the value of the boolean
Numbers evaluate to false if +0, -0, or NaN, otherwise true
Strings evaluate to false if an empty string '', otherwise true

if ([0] && []) {
  // true
  // an array (even an empty one) is an object, objects will evaluate to true
}





15.3 Use shortcuts for booleans, but explicit comparisons for strings and numbers.
// bad
if (isValid === true) {
  // ...
}

// good
if (isValid) {
  // ...
}

// bad
if (name) {
  // ...
}

// good
if (name !== '') {
  // ...
}

// bad
if (collection.length) {
  // ...
}

// good
if (collection.length > 0) {
  // ...
}




15.4 For more information see Truth Equality and JavaScript by Angus Croll.




15.5 Use braces to create blocks in case and default clauses that contain lexical declarations (e.g. let, const, function, and class). eslint: no-case-declarations

Why? Lexical declarations are visible in the entire switch block but only get initialized when assigned, which only happens when its case is reached. This causes problems when multiple case clauses attempt to define the same thing.

// bad
switch (foo) {
  case 1:
    let x = 1;
    break;
  case 2:
    const y = 2;
    break;
  case 3:
    function f() {
      // ...
    }
    break;
  default:
    class C {}
}

// good
switch (foo) {
  case 1: {
    let x = 1;
    break;
  }
  case 2: {
    const y = 2;
    break;
  }
  case 3: {
    function f() {
      // ...
    }
    break;
  }
  case 4:
    bar();
    break;
  default: {
    class C {}
  }
}





15.6 Ternaries should not be nested and generally be single line expressions. eslint: no-nested-ternary
// bad
const foo = maybe1 > maybe2
  ? ""bar""
  : value1 > value2 ? ""baz"" : null;

// split into 2 separated ternary expressions
const maybeNull = value1 > value2 ? 'baz' : null;

// better
const foo = maybe1 > maybe2
  ? 'bar'
  : maybeNull;

// best
const foo = maybe1 > maybe2 ? 'bar' : maybeNull;





15.7 Avoid unneeded ternary statements. eslint: no-unneeded-ternary
// bad
const foo = a ? a : b;
const bar = c ? true : false;
const baz = c ? false : true;

// good
const foo = a || b;
const bar = !!c;
const baz = !c;





15.8 When mixing operators, enclose them in parentheses. The only exception is the standard arithmetic operators: +, -, and ** since their precedence is broadly understood. We recommend enclosing / and * in parentheses because their precedence can be ambiguous when they are mixed.
eslint: no-mixed-operators

Why? This improves readability and clarifies the developer’s intention.

// bad
const foo = a && b < 0 || c > 0 || d + 1 === 0;

// bad
const bar = a ** b - 5 % d;

// bad
// one may be confused into thinking (a || b) && c
if (a || b && c) {
  return d;
}

// bad
const bar = a + b / c * d;

// good
const foo = (a && b < 0) || c > 0 || (d + 1 === 0);

// good
const bar = a ** b - (5 % d);

// good
if (a || (b && c)) {
  return d;
}

// good
const bar = a + (b / c) * d;


⬆ back to top
Blocks



16.1 Use braces with all multi-line blocks. eslint: nonblock-statement-body-position
// bad
if (test)
  return false;

// good
if (test) return false;

// good
if (test) {
  return false;
}

// bad
function foo() { return false; }

// good
function bar() {
  return false;
}





16.2 If you’re using multi-line blocks with if and else, put else on the same line as your if block’s closing brace. eslint: brace-style
// bad
if (test) {
  thing1();
  thing2();
}
else {
  thing3();
}

// good
if (test) {
  thing1();
  thing2();
} else {
  thing3();
}





16.3 If an if block always executes a return statement, the subsequent else block is unnecessary. A return in an else if block following an if block that contains a return can be separated into multiple if blocks. eslint: no-else-return
// bad
function foo() {
  if (x) {
    return x;
  } else {
    return y;
  }
}

// bad
function cats() {
  if (x) {
    return x;
  } else if (y) {
    return y;
  }
}

// bad
function dogs() {
  if (x) {
    return x;
  } else {
    if (y) {
      return y;
    }
  }
}

// good
function foo() {
  if (x) {
    return x;
  }

  return y;
}

// good
function cats() {
  if (x) {
    return x;
  }

  if (y) {
    return y;
  }
}

// good
function dogs(x) {
  if (x) {
    if (z) {
      return y;
    }
  } else {
    return z;
  }
}


⬆ back to top
Control Statements



17.1 In case your control statement (if, while etc.) gets too long or exceeds the maximum line length, each (grouped) condition could be put into a new line. The logical operator should begin the line.

Why? Requiring operators at the beginning of the line keeps the operators aligned and follows a pattern similar to method chaining. This also improves readability by making it easier to visually follow complex logic.

// bad
if ((foo === 123 || bar === 'abc') && doesItLookGoodWhenItBecomesThatLong() && isThisReallyHappening()) {
  thing1();
}

// bad
if (foo === 123 &&
  bar === 'abc') {
  thing1();
}

// bad
if (foo === 123
  && bar === 'abc') {
  thing1();
}

// bad
if (
  foo === 123 &&
  bar === 'abc'
) {
  thing1();
}

// good
if (
  foo === 123
  && bar === 'abc'
) {
  thing1();
}

// good
if (
  (foo === 123 || bar === 'abc')
  && doesItLookGoodWhenItBecomesThatLong()
  && isThisReallyHappening()
) {
  thing1();
}

// good
if (foo === 123 && bar === 'abc') {
  thing1();
}





17.2 Don't use selection operators in place of control statements.
// bad
!isRunning && startRunning();

// good
if (!isRunning) {
  startRunning();
}


⬆ back to top
Comments



18.1 Use /** ... */ for multi-line comments.
// bad
// make() returns a new element
// based on the passed in tag name
//
// @param {String} tag
// @return {Element} element
function make(tag) {

  // ...

  return element;
}

// good
/**
 * make() returns a new element
 * based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}





18.2 Use // for single line comments. Place single line comments on a newline above the subject of the comment. Put an empty line before the comment unless it’s on the first line of a block.
// bad
const active = true;  // is current tab

// good
// is current tab
const active = true;

// bad
function getType() {
  console.log('fetching type...');
  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}

// good
function getType() {
  console.log('fetching type...');

  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}

// also good
function getType() {
  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}





18.3 Start all comments with a space to make it easier to read. eslint: spaced-comment
// bad
//is current tab
const active = true;

// good
// is current tab
const active = true;

// bad
/**
 *make() returns a new element
 *based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}

// good
/**
 * make() returns a new element
 * based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}




18.4 Prefixing your comments with FIXME or TODO helps other developers quickly understand if you’re pointing out a problem that needs to be revisited, or if you’re suggesting a solution to the problem that needs to be implemented. These are different than regular comments because they are actionable. The actions are FIXME: -- need to figure this out or TODO: -- need to implement.




18.5 Use // FIXME: to annotate problems.
class Calculator extends Abacus {
  constructor() {
    super();

    // FIXME: shouldn’t use a global here
    total = 0;
  }
}





18.6 Use // TODO: to annotate solutions to problems.
class Calculator extends Abacus {
  constructor() {
    super();

    // TODO: total should be configurable by an options param
    this.total = 0;
  }
}


⬆ back to top
Whitespace



19.1 Use soft tabs (space character) set to 2 spaces. eslint: indent
// bad
function foo() {
∙∙∙∙let name;
}

// bad
function bar() {
∙let name;
}

// good
function baz() {
∙∙let name;
}





19.2 Place 1 space before the leading brace. eslint: space-before-blocks
// bad
function test(){
  console.log('test');
}

// good
function test() {
  console.log('test');
}

// bad
dog.set('attr',{
  age: '1 year',
  breed: 'Bernese Mountain Dog',
});

// good
dog.set('attr', {
  age: '1 year',
  breed: 'Bernese Mountain Dog',
});





19.3 Place 1 space before the opening parenthesis in control statements (if, while etc.). Place no space between the argument list and the function name in function calls and declarations. eslint: keyword-spacing
// bad
if(isJedi) {
  fight ();
}

// good
if (isJedi) {
  fight();
}

// bad
function fight () {
  console.log ('Swooosh!');
}

// good
function fight() {
  console.log('Swooosh!');
}





19.4 Set off operators with spaces. eslint: space-infix-ops
// bad
const x=y+5;

// good
const x = y + 5;





19.5 End files with a single newline character. eslint: eol-last
// bad
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;
// bad
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;↵
↵
// good
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;↵





19.6 Use indentation when making long method chains (more than 2 method chains). Use a leading dot, which
emphasizes that the line is a method call, not a new statement. eslint: newline-per-chained-call no-whitespace-before-property
// bad
$('#items').find('.selected').highlight().end().find('.open').updateCount();

// bad
$('#items').
  find('.selected').
    highlight().
    end().
  find('.open').
    updateCount();

// good
$('#items')
  .find('.selected')
    .highlight()
    .end()
  .find('.open')
    .updateCount();

// bad
const leds = stage.selectAll('.led').data(data).enter().append('svg:svg').classed('led', true)
    .attr('width', (radius + margin) * 2).append('svg:g')
    .attr('transform', `translate(${radius + margin},${radius + margin})`)
    .call(tron.led);

// good
const leds = stage.selectAll('.led')
    .data(data)
  .enter().append('svg:svg')
    .classed('led', true)
    .attr('width', (radius + margin) * 2)
  .append('svg:g')
    .attr('transform', `translate(${radius + margin},${radius + margin})`)
    .call(tron.led);

// good
const leds = stage.selectAll('.led').data(data);





19.7 Leave a blank line after blocks and before the next statement.
// bad
if (foo) {
  return bar;
}
return baz;

// good
if (foo) {
  return bar;
}

return baz;

// bad
const obj = {
  foo() {
  },
  bar() {
  },
};
return obj;

// good
const obj = {
  foo() {
  },

  bar() {
  },
};

return obj;

// bad
const arr = [
  function foo() {
  },
  function bar() {
  },
];
return arr;

// good
const arr = [
  function foo() {
  },

  function bar() {
  },
];

return arr;





19.8 Do not pad your blocks with blank lines. eslint: padded-blocks
// bad
function bar() {

  console.log(foo);

}

// bad
if (baz) {

  console.log(qux);
} else {
  console.log(foo);

}

// bad
class Foo {

  constructor(bar) {
    this.bar = bar;
  }
}

// good
function bar() {
  console.log(foo);
}

// good
if (baz) {
  console.log(qux);
} else {
  console.log(foo);
}





19.9 Do not use multiple blank lines to pad your code. eslint: no-multiple-empty-lines
// bad
class Person {
  constructor(fullName, email, birthday) {
    this.fullName = fullName;


    this.email = email;


    this.setAge(birthday);
  }


  setAge(birthday) {
    const today = new Date();


    const age = this.getAge(today, birthday);


    this.age = age;
  }


  getAge(today, birthday) {
    // ..
  }
}

// good
class Person {
  constructor(fullName, email, birthday) {
    this.fullName = fullName;
    this.email = email;
    this.setAge(birthday);
  }

  setAge(birthday) {
    const today = new Date();
    const age = getAge(today, birthday);
    this.age = age;
  }

  getAge(today, birthday) {
    // ..
  }
}





19.10 Do not add spaces inside parentheses. eslint: space-in-parens
// bad
function bar( foo ) {
  return foo;
}

// good
function bar(foo) {
  return foo;
}

// bad
if ( foo ) {
  console.log(foo);
}

// good
if (foo) {
  console.log(foo);
}





19.11 Do not add spaces inside brackets. eslint: array-bracket-spacing
// bad
const foo = [ 1, 2, 3 ];
console.log(foo[ 0 ]);

// good
const foo = [1, 2, 3];
console.log(foo[0]);





19.12 Add spaces inside curly braces. eslint: object-curly-spacing
// bad
const foo = {clark: 'kent'};

// good
const foo = { clark: 'kent' };





19.13 Avoid having lines of code that are longer than 100 characters (including whitespace). Note: per above, long strings are exempt from this rule, and should not be broken up. eslint: max-len

Why? This ensures readability and maintainability.

// bad
const foo = jsonData && jsonData.foo && jsonData.foo.bar && jsonData.foo.bar.baz && jsonData.foo.bar.baz.quux && jsonData.foo.bar.baz.quux.xyzzy;

// bad
$.ajax({ method: 'POST', url: 'https://airbnb.com/', data: { name: 'John' } }).done(() => console.log('Congratulations!')).fail(() => console.log('You have failed this city.'));

// good
const foo = jsonData
  && jsonData.foo
  && jsonData.foo.bar
  && jsonData.foo.bar.baz
  && jsonData.foo.bar.baz.quux
  && jsonData.foo.bar.baz.quux.xyzzy;

// good
$.ajax({
  method: 'POST',
  url: 'https://airbnb.com/',
  data: { name: 'John' },
})
  .done(() => console.log('Congratulations!'))
  .fail(() => console.log('You have failed this city.'));





19.14 Require consistent spacing inside an open block token and the next token on the same line. This rule also enforces consistent spacing inside a close block token and previous token on the same line. eslint: block-spacing
// bad
function foo() {return true;}
if (foo) { bar = 0;}

// good
function foo() { return true; }
if (foo) { bar = 0; }





19.15 Avoid spaces before commas and require a space after commas. eslint: comma-spacing
// bad
var foo = 1,bar = 2;
var arr = [1 , 2];

// good
var foo = 1, bar = 2;
var arr = [1, 2];





19.16 Enforce spacing inside of computed property brackets. eslint: computed-property-spacing
// bad
obj[foo ]
obj[ 'foo']
var x = {[ b ]: a}
obj[foo[ bar ]]

// good
obj[foo]
obj['foo']
var x = { [b]: a }
obj[foo[bar]]





19.17 Avoid spaces between functions and their invocations. eslint: func-call-spacing
// bad
func ();

func
();

// good
func();





19.18 Enforce spacing between keys and values in object literal properties. eslint: key-spacing
// bad
var obj = { ""foo"" : 42 };
var obj2 = { ""foo"":42 };

// good
var obj = { ""foo"": 42 };




19.19 Avoid trailing spaces at the end of lines. eslint: no-trailing-spaces




19.20 Avoid multiple empty lines, only allow one newline at the end of files, and avoid a newline at the beginning of files. eslint: no-multiple-empty-lines
// bad - multiple empty lines
var x = 1;


var y = 2;

// bad - 2+ newlines at end of file
var x = 1;
var y = 2;


// bad - 1+ newline(s) at beginning of file

var x = 1;
var y = 2;

// good
var x = 1;
var y = 2;



⬆ back to top
Commas



20.1 Leading commas: Nope. eslint: comma-style
// bad
const story = [
    once
  , upon
  , aTime
];

// good
const story = [
  once,
  upon,
  aTime,
];

// bad
const hero = {
    firstName: 'Ada'
  , lastName: 'Lovelace'
  , birthYear: 1815
  , superPower: 'computers'
};

// good
const hero = {
  firstName: 'Ada',
  lastName: 'Lovelace',
  birthYear: 1815,
  superPower: 'computers',
};





20.2 Additional trailing comma: Yup. eslint: comma-dangle

Why? This leads to cleaner git diffs. Also, transpilers like Babel will remove the additional trailing comma in the transpiled code which means you don’t have to worry about the trailing comma problem in legacy browsers.

// bad - git diff without trailing comma
const hero = {
     firstName: 'Florence',
-    lastName: 'Nightingale'
+    lastName: 'Nightingale',
+    inventorOf: ['coxcomb chart', 'modern nursing']
};

// good - git diff with trailing comma
const hero = {
     firstName: 'Florence',
     lastName: 'Nightingale',
+    inventorOf: ['coxcomb chart', 'modern nursing'],
};
// bad
const hero = {
  firstName: 'Dana',
  lastName: 'Scully'
};

const heroes = [
  'Batman',
  'Superman'
];

// good
const hero = {
  firstName: 'Dana',
  lastName: 'Scully',
};

const heroes = [
  'Batman',
  'Superman',
];

// bad
function createHero(
  firstName,
  lastName,
  inventorOf
) {
  // does nothing
}

// good
function createHero(
  firstName,
  lastName,
  inventorOf,
) {
  // does nothing
}

// good (note that a comma must not appear after a ""rest"" element)
function createHero(
  firstName,
  lastName,
  inventorOf,
  ...heroArgs
) {
  // does nothing
}

// bad
createHero(
  firstName,
  lastName,
  inventorOf
);

// good
createHero(
  firstName,
  lastName,
  inventorOf,
);

// good (note that a comma must not appear after a ""rest"" element)
createHero(
  firstName,
  lastName,
  inventorOf,
  ...heroArgs
);


⬆ back to top
Semicolons



21.1 Yup. eslint: semi

Why? When JavaScript encounters a line break without a semicolon, it uses a set of rules called Automatic Semicolon Insertion to determine whether or not it should regard that line break as the end of a statement, and (as the name implies) place a semicolon into your code before the line break if it thinks so. ASI contains a few eccentric behaviors, though, and your code will break if JavaScript misinterprets your line break. These rules will become more complicated as new features become a part of JavaScript. Explicitly terminating your statements and configuring your linter to catch missing semicolons will help prevent you from encountering issues.

// bad - raises exception
const luke = {}
const leia = {}
[luke, leia].forEach((jedi) => jedi.father = 'vader')

// bad - raises exception
const reaction = ""No! That’s impossible!""
(async function meanwhileOnTheFalcon() {
  // handle `leia`, `lando`, `chewie`, `r2`, `c3p0`
  // ...
}())

// bad - returns `undefined` instead of the value on the next line - always happens when `return` is on a line by itself because of ASI!
function foo() {
  return
    'search your feelings, you know it to be foo'
}

// good
const luke = {};
const leia = {};
[luke, leia].forEach((jedi) => {
  jedi.father = 'vader';
});

// good
const reaction = ""No! That’s impossible!"";
(async function meanwhileOnTheFalcon() {
  // handle `leia`, `lando`, `chewie`, `r2`, `c3p0`
  // ...
}());

// good
function foo() {
  return 'search your feelings, you know it to be foo';
}
Read more.


⬆ back to top
Type Casting & Coercion


22.1 Perform type coercion at the beginning of the statement.




22.2 Strings: eslint: no-new-wrappers
// => this.reviewScore = 9;

// bad
const totalScore = new String(this.reviewScore); // typeof totalScore is ""object"" not ""string""

// bad
const totalScore = this.reviewScore + ''; // invokes this.reviewScore.valueOf()

// bad
const totalScore = this.reviewScore.toString(); // isn’t guaranteed to return a string

// good
const totalScore = String(this.reviewScore);





22.3 Numbers: Use Number for type casting and parseInt always with a radix for parsing strings. eslint: radix no-new-wrappers
const inputValue = '4';

// bad
const val = new Number(inputValue);

// bad
const val = +inputValue;

// bad
const val = inputValue >> 0;

// bad
const val = parseInt(inputValue);

// good
const val = Number(inputValue);

// good
const val = parseInt(inputValue, 10);





22.4 If for whatever reason you are doing something wild and parseInt is your bottleneck and need to use Bitshift for performance reasons, leave a comment explaining why and what you’re doing.
// good
/**
 * parseInt was the reason my code was slow.
 * Bitshifting the String to coerce it to a
 * Number made it a lot faster.
 */
const val = inputValue >> 0;





22.5 Note: Be careful when using bitshift operations. Numbers are represented as 64-bit values, but bitshift operations always return a 32-bit integer (source). Bitshift can lead to unexpected behavior for integer values larger than 32 bits. Discussion. Largest signed 32-bit Int is 2,147,483,647:
2147483647 >> 0; // => 2147483647
2147483648 >> 0; // => -2147483648
2147483649 >> 0; // => -2147483647





22.6 Booleans: eslint: no-new-wrappers
const age = 0;

// bad
const hasAge = new Boolean(age);

// good
const hasAge = Boolean(age);

// best
const hasAge = !!age;


⬆ back to top
Naming Conventions



23.1 Avoid single letter names. Be descriptive with your naming. eslint: id-length
// bad
function q() {
  // ...
}

// good
function query() {
  // ...
}





23.2 Use camelCase when naming objects, functions, and instances. eslint: camelcase
// bad
const OBJEcttsssss = {};
const this_is_my_object = {};
function c() {}

// good
const thisIsMyObject = {};
function thisIsMyFunction() {}





23.3 Use PascalCase only when naming constructors or classes. eslint: new-cap
// bad
function user(options) {
  this.name = options.name;
}

const bad = new user({
  name: 'nope',
});

// good
class User {
  constructor(options) {
    this.name = options.name;
  }
}

const good = new User({
  name: 'yup',
});





23.4 Do not use trailing or leading underscores. eslint: no-underscore-dangle

Why? JavaScript does not have the concept of privacy in terms of properties or methods. Although a leading underscore is a common convention to mean “private”, in fact, these properties are fully public, and as such, are part of your public API contract. This convention might lead developers to wrongly think that a change won’t count as breaking, or that tests aren’t needed. tl;dr: if you want something to be “private”, it must not be observably present.

// bad
this.__firstName__ = 'Panda';
this.firstName_ = 'Panda';
this._firstName = 'Panda';

// good
this.firstName = 'Panda';

// good, in environments where WeakMaps are available
// see https://kangax.github.io/compat-table/es6/#test-WeakMap
const firstNames = new WeakMap();
firstNames.set(this, 'Panda');





23.5 Don’t save references to this. Use arrow functions or Function#bind.
// bad
function foo() {
  const self = this;
  return function () {
    console.log(self);
  };
}

// bad
function foo() {
  const that = this;
  return function () {
    console.log(that);
  };
}

// good
function foo() {
  return () => {
    console.log(this);
  };
}





23.6 A base filename should exactly match the name of its default export.
// file 1 contents
class CheckBox {
  // ...
}
export default CheckBox;

// file 2 contents
export default function fortyTwo() { return 42; }

// file 3 contents
export default function insideDirectory() {}

// in some other file
// bad
import CheckBox from './checkBox'; // PascalCase import/export, camelCase filename
import FortyTwo from './FortyTwo'; // PascalCase import/filename, camelCase export
import InsideDirectory from './InsideDirectory'; // PascalCase import/filename, camelCase export

// bad
import CheckBox from './check_box'; // PascalCase import/export, snake_case filename
import forty_two from './forty_two'; // snake_case import/filename, camelCase export
import inside_directory from './inside_directory'; // snake_case import, camelCase export
import index from './inside_directory/index'; // requiring the index file explicitly
import insideDirectory from './insideDirectory/index'; // requiring the index file explicitly

// good
import CheckBox from './CheckBox'; // PascalCase export/import/filename
import fortyTwo from './fortyTwo'; // camelCase export/import/filename
import insideDirectory from './insideDirectory'; // camelCase export/import/directory name/implicit ""index""
// ^ supports both insideDirectory.js and insideDirectory/index.js





23.7 Use camelCase when you export-default a function. Your filename should be identical to your function’s name.
function makeStyleGuide() {
  // ...
}

export default makeStyleGuide;





23.8 Use PascalCase when you export a constructor / class / singleton / function library / bare object.
const AirbnbStyleGuide = {
  es6: {
  },
};

export default AirbnbStyleGuide;





23.9 Acronyms and initialisms should always be all uppercased, or all lowercased.

Why? Names are for readability, not to appease a computer algorithm.

// bad
import SmsContainer from './containers/SmsContainer';

// bad
const HttpRequests = [
  // ...
];

// good
import SMSContainer from './containers/SMSContainer';

// good
const HTTPRequests = [
  // ...
];

// also good
const httpRequests = [
  // ...
];

// best
import TextMessageContainer from './containers/TextMessageContainer';

// best
const requests = [
  // ...
];





23.10 You may optionally uppercase a constant only if it (1) is exported, (2) is a const (it can not be reassigned), and (3) the programmer can trust it (and its nested properties) to never change.

Why? This is an additional tool to assist in situations where the programmer would be unsure if a variable might ever change. UPPERCASE_VARIABLES are letting the programmer know that they can trust the variable (and its properties) not to change.


What about all const variables? - This is unnecessary, so uppercasing should not be used for constants within a file. It should be used for exported constants however.
What about exported objects? - Uppercase at the top level of export (e.g. EXPORTED_OBJECT.key) and maintain that all nested properties do not change.

// bad
const PRIVATE_VARIABLE = 'should not be unnecessarily uppercased within a file';

// bad
export const THING_TO_BE_CHANGED = 'should obviously not be uppercased';

// bad
export let REASSIGNABLE_VARIABLE = 'do not use let with uppercase variables';

// ---

// allowed but does not supply semantic value
export const apiKey = 'SOMEKEY';

// better in most cases
export const API_KEY = 'SOMEKEY';

// ---

// bad - unnecessarily uppercases key while adding no semantic value
export const MAPPING = {
  KEY: 'value'
};

// good
export const MAPPING = {
  key: 'value'
};


⬆ back to top
Accessors


24.1 Accessor functions for properties are not required.




24.2 Do not use JavaScript getters/setters as they cause unexpected side effects and are harder to test, maintain, and reason about. Instead, if you do make accessor functions, use getVal() and setVal('hello').
// bad
class Dragon {
  get age() {
    // ...
  }

  set age(value) {
    // ...
  }
}

// good
class Dragon {
  getAge() {
    // ...
  }

  setAge(value) {
    // ...
  }
}





24.3 If the property/method is a boolean, use isVal() or hasVal().
// bad
if (!dragon.age()) {
  return false;
}

// good
if (!dragon.hasAge()) {
  return false;
}





24.4 It’s okay to create get() and set() functions, but be consistent.
class Jedi {
  constructor(options = {}) {
    const lightsaber = options.lightsaber || 'blue';
    this.set('lightsaber', lightsaber);
  }

  set(key, val) {
    this[key] = val;
  }

  get(key) {
    return this[key];
  }
}


⬆ back to top
Events



25.1 When attaching data payloads to events (whether DOM events or something more proprietary like Backbone events), pass an object literal (also known as a ""hash"") instead of a raw value. This allows a subsequent contributor to add more data to the event payload without finding and updating every handler for the event. For example, instead of:
// bad
$(this).trigger('listingUpdated', listing.id);

// ...

$(this).on('listingUpdated', (e, listingID) => {
  // do something with listingID
});
prefer:
// good
$(this).trigger('listingUpdated', { listingID: listing.id });

// ...

$(this).on('listingUpdated', (e, data) => {
  // do something with data.listingID
});


⬆ back to top
jQuery



26.1 Prefix jQuery object variables with a $.
// bad
const sidebar = $('.sidebar');

// good
const $sidebar = $('.sidebar');

// good
const $sidebarBtn = $('.sidebar-btn');





26.2 Cache jQuery lookups.
// bad
function setSidebar() {
  $('.sidebar').hide();

  // ...

  $('.sidebar').css({
    'background-color': 'pink',
  });
}

// good
function setSidebar() {
  const $sidebar = $('.sidebar');
  $sidebar.hide();

  // ...

  $sidebar.css({
    'background-color': 'pink',
  });
}




26.3 For DOM queries use Cascading $('.sidebar ul') or parent > child $('.sidebar > ul'). jsPerf




26.4 Use find with scoped jQuery object queries.
// bad
$('ul', '.sidebar').hide();

// bad
$('.sidebar').find('ul').hide();

// good
$('.sidebar ul').hide();

// good
$('.sidebar > ul').hide();

// good
$sidebar.find('ul').hide();


⬆ back to top
ECMAScript 5 Compatibility


27.1 Refer to Kangax’s ES5 compatibility table.

⬆ back to top

ECMAScript 6+ (ES 2015+) Styles


28.1 This is a collection of links to the various ES6+ features.


Arrow Functions
Classes
Object Shorthand
Object Concise
Object Computed Properties
Template Strings
Destructuring
Default Parameters
Rest
Array Spreads
Let and Const
Exponentiation Operator
Iterators and Generators
Modules




28.2 Do not use TC39 proposals that have not reached stage 3.

Why? They are not finalized, and they are subject to change or to be withdrawn entirely. We want to use JavaScript, and proposals are not JavaScript yet.



⬆ back to top
Standard Library
The Standard Library
contains utilities that are functionally broken but remain for legacy reasons.



29.1 Use Number.isNaN instead of global isNaN.
eslint: no-restricted-globals

Why? The global isNaN coerces non-numbers to numbers, returning true for anything that coerces to NaN.
If this behavior is desired, make it explicit.

// bad
isNaN('1.2'); // false
isNaN('1.2.3'); // true

// good
Number.isNaN('1.2.3'); // false
Number.isNaN(Number('1.2.3')); // true





29.2 Use Number.isFinite instead of global isFinite.
eslint: no-restricted-globals

Why? The global isFinite coerces non-numbers to numbers, returning true for anything that coerces to a finite number.
If this behavior is desired, make it explicit.

// bad
isFinite('2e3'); // true

// good
Number.isFinite('2e3'); // false
Number.isFinite(parseInt('2e3', 10)); // true


⬆ back to top
Testing



30.1 Yup.
function foo() {
  return true;
}




30.2 No, but seriously:

Whichever testing framework you use, you should be writing tests!
Strive to write many small pure functions, and minimize where mutations occur.
Be cautious about stubs and mocks - they can make your tests more brittle.
We primarily use mocha and jest at Airbnb. tape is also used occasionally for small, separate modules.
100% test coverage is a good goal to strive for, even if it’s not always practical to reach it.
Whenever you fix a bug, write a regression test. A bug fixed without a regression test is almost certainly going to break again in the future.



⬆ back to top
Performance

On Layout & Web Performance
String vs Array Concat
Try/Catch Cost In a Loop
Bang Function
jQuery Find vs Context, Selector
innerHTML vs textContent for script text
Long String Concatenation
Are JavaScript functions like map(), reduce(), and filter() optimized for traversing arrays?
Loading...

⬆ back to top
Resources
Learning ES6+

Latest ECMA spec
ExploringJS
ES6 Compatibility Table
Comprehensive Overview of ES6 Features

Read This

Standard ECMA-262

Tools

Code Style Linters

ESlint - Airbnb Style .eslintrc
JSHint - Airbnb Style .jshintrc


Neutrino Preset - @neutrinojs/airbnb

Other Style Guides

Google JavaScript Style Guide
Google JavaScript Style Guide (Old)
jQuery Core Style Guidelines
Principles of Writing Consistent, Idiomatic JavaScript
StandardJS

Other Styles

Naming this in nested functions - Christian Johansen
Conditional Callbacks - Ross Allen
Popular JavaScript Coding Conventions on GitHub - JeongHoon Byun
Multiple var statements in JavaScript, not superfluous - Ben Alman

Further Reading

Understanding JavaScript Closures - Angus Croll
Basic JavaScript for the impatient programmer - Dr. Axel Rauschmayer
You Might Not Need jQuery - Zack Bloom & Adam Schwartz
ES6 Features - Luke Hoban
Frontend Guidelines - Benjamin De Cock

Books

JavaScript: The Good Parts - Douglas Crockford
JavaScript Patterns - Stoyan Stefanov
Pro JavaScript Design Patterns - Ross Harmes and Dustin Diaz
High Performance Web Sites: Essential Knowledge for Front-End Engineers - Steve Souders
Maintainable JavaScript - Nicholas C. Zakas
JavaScript Web Applications - Alex MacCaw
Pro JavaScript Techniques - John Resig
Smashing Node.js: JavaScript Everywhere - Guillermo Rauch
Secrets of the JavaScript Ninja - John Resig and Bear Bibeault
Human JavaScript - Henrik Joreteg
Superhero.js - Kim Joar Bekkelund, Mads Mobæk, & Olav Bjorkoy
JSBooks - Julien Bouquillon
Third Party JavaScript - Ben Vinegar and Anton Kovalyov
Effective JavaScript: 68 Specific Ways to Harness the Power of JavaScript - David Herman
Eloquent JavaScript - Marijn Haverbeke
You Don’t Know JS: ES6 & Beyond - Kyle Simpson

Blogs

JavaScript Weekly
JavaScript, JavaScript...
Bocoup Weblog
Adequately Good
NCZOnline
Perfection Kills
Ben Alman
Dmitry Baranovskiy
nettuts

Podcasts

JavaScript Air
JavaScript Jabber

⬆ back to top
In the Wild
This is a list of organizations that are using this style guide. Send us a pull request and we'll add you to the list.

123erfasst: 123erfasst/javascript
3blades: 3Blades
4Catalyzer: 4Catalyzer/javascript
Aan Zee: AanZee/javascript
Adult Swim: adult-swim/javascript
Airbnb: airbnb/javascript
AltSchool: AltSchool/javascript
Apartmint: apartmint/javascript
Ascribe: ascribe/javascript
Avalara: avalara/javascript
Avant: avantcredit/javascript
Axept: axept/javascript
BashPros: BashPros/javascript
Billabong: billabong/javascript
Bisk: bisk
Bonhomme: bonhommeparis/javascript
Brainshark: brainshark/javascript
CaseNine: CaseNine/javascript
Cerner: Cerner
Chartboost: ChartBoost/javascript-style-guide
Coeur d'Alene Tribe: www.cdatribe-nsn.gov
ComparaOnline: comparaonline/javascript
Compass Learning: compasslearning/javascript-style-guide
DailyMotion: dailymotion/javascript
DoSomething: DoSomething/eslint-config
Digitpaint digitpaint/javascript
Drupal: www.drupal.org
Ecosia: ecosia/javascript
Evernote: evernote/javascript-style-guide
Evolution Gaming: evolution-gaming/javascript
EvozonJs: evozonjs/javascript
ExactTarget: ExactTarget/javascript
Expensify Expensify/Style-Guide
Flexberry: Flexberry/javascript-style-guide
Gawker Media: gawkermedia
General Electric: GeneralElectric/javascript
Generation Tux: GenerationTux/javascript
GoodData: gooddata/gdc-js-style
GreenChef: greenchef/javascript
Grooveshark: grooveshark/javascript
Grupo-Abraxas: Grupo-Abraxas/javascript
Happeo: happeo/javascript
Honey: honeyscience/javascript
How About We: howaboutwe/javascript
Huballin: huballin
HubSpot: HubSpot/javascript
Hyper: hyperoslo/javascript-playbook
InterCity Group: intercitygroup/javascript-style-guide
Jam3: Jam3/Javascript-Code-Conventions
JeopardyBot: kesne/jeopardy-bot
JSSolutions: JSSolutions/javascript
Kaplan Komputing: kaplankomputing/javascript
KickorStick: kickorstick
Kinetica Solutions: kinetica/javascript
LEINWAND: LEINWAND/javascript
Lonely Planet: lonelyplanet/javascript
M2GEN: M2GEN/javascript
Mighty Spring: mightyspring/javascript
MinnPost: MinnPost/javascript
MitocGroup: MitocGroup/javascript
ModCloth: modcloth/javascript
Money Advice Service: moneyadviceservice/javascript
Muber: muber
National Geographic: natgeo
Nimbl3: nimbl3/javascript
NullDev: NullDevCo/JavaScript-Styleguide
Nulogy: nulogy/javascript
Orange Hill Development: orangehill/javascript
Orion Health: orionhealth/javascript
OutBoxSoft: OutBoxSoft/javascript
Peerby: Peerby/javascript
Pier 1: Pier1/javascript
Qotto: Qotto/javascript-style-guide
Razorfish: razorfish/javascript-style-guide
reddit: reddit/styleguide/javascript
React: facebook.github.io/react/contributing/how-to-contribute.html#style-guide
REI: reidev/js-style-guide
Ripple: ripple/javascript-style-guide
Sainsbury’s Supermarkets: jsainsburyplc
SeekingAlpha: seekingalpha/javascript-style-guide
Shutterfly: shutterfly/javascript
Sourcetoad: sourcetoad/javascript
Springload: springload
StratoDem Analytics: stratodem/javascript
SteelKiwi Development: steelkiwi/javascript
StudentSphere: studentsphere/javascript
SwoopApp: swoopapp/javascript
SysGarage: sysgarage/javascript-style-guide
Syzygy Warsaw: syzygypl/javascript
Target: target/javascript
Terra: terra
TheLadders: TheLadders/javascript
The Nerdery: thenerdery/javascript-standards
Tomify: tomprats
Traitify: traitify/eslint-config-traitify
T4R Technology: T4R-Technology/javascript
UrbanSim: urbansim
VoxFeed: VoxFeed/javascript-style-guide
WeBox Studio: weboxstudio/javascript
Weggo: Weggo/javascript
Zillow: zillow/javascript
ZocDoc: ZocDoc/javascript

⬆ back to top
Translation
This style guide is also available in other languages:

 Brazilian Portuguese: armoucar/javascript-style-guide
 Bulgarian: borislavvv/javascript
 Catalan: fpmweb/javascript-style-guide
 Chinese (Simplified): lin-123/javascript
 Chinese (Traditional): jigsawye/javascript
 French: nmussy/javascript-style-guide
 German: timofurrer/javascript-style-guide
 Italian: sinkswim/javascript-style-guide
 Japanese: mitsuruog/javascript-style-guide
 Korean: ParkSB/javascript-style-guide
 Russian: leonidlebedev/javascript-airbnb
 Spanish: paolocarrasco/javascript-style-guide
 Thai: lvarayut/javascript-style-guide
 Turkish: eraycetinay/javascript
 Ukrainian: ivanzusko/javascript
 Vietnam: dangkyokhoang/javascript-style-guide

The JavaScript Style Guide Guide

Reference

Chat With Us About JavaScript

Find us on gitter.

Contributors

View Contributors

License
(The MIT License)
Copyright (c) 2012 Airbnb
Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
⬆ back to top
Amendments
We encourage you to fork this guide and change the rules to fit your team’s style guide. Below, you may list some amendments to the style guide. This allows you to periodically update your style guide without having to deal with merge conflicts.
};
"
5,JavaScript,"D3: Data-Driven Documents

D3 (or D3.js) is a JavaScript library for visualizing data using web standards. D3 helps you bring data to life using SVG, Canvas and HTML. D3 combines powerful visualization and interaction techniques with a data-driven approach to DOM manipulation, giving you the full capabilities of modern browsers and the freedom to design the right visual interface for your data.
Resources

API Reference
Release Notes
Gallery
Examples
Wiki

Installing
If you use npm, npm install d3. Otherwise, download the latest release. The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from d3js.org, CDNJS, or unpkg. For example:
<script src=""https://d3js.org/d3.v5.js""></script>
For the minified version:
<script src=""https://d3js.org/d3.v5.min.js""></script>
You can also use the standalone D3 microlibraries. For example, d3-selection:
<script src=""https://d3js.org/d3-selection.v1.js""></script>
D3 is written using ES2015 modules. Create a custom bundle using Rollup, Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:
import {scaleLinear} from ""d3-scale"";
Or import everything into a namespace (here, d3):
import * as d3 from ""d3"";
In Node:
var d3 = require(""d3"");
You can also require individual modules and combine them into a d3 object using Object.assign:
var d3 = Object.assign({}, require(""d3-format""), require(""d3-geo""), require(""d3-geo-projection""));
"
6,JavaScript,"

    React Native
  


Learn once, write anywhere:
  Build mobile apps with React.






















Getting Started
 · 
Learn the Basics
 · 
Showcase
 · 
Contribute
 · 
Community
 · 
Support

React Native brings React's declarative UI framework to iOS and Android. With React Native, you use native UI controls and have full access to the native platform.

Declarative. React makes it painless to create interactive UIs. Declarative views make your code more predictable and easier to debug.
Component-Based. Build encapsulated components that manage their state, then compose them to make complex UIs.
Developer Velocity. See local changes in seconds. Changes to JavaScript code can be live reloaded without rebuilding the native app.
Portability. Reuse code across iOS, Android, and other platforms.

React Native is developed and supported by many companies and individual core contributors. Find out more in our ecosystem overview.
Contents

Requirements
Building your first React Native app
Documentation
Upgrading
How to Contribute
Code of Conduct
License

📋 Requirements
React Native apps may target iOS 9.0 and Android 4.1 (API 16) or newer. You may use Windows, macOS, or Linux as your development operating system, though building and running iOS apps is limited to macOS. Tools like Expo can be used to work around this.
🎉 Building your first React Native app
Follow the Getting Started guide. The recommended way to install React Native depends on your project. Here you can find short guides for the most common scenarios:

Trying out React Native
Creating a New Application
Adding React Native to an Existing Application

📖 Documentation
The full documentation for React Native can be found on our website.
The React Native documentation discusses components, APIs, and topics that are specific to React Native. For further documentation on the React API that is shared between React Native and React DOM, refer to the React documentation.
The source for the React Native documentation and website is hosted on a separate repo, @facebook/react-native-website.
🚀 Upgrading
Upgrading to new versions of React Native may give you access to more APIs, views, developer tools, and other goodies. See the Upgrading Guide for instructions.
React Native releases are discussed in the React Native Community, @react-native-community/react-native-releases.
👏 How to Contribute
The main purpose of this repository is to continue evolving React Native core. We want to make contributing to this project as easy and transparent as possible, and we are grateful to the community for contributing bug fixes and improvements. Read below to learn how you can take part in improving React Native.
Code of Conduct
Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
Please read the full text so that you can understand what actions will and will not be tolerated.
Contributing Guide
Read our Contributing Guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React Native.
Open Source Roadmap
You can learn more about our vision for React Native in the Roadmap.
Good First Issues
We have a list of good first issues that contain bugs which have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.
Discussions
Larger discussions and proposals are discussed in @react-native-community/discussions-and-proposals.
📄 License
React Native is MIT licensed, as found in the LICENSE file.
React Native documentation is Creative Commons licensed, as found in the LICENSE-docs file.
"
7,JavaScript,"Create React App  
Create React apps with no build configuration.

Creating an App – How to create a new app.
User Guide – How to develop apps bootstrapped with Create React App.

Create React App works on macOS, Windows, and Linux.
If something doesn’t work, please file an issue.
If you have questions or need help, please ask in our Spectrum community.
Quick Overview
npx create-react-app my-app
cd my-app
npm start
(npx comes with npm 5.2+ and higher, see instructions for older npm versions)
Then open http://localhost:3000/ to see your app.
When you’re ready to deploy to production, create a minified bundle with npm run build.



Get Started Immediately
You don’t need to install or configure tools like Webpack or Babel.
They are preconfigured and hidden so that you can focus on the code.
Create a project, and you’re good to go.
Creating an App
You’ll need to have Node 8.16.0 or Node 10.16.0 or later version on your local development machine (but it’s not required on the server). You can use nvm (macOS/Linux) or nvm-windows to switch Node versions between different projects.
To create a new app, you may choose one of the following methods:
npx
npx create-react-app my-app
(npx is a package runner tool that comes with npm 5.2+ and higher, see instructions for older npm versions)
npm
npm init react-app my-app
npm init <initializer> is available in npm 6+
Yarn
yarn create react-app my-app
yarn create is available in Yarn 0.25+
It will create a directory called my-app inside the current folder.
Inside that directory, it will generate the initial project structure and install the transitive dependencies:
my-app
├── README.md
├── node_modules
├── package.json
├── .gitignore
├── public
│   ├── favicon.ico
│   ├── index.html
│   └── manifest.json
└── src
    ├── App.css
    ├── App.js
    ├── App.test.js
    ├── index.css
    ├── index.js
    ├── logo.svg
    └── serviceWorker.js

No configuration or complicated folder structures, only the files you need to build your app.
Once the installation is done, you can open your project folder:
cd my-app
Inside the newly created project, you can run some built-in commands:
npm start or yarn start
Runs the app in development mode.
Open http://localhost:3000 to view it in the browser.
The page will automatically reload if you make changes to the code.
You will see the build errors and lint warnings in the console.



npm test or yarn test
Runs the test watcher in an interactive mode.
By default, runs tests related to files changed since the last commit.
Read more about testing.
npm run build or yarn build
Builds the app for production to the build folder.
It correctly bundles React in production mode and optimizes the build for the best performance.
The build is minified and the filenames include the hashes.
Your app is ready to be deployed.
User Guide
You can find detailed instructions on using Create React App and many tips in its documentation.
How to Update to New Versions?
Please refer to the User Guide for this and other information.
Philosophy


One Dependency: There is only one build dependency. It uses Webpack, Babel, ESLint, and other amazing projects, but provides a cohesive curated experience on top of them.


No Configuration Required: You don't need to configure anything. A reasonably good configuration of both development and production builds is handled for you so you can focus on writing code.


No Lock-In: You can “eject” to a custom setup at any time. Run a single command, and all the configuration and build dependencies will be moved directly into your project, so you can pick up right where you left off.


What’s Included?
Your environment will have everything you need to build a modern single-page React app:

React, JSX, ES6, TypeScript and Flow syntax support.
Language extras beyond ES6 like the object spread operator.
Autoprefixed CSS, so you don’t need -webkit- or other prefixes.
A fast interactive unit test runner with built-in support for coverage reporting.
A live development server that warns about common mistakes.
A build script to bundle JS, CSS, and images for production, with hashes and sourcemaps.
An offline-first service worker and a web app manifest, meeting all the Progressive Web App criteria. (Note: Using the service worker is opt-in as of react-scripts@2.0.0 and higher)
Hassle-free updates for the above tools with a single dependency.

Check out this guide for an overview of how these tools fit together.
The tradeoff is that these tools are preconfigured to work in a specific way. If your project needs more customization, you can ""eject"" and customize it, but then you will need to maintain this configuration.
Popular Alternatives
Create React App is a great fit for:

Learning React in a comfortable and feature-rich development environment.
Starting new single-page React applications.
Creating examples with React for your libraries and components.

Here are a few common cases where you might want to try something else:


If you want to try React without hundreds of transitive build tool dependencies, consider using a single HTML file or an online sandbox instead.


If you need to integrate React code with a server-side template framework like Rails, Django or Symfony, or if you’re not building a single-page app, consider using nwb, or Neutrino which are more flexible. For Rails specifically, you can use Rails Webpacker. For Symfony, try Symfony's Webpack Encore.


If you need to publish a React component, nwb can also do this, as well as Neutrino's react-components preset.


If you want to do server rendering with React and Node.js, check out Next.js or Razzle. Create React App is agnostic of the backend, and only produces static HTML/JS/CSS bundles.


If your website is mostly static (for example, a portfolio or a blog), consider using Gatsby instead. Unlike Create React App, it pre-renders the website into HTML at the build time.


Finally, if you need more customization, check out Neutrino and its React preset.


All of the above tools can work with little to no configuration.
If you prefer configuring the build yourself, follow this guide.
React Native
Looking for something similar, but for React Native?
Check out Expo CLI.
Contributing
We'd love to have your helping hand on create-react-app! See CONTRIBUTING.md for more information on what we're looking for and how to get started.
Credits
This project exists thanks to all the people who contribute.

Acknowledgements
We are grateful to the authors of existing related projects for their ideas and collaboration:

@eanplatter
@insin
@mxstbr

License
Create React App is open source software licensed as MIT.
"
8,JavaScript,"axios







Promise based HTTP client for the browser and node.js
Features

Make XMLHttpRequests from the browser
Make http requests from node.js
Supports the Promise API
Intercept request and response
Transform request and response data
Cancel requests
Automatic transforms for JSON data
Client side support for protecting against XSRF

Browser Support













Latest ✔
Latest ✔
Latest ✔
Latest ✔
Latest ✔
11 ✔




Installing
Using npm:
$ npm install axios
Using bower:
$ bower install axios
Using yarn:
$ yarn add axios
Using cdn:
<script src=""https://unpkg.com/axios/dist/axios.min.js""></script>
Example
note: CommonJS usage
In order to gain the TypeScript typings (for intellisense / autocomplete) while using CommonJS imports with require() use the following approach:
const axios = require('axios').default;

// axios.<method> will now provide autocomplete and parameter typings
Performing a GET request
const axios = require('axios');

// Make a request for a user with a given ID
axios.get('/user?ID=12345')
  .then(function (response) {
    // handle success
    console.log(response);
  })
  .catch(function (error) {
    // handle error
    console.log(error);
  })
  .finally(function () {
    // always executed
  });

// Optionally the request above could also be done as
axios.get('/user', {
    params: {
      ID: 12345
    }
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  })
  .finally(function () {
    // always executed
  });  

// Want to use async/await? Add the `async` keyword to your outer function/method.
async function getUser() {
  try {
    const response = await axios.get('/user?ID=12345');
    console.log(response);
  } catch (error) {
    console.error(error);
  }
}

NOTE: async/await is part of ECMAScript 2017 and is not supported in Internet
Explorer and older browsers, so use with caution.

Performing a POST request
axios.post('/user', {
    firstName: 'Fred',
    lastName: 'Flintstone'
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  });
Performing multiple concurrent requests
function getUserAccount() {
  return axios.get('/user/12345');
}

function getUserPermissions() {
  return axios.get('/user/12345/permissions');
}

axios.all([getUserAccount(), getUserPermissions()])
  .then(axios.spread(function (acct, perms) {
    // Both requests are now complete
  }));
axios API
Requests can be made by passing the relevant config to axios.
axios(config)
// Send a POST request
axios({
  method: 'post',
  url: '/user/12345',
  data: {
    firstName: 'Fred',
    lastName: 'Flintstone'
  }
});
// GET request for remote image
axios({
  method: 'get',
  url: 'http://bit.ly/2mTM3nY',
  responseType: 'stream'
})
  .then(function (response) {
    response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))
  });
axios(url[, config])
// Send a GET request (default method)
axios('/user/12345');
Request method aliases
For convenience aliases have been provided for all supported request methods.
axios.request(config)
axios.get(url[, config])
axios.delete(url[, config])
axios.head(url[, config])
axios.options(url[, config])
axios.post(url[, data[, config]])
axios.put(url[, data[, config]])
axios.patch(url[, data[, config]])
NOTE
When using the alias methods url, method, and data properties don't need to be specified in config.
Concurrency
Helper functions for dealing with concurrent requests.
axios.all(iterable)
axios.spread(callback)
Creating an instance
You can create a new instance of axios with a custom config.
axios.create([config])
const instance = axios.create({
  baseURL: 'https://some-domain.com/api/',
  timeout: 1000,
  headers: {'X-Custom-Header': 'foobar'}
});
Instance methods
The available instance methods are listed below. The specified config will be merged with the instance config.
axios#request(config)
axios#get(url[, config])
axios#delete(url[, config])
axios#head(url[, config])
axios#options(url[, config])
axios#post(url[, data[, config]])
axios#put(url[, data[, config]])
axios#patch(url[, data[, config]])
axios#getUri([config])
Request Config
These are the available config options for making requests. Only the url is required. Requests will default to GET if method is not specified.
{
  // `url` is the server URL that will be used for the request
  url: '/user',

  // `method` is the request method to be used when making the request
  method: 'get', // default

  // `baseURL` will be prepended to `url` unless `url` is absolute.
  // It can be convenient to set `baseURL` for an instance of axios to pass relative URLs
  // to methods of that instance.
  baseURL: 'https://some-domain.com/api/',

  // `transformRequest` allows changes to the request data before it is sent to the server
  // This is only applicable for request methods 'PUT', 'POST', 'PATCH' and 'DELETE'
  // The last function in the array must return a string or an instance of Buffer, ArrayBuffer,
  // FormData or Stream
  // You may modify the headers object.
  transformRequest: [function (data, headers) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `transformResponse` allows changes to the response data to be made before
  // it is passed to then/catch
  transformResponse: [function (data) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `headers` are custom headers to be sent
  headers: {'X-Requested-With': 'XMLHttpRequest'},

  // `params` are the URL parameters to be sent with the request
  // Must be a plain object or a URLSearchParams object
  params: {
    ID: 12345
  },

  // `paramsSerializer` is an optional function in charge of serializing `params`
  // (e.g. https://www.npmjs.com/package/qs, http://api.jquery.com/jquery.param/)
  paramsSerializer: function (params) {
    return Qs.stringify(params, {arrayFormat: 'brackets'})
  },

  // `data` is the data to be sent as the request body
  // Only applicable for request methods 'PUT', 'POST', and 'PATCH'
  // When no `transformRequest` is set, must be of one of the following types:
  // - string, plain object, ArrayBuffer, ArrayBufferView, URLSearchParams
  // - Browser only: FormData, File, Blob
  // - Node only: Stream, Buffer
  data: {
    firstName: 'Fred'
  },
  
  // syntax alternative to send data into the body
  // method post
  // only the value is sent, not the key
  data: 'Country=Brasil&City=Belo Horizonte',

  // `timeout` specifies the number of milliseconds before the request times out.
  // If the request takes longer than `timeout`, the request will be aborted.
  timeout: 1000, // default is `0` (no timeout)

  // `withCredentials` indicates whether or not cross-site Access-Control requests
  // should be made using credentials
  withCredentials: false, // default

  // `adapter` allows custom handling of requests which makes testing easier.
  // Return a promise and supply a valid response (see lib/adapters/README.md).
  adapter: function (config) {
    /* ... */
  },

  // `auth` indicates that HTTP Basic auth should be used, and supplies credentials.
  // This will set an `Authorization` header, overwriting any existing
  // `Authorization` custom headers you have set using `headers`.
  // Please note that only HTTP Basic auth is configurable through this parameter.
  // For Bearer tokens and such, use `Authorization` custom headers instead.
  auth: {
    username: 'janedoe',
    password: 's00pers3cret'
  },

  // `responseType` indicates the type of data that the server will respond with
  // options are: 'arraybuffer', 'document', 'json', 'text', 'stream'
  //   browser only: 'blob'
  responseType: 'json', // default

  // `responseEncoding` indicates encoding to use for decoding responses
  // Note: Ignored for `responseType` of 'stream' or client-side requests
  responseEncoding: 'utf8', // default

  // `xsrfCookieName` is the name of the cookie to use as a value for xsrf token
  xsrfCookieName: 'XSRF-TOKEN', // default

  // `xsrfHeaderName` is the name of the http header that carries the xsrf token value
  xsrfHeaderName: 'X-XSRF-TOKEN', // default

  // `onUploadProgress` allows handling of progress events for uploads
  onUploadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `onDownloadProgress` allows handling of progress events for downloads
  onDownloadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `maxContentLength` defines the max size of the http response content in bytes allowed
  maxContentLength: 2000,

  // `validateStatus` defines whether to resolve or reject the promise for a given
  // HTTP response status code. If `validateStatus` returns `true` (or is set to `null`
  // or `undefined`), the promise will be resolved; otherwise, the promise will be
  // rejected.
  validateStatus: function (status) {
    return status >= 200 && status < 300; // default
  },

  // `maxRedirects` defines the maximum number of redirects to follow in node.js.
  // If set to 0, no redirects will be followed.
  maxRedirects: 5, // default

  // `socketPath` defines a UNIX Socket to be used in node.js.
  // e.g. '/var/run/docker.sock' to send requests to the docker daemon.
  // Only either `socketPath` or `proxy` can be specified.
  // If both are specified, `socketPath` is used.
  socketPath: null, // default

  // `httpAgent` and `httpsAgent` define a custom agent to be used when performing http
  // and https requests, respectively, in node.js. This allows options to be added like
  // `keepAlive` that are not enabled by default.
  httpAgent: new http.Agent({ keepAlive: true }),
  httpsAgent: new https.Agent({ keepAlive: true }),

  // 'proxy' defines the hostname and port of the proxy server.
  // You can also define your proxy using the conventional `http_proxy` and
  // `https_proxy` environment variables. If you are using environment variables
  // for your proxy configuration, you can also define a `no_proxy` environment
  // variable as a comma-separated list of domains that should not be proxied.
  // Use `false` to disable proxies, ignoring environment variables.
  // `auth` indicates that HTTP Basic auth should be used to connect to the proxy, and
  // supplies credentials.
  // This will set an `Proxy-Authorization` header, overwriting any existing
  // `Proxy-Authorization` custom headers you have set using `headers`.
  proxy: {
    host: '127.0.0.1',
    port: 9000,
    auth: {
      username: 'mikeymike',
      password: 'rapunz3l'
    }
  },

  // `cancelToken` specifies a cancel token that can be used to cancel the request
  // (see Cancellation section below for details)
  cancelToken: new CancelToken(function (cancel) {
  })
}
Response Schema
The response for a request contains the following information.
{
  // `data` is the response that was provided by the server
  data: {},

  // `status` is the HTTP status code from the server response
  status: 200,

  // `statusText` is the HTTP status message from the server response
  statusText: 'OK',

  // `headers` the headers that the server responded with
  // All header names are lower cased
  headers: {},

  // `config` is the config that was provided to `axios` for the request
  config: {},

  // `request` is the request that generated this response
  // It is the last ClientRequest instance in node.js (in redirects)
  // and an XMLHttpRequest instance in the browser
  request: {}
}
When using then, you will receive the response as follows:
axios.get('/user/12345')
  .then(function (response) {
    console.log(response.data);
    console.log(response.status);
    console.log(response.statusText);
    console.log(response.headers);
    console.log(response.config);
  });
When using catch, or passing a rejection callback as second parameter of then, the response will be available through the error object as explained in the Handling Errors section.
Config Defaults
You can specify config defaults that will be applied to every request.
Global axios defaults
axios.defaults.baseURL = 'https://api.example.com';
axios.defaults.headers.common['Authorization'] = AUTH_TOKEN;
axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded';
Custom instance defaults
// Set config defaults when creating the instance
const instance = axios.create({
  baseURL: 'https://api.example.com'
});

// Alter defaults after instance has been created
instance.defaults.headers.common['Authorization'] = AUTH_TOKEN;
Config order of precedence
Config will be merged with an order of precedence. The order is library defaults found in lib/defaults.js, then defaults property of the instance, and finally config argument for the request. The latter will take precedence over the former. Here's an example.
// Create an instance using the config defaults provided by the library
// At this point the timeout config value is `0` as is the default for the library
const instance = axios.create();

// Override timeout default for the library
// Now all requests using this instance will wait 2.5 seconds before timing out
instance.defaults.timeout = 2500;

// Override timeout for this request as it's known to take a long time
instance.get('/longRequest', {
  timeout: 5000
});
Interceptors
You can intercept requests or responses before they are handled by then or catch.
// Add a request interceptor
axios.interceptors.request.use(function (config) {
    // Do something before request is sent
    return config;
  }, function (error) {
    // Do something with request error
    return Promise.reject(error);
  });

// Add a response interceptor
axios.interceptors.response.use(function (response) {
    // Any status code that lie within the range of 2xx cause this function to trigger
    // Do something with response data
    return response;
  }, function (error) {
    // Any status codes that falls outside the range of 2xx cause this function to trigger
    // Do something with response error
    return Promise.reject(error);
  });
If you need to remove an interceptor later you can.
const myInterceptor = axios.interceptors.request.use(function () {/*...*/});
axios.interceptors.request.eject(myInterceptor);
You can add interceptors to a custom instance of axios.
const instance = axios.create();
instance.interceptors.request.use(function () {/*...*/});
Handling Errors
axios.get('/user/12345')
  .catch(function (error) {
    if (error.response) {
      // The request was made and the server responded with a status code
      // that falls out of the range of 2xx
      console.log(error.response.data);
      console.log(error.response.status);
      console.log(error.response.headers);
    } else if (error.request) {
      // The request was made but no response was received
      // `error.request` is an instance of XMLHttpRequest in the browser and an instance of
      // http.ClientRequest in node.js
      console.log(error.request);
    } else {
      // Something happened in setting up the request that triggered an Error
      console.log('Error', error.message);
    }
    console.log(error.config);
  });
Using the validateStatus config option, you can define HTTP code(s) that should throw an error.
axios.get('/user/12345', {
  validateStatus: function (status) {
    return status < 500; // Reject only if the status code is greater than or equal to 500
  }
})
Using toJSON you get an object with more information about the HTTP error.
axios.get('/user/12345')
  .catch(function (error) {
    console.log(error.toJSON());
  });
Cancellation
You can cancel a request using a cancel token.

The axios cancel token API is based on the withdrawn cancelable promises proposal.

You can create a cancel token using the CancelToken.source factory as shown below:
const CancelToken = axios.CancelToken;
const source = CancelToken.source();

axios.get('/user/12345', {
  cancelToken: source.token
}).catch(function (thrown) {
  if (axios.isCancel(thrown)) {
    console.log('Request canceled', thrown.message);
  } else {
    // handle error
  }
});

axios.post('/user/12345', {
  name: 'new name'
}, {
  cancelToken: source.token
})

// cancel the request (the message parameter is optional)
source.cancel('Operation canceled by the user.');
You can also create a cancel token by passing an executor function to the CancelToken constructor:
const CancelToken = axios.CancelToken;
let cancel;

axios.get('/user/12345', {
  cancelToken: new CancelToken(function executor(c) {
    // An executor function receives a cancel function as a parameter
    cancel = c;
  })
});

// cancel the request
cancel();

Note: you can cancel several requests with the same cancel token.

Using application/x-www-form-urlencoded format
By default, axios serializes JavaScript objects to JSON. To send data in the application/x-www-form-urlencoded format instead, you can use one of the following options.
Browser
In a browser, you can use the URLSearchParams API as follows:
const params = new URLSearchParams();
params.append('param1', 'value1');
params.append('param2', 'value2');
axios.post('/foo', params);

Note that URLSearchParams is not supported by all browsers (see caniuse.com), but there is a polyfill available (make sure to polyfill the global environment).

Alternatively, you can encode data using the qs library:
const qs = require('qs');
axios.post('/foo', qs.stringify({ 'bar': 123 }));
Or in another way (ES6),
import qs from 'qs';
const data = { 'bar': 123 };
const options = {
  method: 'POST',
  headers: { 'content-type': 'application/x-www-form-urlencoded' },
  data: qs.stringify(data),
  url,
};
axios(options);
Node.js
In node.js, you can use the querystring module as follows:
const querystring = require('querystring');
axios.post('http://something.com/', querystring.stringify({ foo: 'bar' }));
You can also use the qs library.
NOTE
The qs library is preferable if you need to stringify nested objects, as the querystring method has known issues with that use case (https://github.com/nodejs/node-v0.x-archive/issues/1665).
Semver
Until axios reaches a 1.0 release, breaking changes will be released with a new minor version. For example 0.5.1, and 0.5.4 will have the same API, but 0.6.0 will have breaking changes.
Promises
axios depends on a native ES6 Promise implementation to be supported.
If your environment doesn't support ES6 Promises, you can polyfill.
TypeScript
axios includes TypeScript definitions.
import axios from 'axios';
axios.get('/user?ID=12345');
Resources

Changelog
Upgrade Guide
Ecosystem
Contributing Guide
Code of Conduct

Credits
axios is heavily inspired by the $http service provided in Angular. Ultimately axios is an effort to provide a standalone $http-like service for use outside of Angular.
License
MIT
"
9,JavaScript,"




Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. For
more information on using Node.js, see the Node.js Website.
The Node.js project uses an open governance model. The
OpenJS Foundation provides support for the project.
This project is bound by a Code of Conduct.
Table of Contents

Support
Release Types

Download

Current and LTS Releases
Nightly Releases
API Documentation


Verifying Binaries


Building Node.js
Security
Contributing to Node.js
Current Project Team Members

TSC (Technical Steering Committee)
Collaborators
Release Keys



Support
Looking for help? Check out the
instructions for getting support.
Release Types

Current: Under active development. Code for the Current release is in the
branch for its major version number (for example,
v10.x). Node.js releases a new
major version every 6 months, allowing for breaking changes. This happens in
April and October every year. Releases appearing each October have a support
life of 8 months. Releases appearing each April convert to LTS (see below)
each October.
LTS: Releases that receive Long-term Support, with a focus on stability
and security. Every even-numbered major version will become an LTS release.
LTS releases receive 12 months of Active LTS support and a further 18 months
of Maintenance. LTS release lines have alphabetically-ordered codenames,
beginning with v4 Argon. There are no breaking changes or feature additions,
except in some special circumstances.
Nightly: Code from the Current branch built every 24-hours when there are
changes. Use with caution.

Current and LTS releases follow Semantic Versioning. A
member of the Release Team signs each Current and LTS release.
For more information, see the
Release README.
Download
Binaries, installers, and source tarballs are available at
https://nodejs.org/en/download/.
Current and LTS Releases
https://nodejs.org/download/release/
The latest directory is an
alias for the latest Current release. The latest-codename directory is an
alias for the latest release from an LTS line. For example, the
latest-carbon directory
contains the latest Carbon (Node.js 8) release.
Nightly Releases
https://nodejs.org/download/nightly/
Each directory name and filename contains a date (in UTC time) and the commit
SHA at the HEAD of the release.
API Documentation
Documentation for the latest Current release is at https://nodejs.org/api/.
Version-specific documentation is available in each release directory in the
docs subdirectory. Version-specific documentation is also at
https://nodejs.org/download/docs/.
Verifying Binaries
Download directories contain a SHASUMS256.txt file with SHA checksums for the
files.
To download SHASUMS256.txt using curl:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt
To check that a downloaded file matches the checksum, run
it through sha256sum with a command such as:
$ grep node-vx.y.z.tar.gz SHASUMS256.txt | sha256sum -c -
For Current and LTS, the GPG detached signature of SHASUMS256.txt is in
SHASUMS256.txt.sig. You can use it with gpg to verify the integrity of
SHASUM256.txt. You will first need to import
the GPG keys of individuals authorized to create releases. To
import the keys:
$ gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
See the bottom of this README for a full script to import active release keys.
Next, download the SHASUMS256.txt.sig for the release:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt.sig
Then use gpg --verify SHASUMS256.txt.sig SHASUMS256.txt to verify
the file's signature.
Building Node.js
See BUILDING.md for instructions on how to build Node.js from
source and a list of supported platforms.
Security
For information on reporting security vulnerabilities in Node.js, see
SECURITY.md.
Contributing to Node.js

Contributing to the project
Working Groups
Strategic Initiatives

Current Project Team Members
For information about the governance of the Node.js project, see
GOVERNANCE.md.
TSC (Technical Steering Committee)

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
BethGriggs -
Beth Griggs <Bethany.Griggs@uk.ibm.com> (she/her)
ChALkeR -
Сковорода Никита Андреевич <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
sam-github -
Sam Roberts <vieuxtech@gmail.com>
targos -
Michaël Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
tniessen -
Tobias Nießen <tniessen@tnie.de>
Trott -
Rich Trott <rtrott@gmail.com> (he/him)

TSC Emeriti

bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
isaacs -
Isaac Z. Schlueter <i@izs.me>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
mscdex -
Brian White <mscdex@mscdex.net>
nebrius -
Bryan Hughes <bryan@nebri.us>
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
rvagg -
Rod Vagg <r@va.gg>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
trevnorris -
Trevor Norris <trev.norris@gmail.com>

Collaborators

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
ak239 -
Aleksei Koziatinskii <ak239spb@gmail.com>
AndreasMadsen -
Andreas Madsen <amwebdk@gmail.com> (he/him)
antsmartian -
Anto Aravinth <anto.aravinth.cse@gmail.com> (he/him)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
aqrln -
Alexey Orlenko <eaglexrlnk@gmail.com> (he/him)
bcoe -
Ben Coe <bencoe@gmail.com> (he/him)
bengl -
Bryan English <bryan@bryanenglish.com> (he/him)
benjamingr -
Benjamin Gruenbaum <benjamingr@gmail.com>
BethGriggs -
Beth Griggs <Bethany.Griggs@uk.ibm.com> (she/her)
bmeck -
Bradley Farias <bradley.meck@gmail.com>
bmeurer -
Benedikt Meurer <benedikt.meurer@gmail.com>
bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
boneskull -
Christopher Hiller <boneskull@boneskull.com> (he/him)
BridgeAR -
Ruben Bridgewater <ruben@bridgewater.de> (he/him)
bzoz -
Bartosz Sosnowski <bartosz@janeasystems.com>
calvinmetcalf -
Calvin Metcalf <calvin.metcalf@gmail.com>
cclauss -
Christian Clauss <cclauss@me.com> (he/him)
ChALkeR -
Сковорода Никита Андреевич <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
claudiorodriguez -
Claudio Rodriguez <cjrodr@yahoo.com>
codebytere -
Shelley Vohr <codebytere@gmail.com> (she/her)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
davisjam -
Jamie Davis <davisjam@vt.edu> (he/him)
devnexen -
David Carlier <devnexen@gmail.com>
devsnek -
Gus Caplan <me@gus.host> (he/him)
digitalinfinity -
Hitesh Kanwathirtha <digitalinfinity@gmail.com> (he/him)
edsadr -
Adrian Estrada <edsadr@gmail.com> (he/him)
eljefedelrodeodeljefe -
Robert Jefe Lindstaedt <robert.lindstaedt@gmail.com>
eugeneo -
Eugene Ostroukhov <eostroukhov@google.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gdams -
George Adams <george.adams@uk.ibm.com> (he/him)
geek -
Wyatt Preul <wpreul@gmail.com>
gengjiawen -
Jiawen Geng <technicalcute@gmail.com>
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
guybedford -
Guy Bedford <guybedford@gmail.com> (he/him)
hashseed -
Yang Guo <yangguo@chromium.org> (he/him)
hiroppy -
Yuta Hiroto <hello@hiroppy.me> (he/him)
iarna -
Rebecca Turner <me@re-becca.org>
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
italoacasas -
Italo A. Casas <me@italoacasas.com> (he/him)
JacksonTian -
Jackson Tian <shyvo1987@gmail.com>
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
jbergstroem -
Johan Bergström <bugs@bergstroem.nu>
jdalton -
John-David Dalton <john.david.dalton@gmail.com>
jkrems -
Jan Krems <jan.krems@gmail.com> (he/him)
joaocgreis -
João Reis <reis@janeasystems.com>
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
julianduque -
Julian Duque <julianduquej@gmail.com> (he/him)
JungMinu -
Minwoo Jung <nodecorelab@gmail.com> (he/him)
kfarnung -
Kyle Farnung <kfarnung@microsoft.com> (he/him)
lance -
Lance Ball <lball@redhat.com> (he/him)
legendecas -
Chengzhong Wu <legendecas@gmail.com> (he/him)
Leko -
Shingo Inoue <leko.noor@gmail.com> (he/him)
lpinca -
Luigi Pinca <luigipinca@gmail.com> (he/him)
lundibundi -
Denys Otrishko <shishugi@gmail.com> (he/him)
maclover7 -
Jon Moss <me@jonathanmoss.me> (he/him)
mafintosh
Mathias Buus <mathiasbuus@gmail.com> (he/him)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
misterdjules -
Julien Gilli <jgilli@nodejs.org>
mmarchini -
Matheus Marchini <mat@mmarchini.me>
MoonBall -
Chen Gang <gangc.cxy@foxmail.com>
mscdex -
Brian White <mscdex@mscdex.net>
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
not-an-aardvark -
Teddy Katz <teddy.katz@gmail.com> (he/him)
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
oyyd -
Ouyang Yadong <oyydoibh@gmail.com> (he/him)
princejwesley -
Prince John Wesley <princejohnwesley@gmail.com>
psmarshall -
Peter Marshall <petermarshall@chromium.org> (he/him)
Qard -
Stephen Belanger <admin@stephenbelanger.com> (he/him)
refack -
Refael Ackermann (רפאל פלחי) <refack@gmail.com> (he/him/הוא/אתה)
richardlau -
Richard Lau <riclau@uk.ibm.com>
ronkorving -
Ron Korving <ron@ronkorving.nl>
rubys -
Sam Ruby <rubys@intertwingly.net>
rvagg -
Rod Vagg <rod@vagg.org>
ryzokuken -
Ujjwal Sharma <usharma1998@gmail.com> (he/him)
saghul -
Saúl Ibarra Corretgé <saghul@gmail.com>
sam-github -
Sam Roberts <vieuxtech@gmail.com>
santigimeno -
Santiago Gimeno <santiago.gimeno@gmail.com>
sebdeckers -
Sebastiaan Deckers <sebdeckers83@gmail.com>
seishun -
Nikolai Vavilov <vvnicholas@gmail.com>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
shisama -
Masashi Hirano <shisama07@gmail.com> (he/him)
silverwind -
Roman Reiss <me@silverwind.io>
srl295 -
Steven R Loomis <srloomis@us.ibm.com>
starkwang -
Weijia Wang <starkwang@126.com>
targos -
Michaël Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
thekemkid -
Glen Keane <glenkeane.94@gmail.com> (he/him)
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
tniessen -
Tobias Nießen <tniessen@tnie.de>
trevnorris -
Trevor Norris <trev.norris@gmail.com>
trivikr -
Trivikram Kamat <trivikr.dev@gmail.com>
Trott -
Rich Trott <rtrott@gmail.com> (he/him)
vdeturckheim -
Vladimir de Turckheim <vlad2t@hotmail.com> (he/him)
vkurchatkin -
Vladimir Kurchatkin <vladimir.kurchatkin@gmail.com>
watilde -
Daijiro Wachi <daijiro.wachi@gmail.com> (he/him)
watson -
Thomas Watson <w@tson.dk>
XadillaX -
Khaidi Chu <i@2333.moe> (he/him)
yhwang -
Yihong Wang <yh.wang@ibm.com>
yorkie -
Yorkie Liu <yorkiefixer@gmail.com>
yosuke-furukawa -
Yosuke Furukawa <yosuke.furukawa@gmail.com>
ZYSzys -
Yongsheng Zhang <zyszys98@gmail.com> (he/him)

Collaborator Emeriti

andrasq -
Andras <andras@kinvey.com>
AnnaMag -
Anna M. Kedzierska <anna.m.kedzierska@gmail.com>
brendanashworth -
Brendan Ashworth <brendan.ashworth@me.com>
estliberitas -
Alexander Makarenko <estliberitas@gmail.com>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
DavidCai1993 -
David Cai <davidcai1993@yahoo.com> (he/him)
firedfox -
Daniel Wang <wangyang0123@gmail.com>
imran-iq -
Imran Iqbal <imran@imraniqbal.org>
imyller -
Ilkka Myller <ilkka.myller@nodefield.com>
isaacs -
Isaac Z. Schlueter <i@izs.me>
jasongin -
Jason Ginchereau <jasongin@microsoft.com>
jhamhader -
Yuval Brik <yuval@brik.org.il>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
kunalspathak -
Kunal Pathak <kunal.pathak@microsoft.com>
lucamaraschi -
Luca Maraschi <luca.maraschi@gmail.com> (he/him)
lxe -
Aleksey Smolenchuk <lxe@lxe.co>
matthewloring -
Matthew Loring <mattloring@google.com>
micnic -
Nicu Micleușanu <micnic90@gmail.com> (he/him)
mikeal -
Mikeal Rogers <mikeal.rogers@gmail.com>
monsanto -
Christopher Monsanto <chris@monsan.to>
Olegas -
Oleg Elifantiev <oleg@elifantiev.ru>
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
othiym23 -
Forrest L Norvell <ogd@aoaioxxysz.net> (he/him)
petkaantonov -
Petka Antonov <petka_antonov@hotmail.com>
phillipj -
Phillip Johnsen <johphi@gmail.com>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
pmq20 -
Minqi Pan <pmq2001@gmail.com>
rlidwka -
Alex Kocharin <alex@kocharin.ru>
rmg -
Ryan Graham <r.m.graham@gmail.com>
robertkowalski -
Robert Kowalski <rok@kowalski.gd>
romankl -
Roman Klauke <romaaan.git@gmail.com>
RReverser -
Ingvar Stepanyan <me@rreverser.com>
stefanmb -
Stefan Budeanu <stefan@budeanu.com>
tellnes -
Christian Tellnes <christian@tellnes.no>
thlorenz -
Thorsten Lorenz <thlorenz@gmx.de>
tunniclm -
Mike Tunnicliffe <m.j.tunnicliffe@gmail.com>
vsemozhetbyt -
Vse Mozhet Byt <vsemozhetbyt@gmail.com> (he/him)
whitlockjc -
Jeremy Whitlock <jwhitlock@apache.org>

Collaborators follow the COLLABORATOR_GUIDE.md in
maintaining the Node.js project.
Release Keys
GPG keys used to sign Node.js releases:

Beth Griggs <bethany.griggs@uk.ibm.com>
4ED778F539E3634C779C87C6D7062848A1AB005C
Colin Ihrig <cjihrig@gmail.com>
94AE36675C464D64BAFA68DD7434390BDBE9B9C5
Evan Lucas <evanlucas@me.com>
B9AE9905FFD7803F25714661B63B535A4C206CA9
Gibson Fahnestock <gibfahn@gmail.com>
77984A986EBC2AA786BC0F66B01FBB92821C587A
James M Snell <jasnell@keybase.io>
71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
Jeremiah Senkpiel <fishrock@keybase.io>
FD3A5288F042B6850C66B31F09FE44734EB7990E
Michaël Zasso <targos@protonmail.com>
8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
Myles Borins <myles.borins@gmail.com>
C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
Rod Vagg <rod@vagg.org>
DD8F2338BAE7501E3DD5AC78C273792F7D83545D
Ruben Bridgewater <ruben@bridgewater.de>
A48C2BEE680E841632CD4E44F07496B3EB3C1762
Shelley Vohr <shelley.vohr@gmail.com>
B9E2F5981AA6E0CD28160D9FF13993A75599653C

To import the full set of trusted release keys:
gpg --keyserver pool.sks-keyservers.net --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C
gpg --keyserver pool.sks-keyservers.net --recv-keys B9E2F5981AA6E0CD28160D9FF13993A75599653C
gpg --keyserver pool.sks-keyservers.net --recv-keys 94AE36675C464D64BAFA68DD7434390BDBE9B9C5
gpg --keyserver pool.sks-keyservers.net --recv-keys B9AE9905FFD7803F25714661B63B535A4C206CA9
gpg --keyserver pool.sks-keyservers.net --recv-keys 77984A986EBC2AA786BC0F66B01FBB92821C587A
gpg --keyserver pool.sks-keyservers.net --recv-keys 71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
gpg --keyserver pool.sks-keyservers.net --recv-keys FD3A5288F042B6850C66B31F09FE44734EB7990E
gpg --keyserver pool.sks-keyservers.net --recv-keys 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
gpg --keyserver pool.sks-keyservers.net --recv-keys C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
gpg --keyserver pool.sks-keyservers.net --recv-keys A48C2BEE680E841632CD4E44F07496B3EB3C1762
See the section above on Verifying Binaries for how to
use these keys to verify a downloaded file.
Other keys used to sign some previous releases:

Chris Dickinson <christopher.s.dickinson@gmail.com>
9554F04D7259F04124DE6B476D5A82AC7E37093B
Isaac Z. Schlueter <i@izs.me>
93C7E9E91B49E432C2F75674B0A78B0A6C481CF6
Italo A. Casas <me@italoacasas.com>
56730D5401028683275BD23C23EFEFE93C4CFFFE
Julien Gilli <jgilli@fastmail.fm>
114F43EE0176B71C7BC219DD50A3051F888C628D
Timothy J Fontaine <tjfontaine@gmail.com>
7937DFD2AB06298B2293C3187D33FF9D0246406D

"
10,JavaScript,"

Version 5 – the iconic SVG, font, and CSS framework

The internet's most popular icon toolkit has been redesigned and built from
scratch. On top of this, features like icon font ligatures, an SVG framework,
official NPM packages for popular frontend libraries like React, and access to
a new CDN.
Not familiar with Font Awesome 5? Learn
more about our
successful Kickstarter and plan. You can also order Font Awesome
Pro which includes tons more icons directly
from fontawesome.com.
Documentation
Learn how to get started with Font Awesome and then dive deeper into other and advanced topics:
Using Font Awesome on the Web

With SVG with JavaScript
With web fonts with CSS
Upgrading from version 4
Installing Font Awesome with a package manager
Downloading + hosting Font Awesome yourself
Performance and security
Accessibility
Troubleshooting

Advanced Options & Techniques

Using CSS pseudo-elements
SVG sprites
The Font Awesome API
SVG symbols
SVG JavaScript Core
Server side rendering

Using Font Awesome on the Desktop

Getting started
Upgrading from version 4
Using ligatures
Using glyphs
Troubleshooting

Where did Font Awesome 4 (or 3) go?
Now that Font Awesome 5 has been released we are marking version 4 as
end-of-life. We don't plan on releasing any further versions of the 4.x or 3.x.
Documentation is still available but it's moved to
https://fontawesome.com/v4.7.0 and
https://fontawesome.com/v3.2.1.
The Git repository for
v4.7.0 and
v3.2.1 can
be found in our GitHub releases.
Change log
We'll keep track of each release in the CHANGELOG.md
Looking for older versions of Font Awesome? Check the releases.
Upgrading
From time-to-time we'll have special upgrading instructions from one version to the next.
Check out the UPGRADING.md guide when you upgrade your dependencies.
Code of conduct
We will behave ourselves if you behave yourselves. For more details see our
CODE_OF_CONDUCT.md.
Contributing
Please read through our contributing guidelines.  Included
are directions for opening issues.
Versioning
Font Awesome will be maintained under the Semantic Versioning guidelines as much as possible. Releases will be numbered
with the following format:
<major>.<minor>.<patch>
For more information on SemVer, please visit http://semver.org.
The major version ""5"" is part of an umbrella release.  It includes many different types of files and technologies. Therefore
we deviate from normal SemVer in the following ways:

Any release may update the design, look-and-feel, or branding of an existing
icon
We will never intentionally release a patch version update that breaks
backward compatibility
A minor release may include backward-incompatible changes but we will
write clear upgrading instructions in UPGRADING.md
A minor or patch release will never remove icons
Bug fixes will be addressed as patch releases unless they include backward
incompatibility then they will be minor releases

License
Font Awesome Free is free, open source, and GPL friendly. You can use it for
commercial projects, open source projects, or really almost whatever you want.

Icons — CC BY 4.0 License

In the Font Awesome Free download, the CC BY 4.0 license applies to all icons packaged as .svg and .js files types.


Fonts — SIL OFL 1.1 License

In the Font Awesome Free download, the SIL OLF license applies to all icons packaged as web and desktop font files.


Code — MIT License

In the Font Awesome Free download, the MIT license applies to all non-font and non-icon files.



Attribution is required by MIT, SIL OLF, and CC BY licenses. Downloaded Font
Awesome Free files already contain embedded comments with sufficient
attribution, so you shouldn't need to do anything additional when using these
files normally.
We've kept attribution comments terse, so we ask that you do not actively work
to remove them from files, especially code. They're a great way for folks to
learn about Font Awesome.
Team

Dave Gandy
Travis Chase
Rob Madole
Brian Talbot
Jory Raphael
Mike Wilkerson
Trevor Chase
Jason Lundien
Jason Otero
Edward Emanuel
Geremia Taglialatela

"
11,JavaScript,"AngularJS 
AngularJS lets you write client-side web applications as if you had a smarter browser.  It lets you
use good old HTML (or HAML, Jade/Pug and friends!) as your template language and lets you extend HTML’s
syntax to express your application’s components clearly and succinctly.  It automatically
synchronizes data from your UI (view) with your JavaScript objects (model) through 2-way data
binding. To help you structure your application better and make it easy to test, AngularJS teaches
the browser how to do dependency injection and inversion of control.
It also helps with server-side communication, taming async callbacks with promises and deferred objects,
and it makes client-side navigation and deep linking with hashbang urls or HTML5 pushState a
piece of cake. Best of all? It makes development fun!

On July 1, 2018 AngularJS entered a 3 year Long Term Support period: Find out more
Looking for the new Angular? Go here: https://github.com/angular/angular


Web site: https://angularjs.org
Tutorial: https://docs.angularjs.org/tutorial
API Docs: https://docs.angularjs.org/api
Developer Guide: https://docs.angularjs.org/guide
Contribution guidelines: CONTRIBUTING.md
Core Development: DEVELOPERS.md
Dashboard: https://dashboard.angularjs.org

Documentation
Go to https://docs.angularjs.org
Contribute
We've set up a separate document for our
contribution guidelines.
Develop
We've set up a separate document for
developers.

What to use AngularJS for and when to use it
AngularJS is the next generation framework where each component is designed to work with every other
component in an interconnected way like a well-oiled machine. AngularJS is JavaScript MVC made easy
and done right. (Well it is not really MVC, read on, to understand what this means.)
MVC, no, MV* done the right way!
MVC, short for Model-View-Controller, is a design pattern, i.e. how the code should be organized and
how the different parts of an application separated for proper readability and debugging. Model is
the data and the database. View is the user interface and what the user sees. Controller is the main
link between Model and View. These are the three pillars of major programming frameworks present on
the market today. On the other hand AngularJS works on MV*, short for Model-View-Whatever. The
Whatever is AngularJS's way of telling that you may create any kind of linking between the Model
and the View here.
Unlike other frameworks in any programming language, where MVC, the three separate components, each
one has to be written and then connected by the programmer, AngularJS helps the programmer by asking
him/her to just create these and everything else will be taken care of by AngularJS.
Interconnection with HTML at the root level
AngularJS uses HTML to define the user's interface. AngularJS also enables the programmer to write
new HTML tags (AngularJS Directives) and increase the readability and understandability of the HTML
code. Directives are AngularJS’s way of bringing additional functionality to HTML. Directives
achieve this by enabling us to invent our own HTML elements. This also helps in making the code DRY
(Don't Repeat Yourself), which means once created, a new directive can be used anywhere within the
application.
HTML is also used to determine the wiring of the app. Special attributes in the HTML determine where
to load the app, which components or controllers to use for each element, etc. We specify ""what""
gets loaded, but not ""how"". This declarative approach greatly simplifies app development in a sort
of WYSIWYG way. Rather than spending time on how the program flows and orchestrating the various
moving parts, we simply define what we want and AngularJS will take care of the dependencies.
Data Handling made simple
Data and Data Models in AngularJS are plain JavaScript objects and one can add and change properties
directly on it and loop over objects and arrays at will.
Two-way Data Binding
One of AngularJS's strongest features. Two-way Data Binding means that if something changes in the
Model, the change gets reflected in the View instantaneously, and the same happens the other way
around. This is also referred to as Reactive Programming, i.e. suppose a = b + c is being
programmed and after this, if the value of b and/or c is changed then the value of a will be
automatically updated to reflect the change. AngularJS uses its ""scopes"" as a glue between the Model
and View and makes these updates in one available for the other.
Less Written Code and Easily Maintainable Code
Everything in AngularJS is created to enable the programmer to end up writing less code that is
easily maintainable and readable by any other new person on the team. Believe it or not, one can
write a complete working two-way data binded application in less than 10 lines of code. Try and see
for yourself!
Testing Ready
AngularJS has Dependency Injection, i.e. it takes care of providing all the necessary dependencies
to its controllers and services whenever required. This helps in making the AngularJS code ready for
unit testing by making use of mock dependencies created and injected. This makes AngularJS more
modular and easily testable thus in turn helping a team create more robust applications.
"
12,JavaScript,"JavaScript Algorithms and Data Structures


This repository contains JavaScript based examples of many
popular algorithms and data structures.
Each algorithm and data structure has its own separate README
with related explanations and links for further reading (including ones
to YouTube videos).
Read this in other languages:
简体中文,
繁體中文,
한국어,
日本語,
Polski,
Français,
Español,
Português
☝ Note that this project is meant to be used for learning and researching purposes
only and it is not meant to be used for production.
Data Structures
A data structure is a particular way of organizing and storing data in a computer so that it can
be accessed and modified efficiently. More precisely, a data structure is a collection of data
values, the relationships among them, and the functions or operations that can be applied to
the data.
B - Beginner, A - Advanced

B Linked List
B Doubly Linked List
B Queue
B Stack
B Hash Table
B Heap - max and min heap versions
B Priority Queue
A Trie
A Tree

A Binary Search Tree
A AVL Tree
A Red-Black Tree
A Segment Tree - with min/max/sum range queries examples
A Fenwick Tree (Binary Indexed Tree)


A Graph (both directed and undirected)
A Disjoint Set
A Bloom Filter

Algorithms
An algorithm is an unambiguous specification of how to solve a class of problems. It is
a set of rules that precisely define a sequence of operations.
B - Beginner, A - Advanced
Algorithms by Topic

Math

B Bit Manipulation - set/get/update/clear bits, multiplication/division by two, make negative etc.
B Factorial
B Fibonacci Number - classic and closed-form versions
B Primality Test (trial division method)
B Euclidean Algorithm - calculate the Greatest Common Divisor (GCD)
B Least Common Multiple (LCM)
B Sieve of Eratosthenes - finding all prime numbers up to any given limit
B Is Power of Two - check if the number is power of two (naive and bitwise algorithms)
B Pascal's Triangle
B Complex Number - complex numbers and basic operations with them
B Radian & Degree - radians to degree and backwards conversion
B Fast Powering
A Integer Partition
A Square Root - Newton's method
A Liu Hui π Algorithm - approximate π calculations based on N-gons
A Discrete Fourier Transform - decompose a function of time (a signal) into the frequencies that make it up


Sets

B Cartesian Product - product of multiple sets
B Fisher–Yates Shuffle - random permutation of a finite sequence
A Power Set - all subsets of a set (bitwise and backtracking solutions)
A Permutations (with and without repetitions)
A Combinations (with and without repetitions)
A Longest Common Subsequence (LCS)
A Longest Increasing Subsequence
A Shortest Common Supersequence (SCS)
A Knapsack Problem - ""0/1"" and ""Unbound"" ones
A Maximum Subarray - ""Brute Force"" and ""Dynamic Programming"" (Kadane's) versions
A Combination Sum - find all combinations that form specific sum


Strings

B Hamming Distance - number of positions at which the symbols are different
A Levenshtein Distance - minimum edit distance between two sequences
A Knuth–Morris–Pratt Algorithm (KMP Algorithm) - substring search (pattern matching)
A Z Algorithm - substring search (pattern matching)
A Rabin Karp Algorithm - substring search
A Longest Common Substring
A Regular Expression Matching


Searches

B Linear Search
B Jump Search (or Block Search) - search in sorted array
B Binary Search - search in sorted array
B Interpolation Search - search in uniformly distributed sorted array


Sorting

B Bubble Sort
B Selection Sort
B Insertion Sort
B Heap Sort
B Merge Sort
B Quicksort - in-place and non-in-place implementations
B Shellsort
B Counting Sort
B Radix Sort


Linked Lists

B Straight Traversal
B Reverse Traversal


Trees

B Depth-First Search (DFS)
B Breadth-First Search (BFS)


Graphs

B Depth-First Search (DFS)
B Breadth-First Search (BFS)
B Kruskal’s Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Dijkstra Algorithm - finding shortest paths to all graph vertices from single vertex
A Bellman-Ford Algorithm - finding shortest paths to all graph vertices from single vertex
A Floyd-Warshall Algorithm - find shortest paths between all pairs of vertices
A Detect Cycle - for both directed and undirected graphs (DFS and Disjoint Set based versions)
A Prim’s Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Topological Sorting - DFS method
A Articulation Points - Tarjan's algorithm (DFS based)
A Bridges - DFS based algorithm
A Eulerian Path and Eulerian Circuit - Fleury's algorithm - Visit every edge exactly once
A Hamiltonian Cycle - Visit every vertex exactly once
A Strongly Connected Components - Kosaraju's algorithm
A Travelling Salesman Problem - shortest possible route that visits each city and returns to the origin city


Cryptography

B Polynomial Hash - rolling hash function based on polynomial


Machine Learning

B NanoNeuron - 7 simple JS functions that illustrate how machines can actually learn (forward/backward propagation)


Uncategorized

B Tower of Hanoi
B Square Matrix Rotation - in-place algorithm
B Jump Game - backtracking, dynamic programming (top-down + bottom-up) and greedy examples
B Unique Paths - backtracking, dynamic programming and Pascal's Triangle based examples
B Rain Terraces - trapping rain water problem (dynamic programming and brute force versions)
B Recursive Staircase - count the number of ways to reach to the top (4 solutions)
A N-Queens Problem
A Knight's Tour



Algorithms by Paradigm
An algorithmic paradigm is a generic method or approach which underlies the design of a class
of algorithms. It is an abstraction higher than the notion of an algorithm, just as an
algorithm is an abstraction higher than a computer program.

Brute Force - look at all the possibilities and selects the best solution

B Linear Search
B Rain Terraces - trapping rain water problem
B Recursive Staircase - count the number of ways to reach to the top
A Maximum Subarray
A Travelling Salesman Problem - shortest possible route that visits each city and returns to the origin city
A Discrete Fourier Transform - decompose a function of time (a signal) into the frequencies that make it up


Greedy - choose the best option at the current time, without any consideration for the future

B Jump Game
A Unbound Knapsack Problem
A Dijkstra Algorithm - finding shortest path to all graph vertices
A Prim’s Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Kruskal’s Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph


Divide and Conquer - divide the problem into smaller parts and then solve those parts

B Binary Search
B Tower of Hanoi
B Pascal's Triangle
B Euclidean Algorithm - calculate the Greatest Common Divisor (GCD)
B Merge Sort
B Quicksort
B Tree Depth-First Search (DFS)
B Graph Depth-First Search (DFS)
B Jump Game
B Fast Powering
A Permutations (with and without repetitions)
A Combinations (with and without repetitions)


Dynamic Programming - build up a solution using previously found sub-solutions

B Fibonacci Number
B Jump Game
B Unique Paths
B Rain Terraces - trapping rain water problem
B Recursive Staircase - count the number of ways to reach to the top
A Levenshtein Distance - minimum edit distance between two sequences
A Longest Common Subsequence (LCS)
A Longest Common Substring
A Longest Increasing Subsequence
A Shortest Common Supersequence
A 0/1 Knapsack Problem
A Integer Partition
A Maximum Subarray
A Bellman-Ford Algorithm - finding shortest path to all graph vertices
A Floyd-Warshall Algorithm - find shortest paths between all pairs of vertices
A Regular Expression Matching


Backtracking - similarly to brute force, try to generate all possible solutions, but each time you generate next solution you test
if it satisfies all conditions, and only then continue generating subsequent solutions. Otherwise, backtrack, and go on a
different path of finding a solution. Normally the DFS traversal of state-space is being used.

B Jump Game
B Unique Paths
B Power Set - all subsets of a set
A Hamiltonian Cycle - Visit every vertex exactly once
A N-Queens Problem
A Knight's Tour
A Combination Sum - find all combinations that form specific sum


Branch & Bound - remember the lowest-cost solution found at each stage of the backtracking
search, and use the cost of the lowest-cost solution found so far as a lower bound on the cost of
a least-cost solution to the problem, in order to discard partial solutions with costs larger than the
lowest-cost solution found so far. Normally BFS traversal in combination with DFS traversal of state-space
tree is being used.

How to use this repository
Install all dependencies
npm install

Run ESLint
You may want to run it to check code quality.
npm run lint

Run all tests
npm test

Run tests by name
npm test -- 'LinkedList'

Playground
You may play with data-structures and algorithms in ./src/playground/playground.js file and write
tests for it in ./src/playground/__test__/playground.test.js.
Then just simply run the following command to test if your playground code works as expected:
npm test -- 'playground'

Useful Information
References
▶ Data Structures and Algorithms on YouTube
Big O Notation
Big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows.
On the chart below you may find most common orders of growth of algorithms specified in Big O notation.

Source: Big O Cheat Sheet.
Below is the list of some of the most used Big O notations and their performance comparisons against different sizes of the input data.



Big O Notation
Computations for 10 elements
Computations for 100 elements
Computations for 1000 elements




O(1)
1
1
1


O(log N)
3
6
9


O(N)
10
100
1000


O(N log N)
30
600
9000


O(N^2)
100
10000
1000000


O(2^N)
1024
1.26e+29
1.07e+301


O(N!)
3628800
9.3e+157
4.02e+2567



Data Structure Operations Complexity



Data Structure
Access
Search
Insertion
Deletion
Comments




Array
1
n
n
n



Stack
n
n
1
1



Queue
n
n
1
1



Linked List
n
n
1
n



Hash Table
-
n
n
n
In case of perfect hash function costs would be O(1)


Binary Search Tree
n
n
n
n
In case of balanced tree costs would be O(log(n))


B-Tree
log(n)
log(n)
log(n)
log(n)



Red-Black Tree
log(n)
log(n)
log(n)
log(n)



AVL Tree
log(n)
log(n)
log(n)
log(n)



Bloom Filter
-
1
1
-
False positives are possible while searching



Array Sorting Algorithms Complexity



Name
Best
Average
Worst
Memory
Stable
Comments




Bubble sort
n
n2
n2
1
Yes



Insertion sort
n
n2
n2
1
Yes



Selection sort
n2
n2
n2
1
No



Heap sort
n log(n)
n log(n)
n log(n)
1
No



Merge sort
n log(n)
n log(n)
n log(n)
n
Yes



Quick sort
n log(n)
n log(n)
n2
log(n)
No
Quicksort is usually done in-place with O(log(n)) stack space


Shell sort
n log(n)
depends on gap sequence
n (log(n))2
1
No



Counting sort
n + r
n + r
n + r
n + r
Yes
r - biggest number in array


Radix sort
n * k
n * k
n * k
n + k
Yes
k - length of longest key



"
13,JavaScript,"three.js






JavaScript 3D library
The aim of the project is to create an easy to use, lightweight, 3D library with a default WebGL renderer. The library also provides Canvas 2D, SVG and CSS3D renderers in the examples.
Examples —
Documentation —
Wiki —
Migrating —
Questions —
Forum —
Gitter —
Slack
Usage
Download the minified library and include it in your HTML, or install and import it as a module,
Alternatively, see how to build the library yourself.
<script src=""js/three.min.js""></script>
This code creates a scene, a camera, and a geometric cube, and it adds the cube to the scene. It then creates a WebGL renderer for the scene and camera, and it adds that viewport to the document.body element. Finally, it animates the cube within the scene for the camera.
var camera, scene, renderer;
var geometry, material, mesh;

init();
animate();

function init() {

	camera = new THREE.PerspectiveCamera( 70, window.innerWidth / window.innerHeight, 0.01, 10 );
	camera.position.z = 1;

	scene = new THREE.Scene();

	geometry = new THREE.BoxGeometry( 0.2, 0.2, 0.2 );
	material = new THREE.MeshNormalMaterial();

	mesh = new THREE.Mesh( geometry, material );
	scene.add( mesh );

	renderer = new THREE.WebGLRenderer( { antialias: true } );
	renderer.setSize( window.innerWidth, window.innerHeight );
	document.body.appendChild( renderer.domElement );

}

function animate() {

	requestAnimationFrame( animate );

	mesh.rotation.x += 0.01;
	mesh.rotation.y += 0.02;

	renderer.render( scene, camera );

}
If everything went well you should see this.
Change log
Releases
"
14,JavaScript,"Puppeteer
    

API | FAQ | Contributing | Troubleshooting

Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol. Puppeteer runs headless by default, but can be configured to run full (non-headless) Chrome or Chromium.

What can I do?
Most things that you can do manually in the browser can be done using Puppeteer! Here are a few examples to get you started:

Generate screenshots and PDFs of pages.
Crawl a SPA (Single-Page Application) and generate pre-rendered content (i.e. ""SSR"" (Server-Side Rendering)).
Automate form submission, UI testing, keyboard input, etc.
Create an up-to-date, automated testing environment. Run your tests directly in the latest version of Chrome using the latest JavaScript and browser features.
Capture a timeline trace of your site to help diagnose performance issues.
Test Chrome Extensions.

Give it a spin: https://try-puppeteer.appspot.com/
Getting Started
Installation
To use Puppeteer in your project, run:
npm i puppeteer
# or ""yarn add puppeteer""
Note: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, see Environment variables.
puppeteer-core
Since version 1.7.0 we publish the puppeteer-core package,
a version of Puppeteer that doesn't download Chromium by default.
npm i puppeteer-core
# or ""yarn add puppeteer-core""
puppeteer-core is intended to be a lightweight version of Puppeteer for launching an existing browser installation or for connecting to a remote one. Be sure that the version of puppeteer-core you install is compatible with the
browser you intend to connect to.
See puppeteer vs puppeteer-core.
Usage
Puppeteer follows the latest maintenance LTS version of Node.
Note: Prior to v1.18.1, Puppeteer required at least Node v6.4.0. All subsequent versions rely on
Node 8.9.0+. All examples below use async/await which is only supported in Node v7.6.0 or greater.
Puppeteer will be familiar to people using other browser testing frameworks. You create an instance
of Browser, open pages, and then manipulate them with Puppeteer's API.
Example - navigating to https://example.com and saving a screenshot as example.png:
Save file as example.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');
  await page.screenshot({path: 'example.png'});

  await browser.close();
})();
Execute script on the command line
node example.js
Puppeteer sets an initial page size to 800×600px, which defines the screenshot size. The page size can be customized  with Page.setViewport().
Example - create a PDF.
Save file as hn.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});
  await page.pdf({path: 'hn.pdf', format: 'A4'});

  await browser.close();
})();
Execute script on the command line
node hn.js
See Page.pdf() for more information about creating pdfs.
Example - evaluate script in the context of the page
Save file as get-dimensions.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');

  // Get the ""viewport"" of the page, as reported by the page.
  const dimensions = await page.evaluate(() => {
    return {
      width: document.documentElement.clientWidth,
      height: document.documentElement.clientHeight,
      deviceScaleFactor: window.devicePixelRatio
    };
  });

  console.log('Dimensions:', dimensions);

  await browser.close();
})();
Execute script on the command line
node get-dimensions.js
See Page.evaluate() for more information on evaluate and related methods like evaluateOnNewDocument and exposeFunction.
Default runtime settings
1. Uses Headless mode
Puppeteer launches Chromium in headless mode. To launch a full version of Chromium, set the headless option when launching a browser:
const browser = await puppeteer.launch({headless: false}); // default is true
2. Runs a bundled version of Chromium
By default, Puppeteer downloads and uses a specific version of Chromium so its API
is guaranteed to work out of the box. To use Puppeteer with a different version of Chrome or Chromium,
pass in the executable's path when creating a Browser instance:
const browser = await puppeteer.launch({executablePath: '/path/to/Chrome'});
See Puppeteer.launch() for more information.
See this article for a description of the differences between Chromium and Chrome. This article describes some differences for Linux users.
3. Creates a fresh user profile
Puppeteer creates its own Chromium user profile which it cleans up on every run.
Resources

API Documentation
Examples
Community list of Puppeteer resources

Debugging tips


Turn off headless mode - sometimes it's useful to see what the browser is
displaying. Instead of launching in headless mode, launch a full version of
the browser using  headless: false:
 const browser = await puppeteer.launch({headless: false});



Slow it down - the slowMo option slows down Puppeteer operations by the
specified amount of milliseconds. It's another way to help see what's going on.
 const browser = await puppeteer.launch({
   headless: false,
   slowMo: 250 // slow down by 250ms
 });



Capture console output - You can listen for the console event.
This is also handy when debugging code in page.evaluate():
 page.on('console', msg => console.log('PAGE LOG:', msg.text()));

 await page.evaluate(() => console.log(`url is ${location.href}`));



Use debugger in application code browser
There are two execution context: node.js that is running test code, and the browser
running application code being tested. This lets you debug code in the
application code browser; ie code inside evaluate().


Use {devtools: true} when launching Puppeteer:
const browser = await puppeteer.launch({devtools: true});


Change default test timeout:
jest: jest.setTimeout(100000);
jasmine: jasmine.DEFAULT_TIMEOUT_INTERVAL = 100000;
mocha: this.timeout(100000); (don't forget to change test to use function and not '=>')


Add an evaluate statement with debugger inside / add  debugger to an existing evaluate statement:
await page.evaluate(() => {debugger;});
The test will now stop executing in the above evaluate statement, and chromium will stop in debug mode.




Use debugger in node.js
This will let you debug test code. For example, you can step over await page.click() in the node.js script and see the click happen in the application code browser.
Note that you won't be able to run await page.click() in
DevTools console due to this Chromium bug. So if
you want to try something out, you have to add it to your test file.

Add debugger; to your test, eg:
debugger;
await page.click('a[target=_blank]');


Set headless to false
Run node --inspect-brk, eg node --inspect-brk node_modules/.bin/jest tests
In Chrome open chrome://inspect/#devices and click inspect
In the newly opened test browser, type F8 to resume test execution
Now your debugger will be hit and you can debug in the test browser



Enable verbose logging - internal DevTools protocol traffic
will be logged via the debug module under the puppeteer namespace.
 # Basic verbose logging
 env DEBUG=""puppeteer:*"" node script.js

 # Protocol traffic can be rather noisy. This example filters out all Network domain messages
 env DEBUG=""puppeteer:*"" env DEBUG_COLORS=true node script.js 2>&1 | grep -v '""Network'



Debug your Puppeteer (node) code easily, using ndb




npm install -g ndb (or even better, use npx!)


add a debugger to your Puppeteer (node) code


add ndb (or npx ndb) before your test command. For example:
ndb jest or ndb mocha (or npx ndb jest / npx ndb mocha)


debug your test inside chromium like a boss!


Contributing to Puppeteer
Check out contributing guide to get an overview of Puppeteer development.
FAQ
Q: Who maintains Puppeteer?
The Chrome DevTools team maintains the library, but we'd love your help and expertise on the project!
See Contributing.
Q: What are Puppeteer’s goals and principles?
The goals of the project are:

Provide a slim, canonical library that highlights the capabilities of the DevTools Protocol.
Provide a reference implementation for similar testing libraries. Eventually, these other frameworks could adopt Puppeteer as their foundational layer.
Grow the adoption of headless/automated browser testing.
Help dogfood new DevTools Protocol features...and catch bugs!
Learn more about the pain points of automated browser testing and help fill those gaps.

We adapt Chromium principles to help us drive product decisions:

Speed: Puppeteer has almost zero performance overhead over an automated page.
Security: Puppeteer operates off-process with respect to Chromium, making it safe to automate potentially malicious pages.
Stability: Puppeteer should not be flaky and should not leak memory.
Simplicity: Puppeteer provides a high-level API that’s easy to use, understand, and debug.

Q: Is Puppeteer replacing Selenium/WebDriver?
No. Both projects are valuable for very different reasons:

Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers.
Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.

That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver:

Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with. At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all.
Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts.
Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution.
Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.

Q: Why doesn’t Puppeteer v.XXX work with Chromium v.YYY?
We see Puppeteer as an indivisible entity with Chromium. Each version of Puppeteer bundles a specific version of Chromium – the only version it is guaranteed to work with.
This is not an artificial constraint: A lot of work on Puppeteer is actually taking place in the Chromium repository. Here’s a typical story:

A Puppeteer bug is reported: https://github.com/puppeteer/puppeteer/issues/2709
It turned out this is an issue with the DevTools protocol, so we’re fixing it in Chromium: https://chromium-review.googlesource.com/c/chromium/src/+/1102154
Once the upstream fix is landed, we roll updated Chromium into Puppeteer: https://github.com/puppeteer/puppeteer/pull/2769

However, oftentimes it is desirable to use Puppeteer with the official Google Chrome rather than Chromium. For this to work, you should install a puppeteer-core version that corresponds to the Chrome version.
For example, in order to drive Chrome 71 with puppeteer-core, use chrome-71 npm tag:
npm install puppeteer-core@chrome-71
Q: Which Chromium version does Puppeteer use?
Look for chromium_revision in package.json. To find the corresponding Chromium commit and version number, search for the revision prefixed by an r in OmahaProxy's ""Find Releases"" section.
Q: What’s considered a “Navigation”?
From Puppeteer’s standpoint, “navigation” is anything that changes a page’s URL.
Aside from regular navigation where the browser hits the network to fetch a new document from the web server, this includes anchor navigations and History API usage.
With this definition of “navigation,” Puppeteer works seamlessly with single-page applications.
Q: What’s the difference between a “trusted"" and ""untrusted"" input event?
In browsers, input events could be divided into two big groups: trusted vs. untrusted.

Trusted events: events generated by users interacting with the page, e.g. using a mouse or keyboard.
Untrusted event: events generated by Web APIs, e.g. document.createEvent or element.click() methods.

Websites can distinguish between these two groups:

using an Event.isTrusted event flag
sniffing for accompanying events. For example, every trusted 'click' event is preceded by 'mousedown' and 'mouseup' events.

For automation purposes it’s important to generate trusted events. All input events generated with Puppeteer are trusted and fire proper accompanying events. If, for some reason, one needs an untrusted event, it’s always possible to hop into a page context with page.evaluate and generate a fake event:
await page.evaluate(() => {
  document.querySelector('button[type=submit]').click();
});
Q: What features does Puppeteer not support?
You may find that Puppeteer does not behave as expected when controlling pages that incorporate audio and video. (For example, video playback/screenshots is likely to fail.) There are two reasons for this:

Puppeteer is bundled with Chromium — not Chrome — and so by default, it inherits all of Chromium's media-related limitations. This means that Puppeteer does not support licensed formats such as AAC or H.264. (However, it is possible to force Puppeteer to use a separately-installed version Chrome instead of Chromium via the executablePath option to puppeteer.launch. You should only use this configuration if you need an official release of Chrome that supports these media formats.)
Since Puppeteer (in all configurations) controls a desktop version of Chromium/Chrome, features that are only supported by the mobile version of Chrome are not supported. This means that Puppeteer does not support HTTP Live Streaming (HLS).

Q: I am having trouble installing / running Puppeteer in my test environment. Where should I look for help?
We have a troubleshooting guide for various operating systems that lists the required dependencies.
Q: How do I try/test a prerelease version of Puppeteer?
You can check out this repo or install the latest prerelease from npm:
npm i --save puppeteer@next
Please note that prerelease may be unstable and contain bugs.
Q: I have more questions! Where do I ask?
There are many ways to get help on Puppeteer:

bugtracker
Stack Overflow
slack channel

Make sure to search these channels before posting your question.
"
15,JavaScript,"
30 seconds of code

Short JavaScript code snippets for all your development needs


Visit our website to view our snippet collection.
Use the Search page to find snippets that suit your needs. You can search by name, tag, language or using a snippet's description. Just start typing a term and see what comes up.
Browse the JavaScript Snippet List to see all the snippets in this project or click individual tags at the top of the same page to narrow down your search to a specific tag.
Click on each snippet card to view the whole snippet, including code, explanation and examples.
You can use the button on the right side of a snippet card to copy the code to clipboard.
If you like the project, give it a star. It means a lot to the people maintaining it.

Want to contribute?

If you want to help us improve, take a minute to read the Contribution Guidelines first.
Use the Snippet Template to add new snippets to the collection.
If you find a problem with a specific snippet, please open an issue.
If you find a problem with the website, please report it in the web repository.

Credits & Sponsors

This repository is maintained by the 30-seconds organization on GitHub.
All snippets are licensed under the CC0-1.0 License, unless explicitly stated otherwise.
Logos, names and trademarks are not to be used without the explicit consent of the maintainers or owners of the 30 seconds GitHub organization.
Our website is powered by Netlify, Gatsby, Travis CI & GitHub.

"
16,JavaScript,"


Material-UI

React components that implement Google's Material Design.















Installation
Material-UI is available as an npm package.
Stable channel v4
// with npm
npm install @material-ui/core

// with yarn
yarn add @material-ui/core
v3.x (Migration from v3 to v4)
v0.x (Migration to v1)
Please note that @next will only point to pre-releases; to get the latest stable release use @latest instead.
Who sponsors Material-UI?
Diamond 💎
3/3 slots available
Diamond Sponsors are those who have pledged $2,000/month and more to Material-UI.
Please contact us at diamond@material-ui.com to subscribe to this tier.
Gold 🏆
via Patreon




via OpenCollective



Gold Sponsors are those who have pledged $500/month and more to Material-UI.
There is more!
See the full list of our backers.
Usage
Here is a quick example to get you started, it's all you need:
import React from 'react';
import ReactDOM from 'react-dom';
import Button from '@material-ui/core/Button';

function App() {
  return (
    <Button variant=""contained"" color=""primary"">
      Hello World
    </Button>
  );
}

ReactDOM.render(<App />, document.querySelector('#app'));
Yes, it's really all you need to get started as you can see in this live and interactive demo:

Questions
For how-to questions and other non-issues,
please use StackOverflow instead of Github issues.
There is a StackOverflow tag called ""material-ui"" that you can use to tag your questions.
Examples
Are you looking for an example project to get started?
We host some.
Documentation
Check out our documentation website.
Premium Themes
You can find complete templates & themes in our premium themes section.
Contributing
Read our contributing guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to Material-UI.
Notice that contributions go far beyond pull requests and commits.
Although we love giving you the opportunity to put your stamp on Material-UI, we also are thrilled to receive a variety of other contributions.
Changelog
Recently Updated?
Please read the changelog.
Roadmap
The future plans and high priority features and enhancements can be found in the roadmap file.
License
This project is licensed under the terms of the
MIT license.
"
17,JavaScript,"jQuery — New Wave JavaScript


Contribution Guides
In the spirit of open source software development, jQuery always encourages community code contribution. To help you get started and before you jump into writing code, be sure to read these important contribution guidelines thoroughly:

Getting Involved
Core Style Guide
Writing Code for jQuery Foundation Projects

Environments in which to use jQuery

Browser support
jQuery also supports Node, browser extensions, and other non-browser environments.

What you need to build your own jQuery
To build jQuery, you need to have the latest Node.js/npm and git 1.7 or later. Earlier versions might work, but are not supported.
For Windows, you have to download and install git and Node.js.
macOS users should install Homebrew. Once Homebrew is installed, run brew install git to install git,
and brew install node to install Node.js.
Linux/BSD users should use their appropriate package managers to install git and Node.js, or build from source
if you swing that way. Easy-peasy.
How to build your own jQuery
Clone a copy of the main jQuery git repo by running:
git clone git://github.com/jquery/jquery.git
Enter the jquery directory and run the build script:
cd jquery && npm run build
The built version of jQuery will be put in the dist/ subdirectory, along with the minified copy and associated map file.
If you want to create custom build or help with jQuery development, it would be better to install grunt command line interface as a global package:
npm install -g grunt-cli

Make sure you have grunt installed by testing:
grunt -V

Now by running the grunt command, in the jquery directory, you can build a full version of jQuery, just like with an npm run build command:
grunt

There are many other tasks available for jQuery Core:
grunt -help

Modules
Special builds can be created that exclude subsets of jQuery functionality.
This allows for smaller custom builds when the builder is certain that those parts of jQuery are not being used.
For example, an app that only used JSONP for $.ajax() and did not need to calculate offsets or positions of elements could exclude the offset and ajax/xhr modules.
Any module may be excluded except for core, and selector. To exclude a module, pass its path relative to the src folder (without the .js extension).
Some example modules that can be excluded are:

ajax: All AJAX functionality: $.ajax(), $.get(), $.post(), $.ajaxSetup(), .load(), transports, and ajax event shorthands such as .ajaxStart().
ajax/xhr: The XMLHTTPRequest AJAX transport only.
ajax/script: The <script> AJAX transport only; used to retrieve scripts.
ajax/jsonp: The JSONP AJAX transport only; depends on the ajax/script transport.
css: The .css() method. Also removes all modules depending on css (including effects, dimensions, and offset).
css/showHide:  Non-animated .show(), .hide() and .toggle(); can be excluded if you use classes or explicit .css() calls to set the display property. Also removes the effects module.
deprecated: Methods documented as deprecated but not yet removed.
dimensions: The .width() and .height() methods, including inner- and outer- variations.
effects: The .animate() method and its shorthands such as .slideUp() or .hide(""slow"").
event: The .on() and .off() methods and all event functionality. Also removes event/alias.
event/alias: All event attaching/triggering shorthands like .click() or .mouseover().
event/trigger: The .trigger() and .triggerHandler() methods. Used by the alias module.
offset: The .offset(), .position(), .offsetParent(), .scrollLeft(), and .scrollTop() methods.
wrap: The .wrap(), .wrapAll(), .wrapInner(), and .unwrap() methods.
core/ready: Exclude the ready module if you place your scripts at the end of the body. Any ready callbacks bound with jQuery() will simply be called immediately. However, jQuery(document).ready() will not be a function and .on(""ready"", ...) or similar will not be triggered.
deferred: Exclude jQuery.Deferred. This also removes jQuery.Callbacks. Note that modules that depend on jQuery.Deferred(AJAX, effects, core/ready) will not be removed and will still expect jQuery.Deferred to be there. Include your own jQuery.Deferred implementation or exclude those modules as well (grunt custom:-deferred,-ajax,-effects,-core/ready).
exports/global: Exclude the attachment of global jQuery variables ($ and jQuery) to the window.
exports/amd: Exclude the AMD definition.

The build process shows a message for each dependent module it excludes or includes.
AMD name
As an option, you can set the module name for jQuery's AMD definition. By default, it is set to ""jquery"", which plays nicely with plugins and third-party libraries, but there may be cases where you'd like to change this. Simply set the ""amd"" option:
grunt custom --amd=""custom-name""
Or, to define anonymously, set the name to an empty string.
grunt custom --amd=""""
Custom Build Examples
To create a custom build, first check out the version:
git pull; git checkout VERSION
Where VERSION is the version you want to customize. Then, make sure all Node dependencies are installed:
npm install
Create the custom build using the grunt custom option, listing the modules to be excluded.
Exclude all ajax functionality:
grunt custom:-ajax
Excluding css removes modules depending on CSS: effects, offset, dimensions.
grunt custom:-css
Exclude a bunch of modules:
grunt custom:-ajax,-css,-deprecated,-dimensions,-effects,-event/alias,-offset,-wrap
For questions or requests regarding custom builds, please start a thread on the Developing jQuery Core section of the forum. Due to the combinatorics and custom nature of these builds, they are not regularly tested in jQuery's unit test process.
Running the Unit Tests
Make sure you have the necessary dependencies:
npm install
Start grunt watch or npm start to auto-build jQuery as you work:
grunt watch
Run the unit tests with a local server that supports PHP. Ensure that you run the site from the root directory, not the ""test"" directory. No database is required. Pre-configured php local servers are available for Windows and Mac. Here are some options:

Windows: WAMP download
Mac: MAMP download
Linux: Setting up LAMP
Mongoose (most platforms)

Building to a different directory
To copy the built jQuery files from /dist to another directory:
grunt && grunt dist:/path/to/special/location/
With this example, the output files would be:
/path/to/special/location/jquery.js
/path/to/special/location/jquery.min.js
To add a permanent copy destination, create a file in dist/ called "".destination.json"". Inside the file, paste and customize the following:
{
  ""/Absolute/path/to/other/destination"": true
}
Additionally, both methods can be combined.
Essential Git
As the source code is handled by the Git version control system, it's useful to know some features used.
Cleaning
If you want to purge your working directory back to the status of upstream, the following commands can be used (remember everything you've worked on is gone after these):
git reset --hard upstream/master
git clean -fdx
Rebasing
For feature/topic branches, you should always use the --rebase flag to git pull, or if you are usually handling many temporary ""to be in a github pull request"" branches, run the following to automate this:
git config branch.autosetuprebase local
(see man git-config for more information)
Handling merge conflicts
If you're getting merge conflicts when merging, instead of editing the conflicted files manually, you can use the feature
git mergetool. Even though the default tool xxdiff looks awful/old, it's rather useful.
The following are some commands that can be used there:

Ctrl + Alt + M - automerge as much as possible
b - jump to next merge conflict
s - change the order of the conflicted lines
u - undo a merge
left mouse button - mark a block to be the winner
middle mouse button - mark a line to be the winner
Ctrl + S - save
Ctrl + Q - quit

QUnit Reference
Test methods
expect( numAssertions );
stop();
start();
Note: QUnit's eventual addition of an argument to stop/start is ignored in this test suite so that start and stop can be passed as callbacks without worrying about their parameters.
Test assertions
ok( value, [message] );
equal( actual, expected, [message] );
notEqual( actual, expected, [message] );
deepEqual( actual, expected, [message] );
notDeepEqual( actual, expected, [message] );
strictEqual( actual, expected, [message] );
notStrictEqual( actual, expected, [message] );
throws( block, [expected], [message] );
Test Suite Convenience Methods Reference (See test/data/testinit.js)
Returns an array of elements with the given IDs
q( ... );
Example:
q(""main"", ""foo"", ""bar"");

=> [ div#main, span#foo, input#bar ]
Asserts that a selection matches the given IDs
t( testName, selector, [ ""array"", ""of"", ""ids"" ] );
Example:
t(""Check for something"", ""//[a]"", [""foo"", ""bar""]);
Fires a native DOM event without going through jQuery
fireNative( node, eventType )
Example:
fireNative( jQuery(""#elem"")[0], ""click"" );
Add random number to url to stop caching
url( ""some/url"" );
Example:
url(""index.html"");

=> ""data/index.html?10538358428943""


url(""mock.php?foo=bar"");

=> ""data/mock.php?foo=bar&10538358345554""
Run tests in an iframe
Some tests may require a document other than the standard test fixture, and
these can be run in a separate iframe. The actual test code and assertions
remain in jQuery's main test files; only the minimal test fixture markup
and setup code should be placed in the iframe file.
testIframe( testName, fileName,
  function testCallback(
      assert, jQuery, window, document,
	  [ additional args ] ) {
	...
  } );
This loads a page, constructing a url with fileName ""./data/"" + fileName.
The iframed page determines when the callback occurs in the test by
including the ""/test/data/iframeTest.js"" script and calling
startIframeTest( [ additional args ] ) when appropriate. Often this
will be after either document ready or window.onload fires.
The testCallback receives the QUnit assert object created by testIframe
for this test, followed by the global jQuery, window, and document from
the iframe. If the iframe code passes any arguments to startIframeTest,
they follow the document argument.
Questions?
If you have any questions, please feel free to ask on the
Developing jQuery Core forum or in #jquery on irc.freenode.net.
"
18,JavaScript,"




































webpack

    webpack is a module bundler. Its main purpose is to bundle JavaScript files for usage in a browser, yet it is also capable of transforming, bundling, or packaging just about any resource or asset.
  

Table of Contents

Install
Introduction
Concepts
Contributing
Support
Core Team
Sponsoring
Premium Partners
Other Backers and Sponsors
Gold Sponsors
Silver Sponsors
Bronze Sponsors
Backers
Special Thanks

Install
Install with npm:
npm install --save-dev webpack
Install with yarn:
yarn add webpack --dev
Introduction
webpack is a bundler for modules. The main purpose is to bundle JavaScript
files for usage in a browser, yet it is also capable of transforming, bundling,
or packaging just about any resource or asset.
TL;DR

Bundles ES Modules, CommonJS, and AMD modules (even combined).
Can create a single bundle or multiple chunks that are asynchronously loaded at runtime (to reduce initial loading time).
Dependencies are resolved during compilation, reducing the runtime size.
Loaders can preprocess files while compiling, e.g. TypeScript to JavaScript, Handlebars strings to compiled functions, images to Base64, etc.
Highly modular plugin system to do whatever else your application requires.

Get Started
Check out webpack's quick Get Started guide and the other guides.
Browser Compatibility
webpack supports all browsers that are ES5-compliant (IE8 and below are not supported).
webpack also needs Promise for import() and require.ensure(). If you want to support older browsers, you will need to load a polyfill before using these expressions.
Concepts
Plugins
webpack has a rich plugin
interface. Most of the features
within webpack itself use this plugin interface. This makes webpack very
flexible.



Name
Status
Install Size
Description




mini-css-extract-plugin


Extracts CSS into separate files. It creates a CSS file per JS file which contains CSS.


compression-webpack-plugin


Prepares compressed versions of assets to serve them with Content-Encoding


i18n-webpack-plugin


Adds i18n support to your bundles


html-webpack-plugin


Simplifies creation of HTML files (index.html) to serve your bundles


extract-text-webpack-plugin


Extract text from a bundle, or bundles, into a separate file



Loaders
webpack enables the use of loaders to preprocess files. This allows you to bundle
any static resource way beyond JavaScript. You can easily write your own
loaders using Node.js.
Loaders are activated by using loadername! prefixes in require() statements,
or are automatically applied via regex from your webpack configuration.
Files



Name
Status
Install Size
Description




raw-loader


Loads raw content of a file (utf-8)


val-loader


Executes code as module and considers exports as JS code


url-loader


Works like the file loader, but can return a Data Url if the file is smaller than a limit


file-loader


Emits the file into the output folder and returns the (relative) url



JSON



Name
Status
Install Size
Description







Loads a JSON file (included by default)





Loads and transpiles a JSON 5 file





Loads and transpiles a CSON file



Transpiling



Name
Status
Install Size
Description




<script>


Executes a JavaScript file once in global context (like in script tag), require()s are not parsed





Loads ES2015+ code and transpiles to ES5 using Babel





Loads ES2015+ code and transpiles to ES5 using Traceur





Loads TypeScript like JavaScript


awesome-typescript-loader


Awesome TypeScript loader for webpack





Loads CoffeeScript like JavaScript



Templating



Name
Status
Install Size
Description







Exports HTML as string, requires references to static resources





Loads Pug templates and returns a function





Compiles Markdown to HTML





Loads and transforms a HTML file using PostHTML





Compiles Handlebars to HTML



Styling



Name
Status
Install Size
Description




<style>


Add exports of a module as style to DOM





Loads CSS file with resolved imports and returns CSS code





Loads and compiles a LESS file





Loads and compiles a Sass/SCSS file





Loads and compiles a Stylus file





Loads and transforms a CSS/SSS file using PostCSS



Linting & Testing



Name
Status
Install Size
Description







Tests with mocha (Browser/NodeJS)





PreLoader for linting code using ESLint





PreLoader for linting code using JSHint



Frameworks



Name
Status
Install Size
Description







Loads and compiles Vue Components





Process HTML & CSS with preprocessor of choice and require() Web Components like first-class modules





Loads and compiles Angular 2 Components





Riot official webpack loader



Performance
webpack uses async I/O and has multiple caching levels. This makes webpack fast
and incredibly fast on incremental compilations.
Module Formats
webpack supports ES2015+, CommonJS and AMD modules out of the box. It performs clever static
analysis on the AST of your code. It even has an evaluation engine to evaluate
simple expressions. This allows you to support most existing libraries out of the box.
Code Splitting
webpack allows you to split your codebase into multiple chunks. Chunks are
loaded asynchronously at runtime. This reduces the initial loading time.
Optimizations
webpack can do many optimizations to reduce the output size of your
JavaScript by deduplicating frequently used modules, minifying, and giving
you full control of what is loaded initially and what is loaded at runtime
through code splitting. It can also make your code chunks cache
friendly by using hashes.
Contributing
We want contributing to webpack to be fun, enjoyable, and educational for anyone, and everyone. We have a vibrant ecosystem that spans beyond this single repo. We welcome you to check out any of the repositories in our organization or webpack-contrib organization which houses all of our loaders and plugins.
Contributions go far beyond pull requests and commits. Although we love giving you the opportunity to put your stamp on webpack, we also are thrilled to receive a variety of other contributions including:

Documentation updates, enhancements, designs, or bugfixes
Spelling or grammar fixes
README.md corrections or redesigns
Adding unit, or functional tests
Triaging GitHub issues -- especially determining whether an issue still persists or is reproducible.
Searching #webpack on twitter and helping someone else who needs help
Teaching others how to contribute to one of the many webpack's repos!
Blogging, speaking about, or creating tutorials about one of webpack's many features.
Helping others in our webpack gitter channel.

If you are worried or don't know where to start, you can always reach out to Sean Larkin (@TheLarkInn) on Twitter or simply submit an issue and a maintainer can help give you guidance!
We have also started a series on our Medium Publication called The Contributor's Guide to webpack. We welcome you to read it and post any questions or responses if you still need help.
Looking to speak about webpack? We'd love to review your talk abstract/CFP! You can email it to webpack [at] opencollective [dot] com and we can give pointers or tips!!!
Creating your own plugins and loaders
If you create a loader or plugin, we would <3 for you to open source it, and put it on npm. We follow the x-loader, x-webpack-plugin naming convention.
Support
We consider webpack to be a low-level tool used not only individually but also layered beneath other awesome tools. Because of its flexibility, webpack isn't always the easiest entry-level solution, however we do believe it is the most powerful. That said, we're always looking for ways to improve and simplify the tool without compromising functionality. If you have any ideas on ways to accomplish this, we're all ears!
If you're just getting started, take a look at our new docs and concepts page. This has a high level overview that is great for beginners!!
Looking for webpack 1 docs? Please check out the old wiki, but note that this deprecated version is no longer supported.
If you want to discuss something or just need help, here is our Gitter room where there are always individuals looking to help out!
If you are still having difficulty, we would love for you to post
a question to StackOverflow with the webpack tag. It is much easier to answer questions that include your webpack.config.js and relevant files! So if you can provide them, we'd be extremely grateful (and more likely to help you find the answer!)
If you are twitter savvy you can tweet #webpack with your question and someone should be able to reach out and help also.
If you have discovered a 🐜 or have a feature suggestion, feel free to create an issue on Github.
License

Core Team






Tobias Koppers
Core

Founder of webpack




Johannes Ewald
Loaders & Plugins

Early adopter of webpack




Sean T. Larkin
Public Relations

Founder of the core team




Kees Kluskens
Development

Sponsor








Sponsoring
Most of the core team members, webpack contributors and contributors in the ecosystem do this open source work in their free time. If you use webpack for a serious task, and you'd like us to invest more time on it, please donate. This project increases your income/productivity too. It makes development and applications faster and it reduces the required bandwidth.
This is how we use the donations:

Allow the core team to work on webpack
Thank contributors if they invested a large amount of time in contributing
Support projects in the ecosystem that are of great value for users
Support projects that are voted most (work in progress)
Infrastructure cost
Fees for money handling

Premium Partners




Other Backers and Sponsors
Before we started using OpenCollective, donations were made anonymously. Now that we have made the switch, we would like to acknowledge these sponsors (and the ones who continue to donate using OpenCollective). If we've missed someone, please send us a PR, and we'll add you to this list.

Google Angular Team, Architects.io,



Gold Sponsors
Become a gold sponsor and get your logo on our README on Github with a link to your site.
































Silver Sponsors
Become a silver sponsor and get your logo on our README on Github with a link to your site.
































Bronze Sponsors
Become a bronze sponsor and get your logo on our README on Github with a link to your site.







































































































Backers
Become a backer and get your image on our README on Github with a link to your site.





































































































Special Thanks to
(In chronological order)

@google for Google Web Toolkit (GWT), which aims to compile Java to JavaScript. It features a similar Code Splitting as webpack.
@medikoo for modules-webmake, which is a similar project. webpack was born because I wanted Code Splitting for modules-webmake. Interestingly the Code Splitting issue is still open (thanks also to @Phoscur for the discussion).
@substack for browserify, which is a similar project and source for many ideas.
@jrburke for require.js, which is a similar project and source for many ideas.
@defunctzombie for the browser-field spec, which makes modules available for node.js, browserify and webpack.
Every early webpack user, which contributed to webpack by writing issues or PRs. You influenced the direction...
@shama, @jhnns and @sokra for maintaining this project
Everyone who has written a loader for webpack. You are the ecosystem...
Everyone I forgot to mention here, but also influenced webpack.

"
19,JavaScript,"Atom



Atom is a hackable text editor for the 21st century, built on Electron, and based on everything we love about our favorite editors. We designed it to be deeply customizable, but still approachable using the default configuration.


Visit atom.io to learn more or visit the Atom forum.
Follow @AtomEditor on Twitter for important
announcements.
This project adheres to the Contributor Covenant code of conduct.
By participating, you are expected to uphold this code. Please report unacceptable behavior to atom@github.com.
Documentation
If you want to read about using Atom or developing packages in Atom, the Atom Flight Manual is free and available online. You can find the source to the manual in atom/flight-manual.atom.io.
The API reference for developing packages is also documented on Atom.io.
Installing
Prerequisites

Git

macOS
Download the latest Atom release.
Atom will automatically update when a new release is available.
Windows
Download the latest Atom installer. AtomSetup.exe is 32-bit. For 64-bit systems, download AtomSetup-x64.exe.
Atom will automatically update when a new release is available.
You can also download atom-windows.zip (32-bit) or atom-x64-windows.zip (64-bit) from the releases page.
The .zip version will not automatically update.
Using Chocolatey? Run cinst Atom to install the latest version of Atom.
Linux
Atom is only available for 64-bit Linux systems.
Configure your distribution's package manager to install and update Atom by following the Linux installation instructions in the Flight Manual.  You will also find instructions on how to install Atom's official Linux packages without using a package repository, though you will not get automatic updates after installing Atom this way.
Archive extraction
An archive is available for people who don't want to install atom as root.
This version enables you to install multiple Atom versions in parallel. It has been built on Ubuntu 64-bit,
but should be compatible with other Linux distributions.

Install dependencies (on Ubuntu): sudo apt install git gconf2 gconf-service libgtk2.0-0 libudev1 libgcrypt20 libnotify4 libxtst6 libnss3 python gvfs-bin xdg-utils libcap2
Download atom-amd64.tar.gz from the Atom releases page.
Run tar xf atom-amd64.tar.gz in the directory where you want to extract the Atom folder.
Launch Atom using the installed atom command from the newly extracted directory.

The Linux version does not currently automatically update so you will need to
repeat these steps to upgrade to future releases.
Building

Linux
macOS
Windows

Discussion

Discuss Atom on our forums
Chat about Atom on our Slack team -- instructions for joining

License
MIT
When using the Atom or other GitHub logos, be sure to follow the GitHub logo guidelines.
"
20,JavaScript,"reveal.js  
A framework for easily creating beautiful presentations using HTML. Check out the live demo.
reveal.js comes with a broad range of features including nested slides, Markdown contents, PDF export, speaker notes and a JavaScript API. There's also a fully featured visual editor and platform for sharing reveal.js presentations at slides.com.
Table of contents

Online Editor
Installation

Basic setup
Full setup
Folder Structure


Instructions

Markup
Markdown
Element Attributes
Slide Attributes


Configuration
Presentation Size
Dependencies
Ready Event
Auto-sliding
Keyboard Bindings
Vertical Slide Navigation
Touch Navigation
Lazy Loading
API

Slide Changed Event
Presentation State
Slide States
Slide Backgrounds
Parallax Background
Slide Transitions
Internal links
Fragments
Fragment events
Code syntax highlighting
Slide number
Overview mode
Fullscreen mode
Embedded media
Stretching elements
Resize Event
postMessage API


PDF Export
Theming
Speaker Notes

Share and Print Speaker Notes
Server Side Speaker Notes


Plugins
Multiplexing

Master presentation
Client presentation
Socket.io server


MathJax
License

More reading

Changelog: Up-to-date version history.
Examples: Presentations created with reveal.js, add your own!
Browser Support: Explanation of browser support and fallbacks.
Plugins: A list of plugins that can be used to extend reveal.js.

Online Editor
Presentations are written using HTML or Markdown but there's also an online editor for those of you who prefer a graphical interface. Give it a try at https://slides.com.
Installation
The basic setup is for authoring presentations only. The full setup gives you access to all reveal.js features and plugins such as speaker notes as well as the development tasks needed to make changes to the source.
Basic setup
The core of reveal.js is very easy to install. You'll simply need to download a copy of this repository and open the index.html file directly in your browser.

Download the latest version of reveal.js from https://github.com/hakimel/reveal.js/releases
Unzip and replace the example contents in index.html with your own
Open index.html in a browser to view it

Full setup
Some reveal.js features, like external Markdown and speaker notes, require that presentations run from a local web server. The following instructions will set up such a server as well as all of the development tasks needed to make edits to the reveal.js source code.


Install Node.js (4.0.0 or later)


Clone the reveal.js repository
$ git clone https://github.com/hakimel/reveal.js.git


Navigate to the reveal.js folder
$ cd reveal.js


Install dependencies
$ npm install


Serve the presentation and monitor source files for changes
$ npm start


Open http://localhost:8000 to view your presentation
You can change the port by using npm start -- --port=8001.


Folder Structure

css/ Core styles without which the project does not function
js/ Like above but for JavaScript
plugin/ Components that have been developed as extensions to reveal.js
lib/ All other third party assets (JavaScript, CSS, fonts)

Instructions
Markup
Here's a barebones example of a fully working reveal.js presentation:
<html>
	<head>
		<link rel=""stylesheet"" href=""css/reveal.css"">
		<link rel=""stylesheet"" href=""css/theme/white.css"">
	</head>
	<body>
		<div class=""reveal"">
			<div class=""slides"">
				<section>Slide 1</section>
				<section>Slide 2</section>
			</div>
		</div>
		<script src=""js/reveal.js""></script>
		<script>
			Reveal.initialize();
		</script>
	</body>
</html>
The presentation markup hierarchy needs to be .reveal > .slides > section where the section represents one slide and can be repeated indefinitely. If you place multiple section elements inside of another section they will be shown as vertical slides. The first of the vertical slides is the ""root"" of the others (at the top), and will be included in the horizontal sequence. For example:
<div class=""reveal"">
	<div class=""slides"">
		<section>Single Horizontal Slide</section>
		<section>
			<section>Vertical Slide 1</section>
			<section>Vertical Slide 2</section>
		</section>
	</div>
</div>
Markdown
It's possible to write your slides using Markdown. To enable Markdown, add the data-markdown attribute to your <section> elements and wrap the contents in a <textarea data-template> like the example below. You'll also need to add the plugin/markdown/marked.js and plugin/markdown/markdown.js scripts (in that order) to your HTML file.
This is based on data-markdown from Paul Irish modified to use marked to support GitHub Flavored Markdown. Sensitive to indentation (avoid mixing tabs and spaces) and line breaks (avoid consecutive breaks).
<section data-markdown>
	<textarea data-template>
		## Page title

		A paragraph with some text and a [link](http://hakim.se).
	</textarea>
</section>
External Markdown
You can write your content as a separate file and have reveal.js load it at runtime. Note the separator arguments which determine how slides are delimited in the external file: the data-separator attribute defines a regular expression for horizontal slides (defaults to ^\r?\n---\r?\n$, a newline-bounded horizontal rule)  and data-separator-vertical defines vertical slides (disabled by default). The data-separator-notes attribute is a regular expression for specifying the beginning of the current slide's speaker notes (defaults to notes?:, so it will match both ""note:"" and ""notes:""). The data-charset attribute is optional and specifies which charset to use when loading the external file.
When used locally, this feature requires that reveal.js runs from a local web server.  The following example customises all available options:
<section data-markdown=""example.md""
         data-separator=""^\n\n\n""
         data-separator-vertical=""^\n\n""
         data-separator-notes=""^Note:""
         data-charset=""iso-8859-15"">
    <!--
        Note that Windows uses `\r\n` instead of `\n` as its linefeed character.
        For a regex that supports all operating systems, use `\r?\n` instead of `\n`.
    -->
</section>
Element Attributes
Special syntax (through HTML comments) is available for adding attributes to Markdown elements. This is useful for fragments, amongst other things.
<section data-markdown>
	<script type=""text/template"">
		- Item 1 <!-- .element: class=""fragment"" data-fragment-index=""2"" -->
		- Item 2 <!-- .element: class=""fragment"" data-fragment-index=""1"" -->
	</script>
</section>
Slide Attributes
Special syntax (through HTML comments) is available for adding attributes to the slide <section> elements generated by your Markdown.
<section data-markdown>
	<script type=""text/template"">
	<!-- .slide: data-background=""#ff0000"" -->
		Markdown content
	</script>
</section>
Configuring marked
We use marked to parse Markdown. To customise marked's rendering, you can pass in options when configuring Reveal:
Reveal.initialize({
	// Options which are passed into marked
	// See https://marked.js.org/#/USING_ADVANCED.md#options
	markdown: {
		smartypants: true
	}
});
Configuration
At the end of your page you need to initialize reveal by running the following code. Note that all configuration values are optional and will default to the values specified below.
Reveal.initialize({

	// Display presentation control arrows
	controls: true,

	// Help the user learn the controls by providing hints, for example by
	// bouncing the down arrow when they first encounter a vertical slide
	controlsTutorial: true,

	// Determines where controls appear, ""edges"" or ""bottom-right""
	controlsLayout: 'bottom-right',

	// Visibility rule for backwards navigation arrows; ""faded"", ""hidden""
	// or ""visible""
	controlsBackArrows: 'faded',

	// Display a presentation progress bar
	progress: true,

	// Display the page number of the current slide
	slideNumber: false,

	// Add the current slide number to the URL hash so that reloading the
	// page/copying the URL will return you to the same slide
	hash: false,

	// Push each slide change to the browser history. Implies `hash: true`
	history: false,

	// Enable keyboard shortcuts for navigation
	keyboard: true,

	// Enable the slide overview mode
	overview: true,

	// Vertical centering of slides
	center: true,

	// Enables touch navigation on devices with touch input
	touch: true,

	// Loop the presentation
	loop: false,

	// Change the presentation direction to be RTL
	rtl: false,

	// See https://github.com/hakimel/reveal.js/#navigation-mode
	navigationMode: 'default',

	// Randomizes the order of slides each time the presentation loads
	shuffle: false,

	// Turns fragments on and off globally
	fragments: true,

	// Flags whether to include the current fragment in the URL,
	// so that reloading brings you to the same fragment position
	fragmentInURL: false,

	// Flags if the presentation is running in an embedded mode,
	// i.e. contained within a limited portion of the screen
	embedded: false,

	// Flags if we should show a help overlay when the questionmark
	// key is pressed
	help: true,

	// Flags if speaker notes should be visible to all viewers
	showNotes: false,

	// Global override for autoplaying embedded media (video/audio/iframe)
	// - null: Media will only autoplay if data-autoplay is present
	// - true: All media will autoplay, regardless of individual setting
	// - false: No media will autoplay, regardless of individual setting
	autoPlayMedia: null,

	// Global override for preloading lazy-loaded iframes
	// - null: Iframes with data-src AND data-preload will be loaded when within
	//   the viewDistance, iframes with only data-src will be loaded when visible
	// - true: All iframes with data-src will be loaded when within the viewDistance
	// - false: All iframes with data-src will be loaded only when visible
	preloadIframes: null,

	// Number of milliseconds between automatically proceeding to the
	// next slide, disabled when set to 0, this value can be overwritten
	// by using a data-autoslide attribute on your slides
	autoSlide: 0,

	// Stop auto-sliding after user input
	autoSlideStoppable: true,

	// Use this method for navigation when auto-sliding
	autoSlideMethod: Reveal.navigateNext,

	// Specify the average time in seconds that you think you will spend
	// presenting each slide. This is used to show a pacing timer in the
	// speaker view
	defaultTiming: 120,

	// Enable slide navigation via mouse wheel
	mouseWheel: false,

	// Hide cursor if inactive
	hideInactiveCursor: true,

	// Time before the cursor is hidden (in ms)
	hideCursorTime: 5000,

	// Hides the address bar on mobile devices
	hideAddressBar: true,

	// Opens links in an iframe preview overlay
	// Add `data-preview-link` and `data-preview-link=""false""` to customise each link
	// individually
	previewLinks: false,

	// Transition style
	transition: 'slide', // none/fade/slide/convex/concave/zoom

	// Transition speed
	transitionSpeed: 'default', // default/fast/slow

	// Transition style for full page slide backgrounds
	backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

	// Number of slides away from the current that are visible
	viewDistance: 3,

	// Parallax background image
	parallaxBackgroundImage: '', // e.g. ""'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'""

	// Parallax background size
	parallaxBackgroundSize: '', // CSS syntax, e.g. ""2100px 900px""

	// Number of pixels to move the parallax background per slide
	// - Calculated automatically unless specified
	// - Set to 0 to disable movement along an axis
	parallaxBackgroundHorizontal: null,
	parallaxBackgroundVertical: null,

	// The display mode that will be used to show slides
	display: 'block'

});
The configuration can be updated after initialization using the configure method:
// Turn autoSlide off
Reveal.configure({ autoSlide: 0 });

// Start auto-sliding every 5s
Reveal.configure({ autoSlide: 5000 });
Presentation Size
All presentations have a normal size, that is, the resolution at which they are authored. The framework will automatically scale presentations uniformly based on this size to ensure that everything fits on any given display or viewport.
See below for a list of configuration options related to sizing, including default values:
Reveal.initialize({

	// ...

	// The ""normal"" size of the presentation, aspect ratio will be preserved
	// when the presentation is scaled to fit different resolutions. Can be
	// specified using percentage units.
	width: 960,
	height: 700,

	// Factor of the display size that should remain empty around the content
	margin: 0.1,

	// Bounds for smallest/largest possible scale to apply to content
	minScale: 0.2,
	maxScale: 1.5

});
If you wish to disable this behavior and do your own scaling (e.g. using media queries), try these settings:
Reveal.initialize({

	// ...

	width: ""100%"",
	height: ""100%"",
	margin: 0,
	minScale: 1,
	maxScale: 1
});
Dependencies
Reveal.js doesn't rely on any third party scripts to work but a few optional libraries are included by default. These libraries are loaded as dependencies in the order they appear, for example:
Reveal.initialize({
	dependencies: [
		// Interpret Markdown in <section> elements
		{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
		{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

		// Syntax highlight for <code> elements
		{ src: 'plugin/highlight/highlight.js', async: true },

		// Zoom in and out with Alt+click
		{ src: 'plugin/zoom-js/zoom.js', async: true },

		// Speaker notes
		{ src: 'plugin/notes/notes.js', async: true },

		// MathJax
		{ src: 'plugin/math/math.js', async: true }
	]
});
You can add your own extensions using the same syntax. The following properties are available for each dependency object:

src: Path to the script to load
async: [optional] Flags if the script should load after reveal.js has started, defaults to false
callback: [optional] Function to execute when the script has loaded
condition: [optional] Function which must return true for the script to be loaded

Ready Event
A ready event is fired when reveal.js has loaded all non-async dependencies and is ready to start navigating. To check if reveal.js is already 'ready' you can call Reveal.isReady().
Reveal.addEventListener( 'ready', function( event ) {
	// event.currentSlide, event.indexh, event.indexv
} );
Note that we also add a .ready class to the .reveal element so that you can hook into this with CSS.
Auto-sliding
Presentations can be configured to progress through slides automatically, without any user input. To enable this you will need to tell the framework how many milliseconds it should wait between slides:
// Slide every five seconds
Reveal.configure({
  autoSlide: 5000
});
When this is turned on a control element will appear that enables users to pause and resume auto-sliding. Alternatively, sliding can be paused or resumed by pressing »A« on the keyboard. Sliding is paused automatically as soon as the user starts navigating. You can disable these controls by specifying autoSlideStoppable: false in your reveal.js config.
You can also override the slide duration for individual slides and fragments by using the data-autoslide attribute:
<section data-autoslide=""2000"">
	<p>After 2 seconds the first fragment will be shown.</p>
	<p class=""fragment"" data-autoslide=""10000"">After 10 seconds the next fragment will be shown.</p>
	<p class=""fragment"">Now, the fragment is displayed for 2 seconds before the next slide is shown.</p>
</section>
To override the method used for navigation when auto-sliding, you can specify the autoSlideMethod setting. To only navigate along the top layer and ignore vertical slides, set this to Reveal.navigateRight.
Whenever the auto-slide mode is resumed or paused the autoslideresumed and autoslidepaused events are fired.
Keyboard Bindings
If you're unhappy with any of the default keyboard bindings you can override them using the keyboard config option:
Reveal.configure({
  keyboard: {
    13: 'next', // go to the next slide when the ENTER key is pressed
    27: function() {}, // do something custom when ESC is pressed
    32: null // don't do anything when SPACE is pressed (i.e. disable a reveal.js default binding)
  }
});
Vertical Slide Navigation
Slides can be nested within other slides to create vertical stacks (see Markup). When presenting, you use the left/right arrows to step through the main (horizontal) slides. When you arrive at a vertical stack you can optionally press the up/down arrows to view the vertical slides or skip past them by pressing the right arrow. Here's an example showing a bird's-eye view of what this looks like in action:

Navigation Mode
You can finetune the reveal.js navigation behavior by using the navigationMode config option. Note that these options are only useful for presnetations that use a mix of horizontal and vertical slides. The following navigation modes are available:



Value
Description




default
Left/right arrow keys step between horizontal slides. Up/down arrow keys step between vertical slides. Space key steps through all slides (both horizontal and vertical).


linear
Removes the up/down arrows. Left/right arrows step through all slides (both horizontal and vertical).


grid
When this is enabled, stepping left/right from a vertical stack to an adjacent vertical stack will land you at the same vertical index.Consider a deck with six slides ordered in two vertical stacks:1.1    2.11.2    2.21.3    2.3If you're on slide 1.3 and navigate right, you will normally move from 1.3 -> 2.1. With navigationMode set to ""grid"" the same navigation takes you from 1.3 -> 2.3.



Touch Navigation
You can swipe to navigate through a presentation on any touch-enabled device. Horizontal swipes change between horizontal slides, vertical swipes change between vertical slides. If you wish to disable this you can set the touch config option to false when initializing reveal.js.
If there's some part of your content that needs to remain accessible to touch events you'll need to highlight this by adding a data-prevent-swipe attribute to the element. One common example where this is useful is elements that need to be scrolled.
Lazy Loading
When working on presentation with a lot of media or iframe content it's important to load lazily. Lazy loading means that reveal.js will only load content for the few slides nearest to the current slide. The number of slides that are preloaded is determined by the viewDistance configuration option.
To enable lazy loading all you need to do is change your src attributes to data-src as shown below. This is supported for image, video, audio and iframe elements.
<section>
  <img data-src=""image.png"">
  <iframe data-src=""http://hakim.se""></iframe>
  <video>
    <source data-src=""video.webm"" type=""video/webm"" />
    <source data-src=""video.mp4"" type=""video/mp4"" />
  </video>
</section>
Lazy Loading Iframes
Note that lazy loaded iframes ignore the viewDistance configuration and will only load when their containing slide becomes visible. Iframes are also unloaded as soon as the slide is hidden.
When we lazy load a video or audio element, reveal.js won't start playing that content until the slide becomes visible. However there is no way to control this for an iframe since that could contain any kind of content. That means if we loaded an iframe before the slide is visible on screen it could begin playing media and sound in the background.
You can override this behavior with the data-preload attribute. The iframe below will be loaded
according to the viewDistance.
<section>
	<iframe data-src=""http://hakim.se"" data-preload></iframe>
</section>
You can also change the default globally with the preloadIframes configuration option. If set to
true ALL iframes with a data-src attribute will be preloaded when within the viewDistance
regardless of individual data-preload attributes. If set to false, all iframes will only be
loaded when they become visible.
API
The Reveal object exposes a JavaScript API for controlling navigation and reading state:
// Navigation
Reveal.slide( indexh, indexv, indexf );
Reveal.left();
Reveal.right();
Reveal.up();
Reveal.down();
Reveal.prev();
Reveal.next();
Reveal.prevFragment();
Reveal.nextFragment();

// Randomize the order of slides
Reveal.shuffle();

// Toggle presentation states, optionally pass true/false to force on/off
Reveal.toggleOverview();
Reveal.togglePause();
Reveal.toggleAutoSlide();

// Shows a help overlay with keyboard shortcuts, optionally pass true/false
// to force on/off
Reveal.toggleHelp();

// Change a config value at runtime
Reveal.configure({ controls: true });

// Returns the present configuration options
Reveal.getConfig();

// Fetch the current scale of the presentation
Reveal.getScale();

// Retrieves the previous and current slide elements
Reveal.getPreviousSlide();
Reveal.getCurrentSlide();

Reveal.getIndices();        // { h: 0, v: 0, f: 0 }
Reveal.getSlidePastCount();
Reveal.getProgress();       // (0 == first slide, 1 == last slide)
Reveal.getSlides();         // Array of all slides
Reveal.getTotalSlides();    // Total number of slides

// Returns the speaker notes for the current slide
Reveal.getSlideNotes();

// State checks
Reveal.isFirstSlide();
Reveal.isLastSlide();
Reveal.isOverview();
Reveal.isPaused();
Reveal.isAutoSliding();

// Returns the top-level DOM element
getRevealElement(); // <div class=""reveal"">...</div>
Custom Key Bindings
Custom key bindings can be added and removed using the following Javascript API. Custom key bindings will override the default keyboard bindings, but will in turn be overridden by the user defined bindings in the keyboard config option.
Reveal.addKeyBinding( binding, callback );
Reveal.removeKeyBinding( keyCode );
For example
// The binding parameter provides the following properties
//      keyCode: the keycode for binding to the callback
//          key: the key label to show in the help overlay
//  description: the description of the action to show in the help overlay
Reveal.addKeyBinding( { keyCode: 84, key: 'T', description: 'Start timer' }, function() {
	// start timer
} )

// The binding parameter can also be a direct keycode without providing the help description
Reveal.addKeyBinding( 82, function() {
	// reset timer
} )
This allows plugins to add key bindings directly to Reveal so they can

make use of Reveal's pre-processing logic for key handling (for example, ignoring key presses when paused); and
be included in the help overlay (optional)

Slide Changed Event
A slidechanged event is fired each time the slide is changed (regardless of state). The event object holds the index values of the current slide as well as a reference to the previous and current slide HTML nodes.
Some libraries, like MathJax (see #226), get confused by the transforms and display states of slides. Often times, this can be fixed by calling their update or render function from this callback.
Reveal.addEventListener( 'slidechanged', function( event ) {
	// event.previousSlide, event.currentSlide, event.indexh, event.indexv
} );
Presentation State
The presentation's current state can be fetched by using the getState method. A state object contains all of the information required to put the presentation back as it was when getState was first called. Sort of like a snapshot. It's a simple object that can easily be stringified and persisted or sent over the wire.
Reveal.slide( 1 );
// we're on slide 1

var state = Reveal.getState();

Reveal.slide( 3 );
// we're on slide 3

Reveal.setState( state );
// we're back on slide 1
Slide States
If you set data-state=""somestate"" on a slide <section>, ""somestate"" will be applied as a class on the document element when that slide is opened. This allows you to apply broad style changes to the page based on the active slide.
Furthermore you can also listen to these changes in state via JavaScript:
Reveal.addEventListener( 'somestate', function() {
	// TODO: Sprinkle magic
}, false );
Slide Backgrounds
Slides are contained within a limited portion of the screen by default to allow them to fit any display and scale uniformly. You can apply full page backgrounds outside of the slide area by adding a data-background attribute to your <section> elements. Four different types of backgrounds are supported: color, image, video and iframe.
Color Backgrounds
All CSS color formats are supported, including hex values, keywords, rgba() or hsl().
<section data-background-color=""#ff0000"">
	<h2>Color</h2>
</section>
Image Backgrounds
By default, background images are resized to cover the full page. Available options:



Attribute
Default
Description




data-background-image

URL of the image to show. GIFs restart when the slide opens.


data-background-size
cover
See background-size on MDN.


data-background-position
center
See background-position on MDN.


data-background-repeat
no-repeat
See background-repeat on MDN.


data-background-opacity
1
Opacity of the background image on a 0-1 scale. 0 is transparent and 1 is fully opaque.



<section data-background-image=""http://example.com/image.png"">
	<h2>Image</h2>
</section>
<section data-background-image=""http://example.com/image.png"" data-background-size=""100px"" data-background-repeat=""repeat"">
	<h2>This background image will be sized to 100px and repeated</h2>
</section>
Video Backgrounds
Automatically plays a full size video behind the slide.



Attribute
Default
Description




data-background-video

A single video source, or a comma separated list of video sources.


data-background-video-loop
false
Flags if the video should play repeatedly.


data-background-video-muted
false
Flags if the audio should be muted.


data-background-size
cover
Use cover for full screen and some cropping or contain for letterboxing.


data-background-opacity
1
Opacity of the background video on a 0-1 scale. 0 is transparent and 1 is fully opaque.



<section data-background-video=""https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.mp4,https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.webm"" data-background-video-loop data-background-video-muted>
	<h2>Video</h2>
</section>
Iframe Backgrounds
Embeds a web page as a slide background that covers 100% of the reveal.js width and height. The iframe is in the background layer, behind your slides, and as such it's not possible to interact with it by default. To make your background interactive, you can add the data-background-interactive attribute.
<section data-background-iframe=""https://slides.com"" data-background-interactive>
	<h2>Iframe</h2>
</section>
Background Transitions
Backgrounds transition using a fade animation by default. This can be changed to a linear sliding transition by passing backgroundTransition: 'slide' to the Reveal.initialize() call. Alternatively you can set data-background-transition on any section with a background to override that specific transition.
Parallax Background
If you want to use a parallax scrolling background, set the first two properties below when initializing reveal.js (the other two are optional).
Reveal.initialize({

	// Parallax background image
	parallaxBackgroundImage: '', // e.g. ""https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg""

	// Parallax background size
	parallaxBackgroundSize: '', // CSS syntax, e.g. ""2100px 900px"" - currently only pixels are supported (don't use % or auto)

	// Number of pixels to move the parallax background per slide
	// - Calculated automatically unless specified
	// - Set to 0 to disable movement along an axis
	parallaxBackgroundHorizontal: 200,
	parallaxBackgroundVertical: 50

});
Make sure that the background size is much bigger than screen size to allow for some scrolling. View example.
Slide Transitions
The global presentation transition is set using the transition config value. You can override the global transition for a specific slide by using the data-transition attribute:
<section data-transition=""zoom"">
	<h2>This slide will override the presentation transition and zoom!</h2>
</section>

<section data-transition-speed=""fast"">
	<h2>Choose from three transition speeds: default, fast or slow!</h2>
</section>
You can also use different in and out transitions for the same slide:
<section data-transition=""slide"">
    The train goes on …
</section>
<section data-transition=""slide"">
    and on …
</section>
<section data-transition=""slide-in fade-out"">
    and stops.
</section>
<section data-transition=""fade-in slide-out"">
    (Passengers entering and leaving)
</section>
<section data-transition=""slide"">
    And it starts again.
</section>
You can choose from none, fade, slide, convex, concave and zoom.
Internal links
It's easy to link between slides. The first example below targets the index of another slide whereas the second targets a slide with an ID attribute (<section id=""some-slide"">):
<a href=""#/2/2"">Link</a>
<a href=""#/some-slide"">Link</a>
You can also add relative navigation links, similar to the built in reveal.js controls, by appending one of the following classes on any element. Note that each element is automatically given an enabled class when it's a valid navigation route based on the current slide.
<a href=""#"" class=""navigate-left"">
<a href=""#"" class=""navigate-right"">
<a href=""#"" class=""navigate-up"">
<a href=""#"" class=""navigate-down"">
<a href=""#"" class=""navigate-prev""> <!-- Previous vertical or horizontal slide -->
<a href=""#"" class=""navigate-next""> <!-- Next vertical or horizontal slide -->
Fragments
Fragments are used to highlight individual elements on a slide. Every element with the class fragment will be stepped through before moving on to the next slide. Here's an example: http://revealjs.com/#/fragments
The default fragment style is to start out invisible and fade in. This style can be changed by appending a different class to the fragment:
<section>
	<p class=""fragment grow"">grow</p>
	<p class=""fragment shrink"">shrink</p>
	<p class=""fragment fade-out"">fade-out</p>
	<p class=""fragment fade-up"">fade-up (also down, left and right!)</p>
	<p class=""fragment fade-in-then-out"">fades in, then out when we move to the next step</p>
	<p class=""fragment fade-in-then-semi-out"">fades in, then obfuscate when we move to the next step</p>
	<p class=""fragment highlight-current-blue"">blue only once</p>
	<p class=""fragment highlight-red"">highlight-red</p>
	<p class=""fragment highlight-green"">highlight-green</p>
	<p class=""fragment highlight-blue"">highlight-blue</p>
</section>
Multiple fragments can be applied to the same element sequentially by wrapping it, this will fade in the text on the first step and fade it back out on the second.
<section>
	<span class=""fragment fade-in"">
		<span class=""fragment fade-out"">I'll fade in, then out</span>
	</span>
</section>
The display order of fragments can be controlled using the data-fragment-index attribute.
<section>
	<p class=""fragment"" data-fragment-index=""3"">Appears last</p>
	<p class=""fragment"" data-fragment-index=""1"">Appears first</p>
	<p class=""fragment"" data-fragment-index=""2"">Appears second</p>
</section>
Fragment events
When a slide fragment is either shown or hidden reveal.js will dispatch an event.
Some libraries, like MathJax (see #505), get confused by the initially hidden fragment elements. Often times this can be fixed by calling their update or render function from this callback.
Reveal.addEventListener( 'fragmentshown', function( event ) {
	// event.fragment = the fragment DOM element
} );
Reveal.addEventListener( 'fragmenthidden', function( event ) {
	// event.fragment = the fragment DOM element
} );
Code Syntax Highlighting
By default, Reveal is configured with highlight.js for code syntax highlighting. To enable syntax highlighting, you'll have to load the highlight plugin (plugin/highlight/highlight.js) and a highlight.js CSS theme (Reveal comes packaged with the Monokai themes: lib/css/monokai.css).
Reveal.initialize({
	// More info https://github.com/hakimel/reveal.js#dependencies
	dependencies: [
		{ src: 'plugin/highlight/highlight.js', async: true },
	]
});
Below is an example with clojure code that will be syntax highlighted. When the data-trim attribute is present, surrounding whitespace is automatically removed.  HTML will be escaped by default. To avoid this, for example if you are using <mark> to call out a line of code, add the data-noescape attribute to the <code> element.
<section>
	<pre><code data-trim data-noescape>
(def lazy-fib
  (concat
   [0 1]
   <mark>((fn rfib [a b]</mark>
        (lazy-cons (+ a b) (rfib b (+ a b)))) 0 1)))
	</code></pre>
</section>
Line Numbers & Highlights
To enable line numbers, add data-line-numbers to your <code> tags. If you want to highlight specific lines you can provide a comma separated list of line numbers using the same attribute. For example, in the following example lines 4 and 8-11 are highlighted:
<pre><code class=""hljs"" data-line-numbers=""4,8-11"">
import React, { useState } from 'react';
 
function Example() {
  const [count, setCount] = useState(0);
 
  return (
    <div>
      <p>You clicked {count} times</p>
      <button onClick={() => setCount(count + 1)}>
        Click me
      </button>
    </div>
  );
}
</code></pre>

Slide number
If you would like to display the page number of the current slide you can do so using the slideNumber and showSlideNumber configuration values.
// Shows the slide number using default formatting
Reveal.configure({ slideNumber: true });

// Slide number formatting can be configured using these variables:
//  ""h.v"": 	horizontal . vertical slide number (default)
//  ""h/v"": 	horizontal / vertical slide number
//    ""c"": 	flattened slide number
//  ""c/t"": 	flattened slide number / total slides
Reveal.configure({ slideNumber: 'c/t' });

// You can provide a function to fully customize the number:
Reveal.configure({ slideNumber: function() {
    // Ignore numbering of vertical slides
    return [ Reveal.getIndices().h ];
}});

// Control which views the slide number displays on using the ""showSlideNumber"" value:
//     ""all"": show on all views (default)
// ""speaker"": only show slide numbers on speaker notes view
//   ""print"": only show slide numbers when printing to PDF
Reveal.configure({ showSlideNumber: 'speaker' });
Overview mode
Press »ESC« or »O« keys to toggle the overview mode on and off. While you're in this mode, you can still navigate between slides,
as if you were at 1,000 feet above your presentation. The overview mode comes with a few API hooks:
Reveal.addEventListener( 'overviewshown', function( event ) { /* ... */ } );
Reveal.addEventListener( 'overviewhidden', function( event ) { /* ... */ } );

// Toggle the overview mode programmatically
Reveal.toggleOverview();
Fullscreen mode
Just press »F« on your keyboard to show your presentation in fullscreen mode. Press the »ESC« key to exit fullscreen mode.
Embedded media
Add data-autoplay to your media element if you want it to automatically start playing when the slide is shown:
<video data-autoplay src=""http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4""></video>
If you want to enable or disable autoplay globally, for all embedded media, you can use the autoPlayMedia configuration option. If you set this to true ALL media will autoplay regardless of individual data-autoplay attributes. If you initialize with autoPlayMedia: false NO media will autoplay.
Note that embedded HTML5 <video>/<audio> and YouTube/Vimeo iframes are automatically paused when you navigate away from a slide. This can be disabled by decorating your element with a data-ignore attribute.
Embedded iframes
reveal.js automatically pushes two post messages to embedded iframes. slide:start when the slide containing the iframe is made visible and slide:stop when it is hidden.
Stretching elements
Sometimes it's desirable to have an element, like an image or video, stretch to consume as much space as possible within a given slide. This can be done by adding the .stretch class to an element as seen below:
<section>
	<h2>This video will use up the remaining space on the slide</h2>
    <video class=""stretch"" src=""http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4""></video>
</section>
Limitations:

Only direct descendants of a slide section can be stretched
Only one descendant per slide section can be stretched

Resize Event
When reveal.js changes the scale of the slides it fires a resize event. You can subscribe to the event to resize your elements accordingly.
Reveal.addEventListener( 'resize', function( event ) {
	// event.scale, event.oldScale, event.size
} );
postMessage API
The framework has a built-in postMessage API that can be used when communicating with a presentation inside of another window. Here's an example showing how you'd make a reveal.js instance in the given window proceed to slide 2:
<window>.postMessage( JSON.stringify({ method: 'slide', args: [ 2 ] }), '*' );
When reveal.js runs inside of an iframe it can optionally bubble all of its events to the parent. Bubbled events are stringified JSON with three fields: namespace, eventName and state. Here's how you subscribe to them from the parent window:
window.addEventListener( 'message', function( event ) {
	var data = JSON.parse( event.data );
	if( data.namespace === 'reveal' && data.eventName ==='slidechanged' ) {
		// Slide changed, see data.state for slide number
	}
} );
This cross-window messaging can be toggled on or off using configuration flags.
Reveal.initialize({
	// ...

	// Exposes the reveal.js API through window.postMessage
	postMessage: true,

	// Dispatches all reveal.js events to the parent window through postMessage
	postMessageEvents: false
});
PDF Export
Presentations can be exported to PDF via a special print stylesheet. This feature requires that you use Google Chrome or Chromium and to be serving the presentation from a web server.
Here's an example of an exported presentation that's been uploaded to SlideShare: http://www.slideshare.net/hakimel/revealjs-300.
Separate pages for fragments
Fragments are printed on separate slides by default. Meaning if you have a slide with three fragment steps, it will generate three separate slides where the fragments appear incrementally.
If you prefer printing all fragments in their visible states on the same slide you can set the pdfSeparateFragments config option to false.
Page size
Export dimensions are inferred from the configured presentation size. Slides that are too tall to fit within a single page will expand onto multiple pages. You can limit how many pages a slide may expand onto using the pdfMaxPagesPerSlide config option, for example Reveal.configure({ pdfMaxPagesPerSlide: 1 }) ensures that no slide ever grows to more than one printed page.
Print stylesheet
To enable the PDF print capability in your presentation, the special print stylesheet at /css/print/pdf.css must be loaded. The default index.html file handles this for you when print-pdf is included in the query string. If you're using a different HTML template, you can add this to your HEAD:
<script>
	var link = document.createElement( 'link' );
	link.rel = 'stylesheet';
	link.type = 'text/css';
	link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
	document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
Instructions

Open your presentation with print-pdf included in the query string i.e. http://localhost:8000/?print-pdf. You can test this with revealjs.com?print-pdf.


If you want to include speaker notes in your export, you can append showNotes=true to the query string: http://localhost:8000/?print-pdf&showNotes=true


Open the in-browser print dialog (CTRL/CMD+P).
Change the Destination setting to Save as PDF.
Change the Layout to Landscape.
Change the Margins to None.
Enable the Background graphics option.
Click Save.


Alternatively you can use the decktape project.
Theming
The framework comes with a few different themes included:

black: Black background, white text, blue links (default theme)
white: White background, black text, blue links
league: Gray background, white text, blue links (default theme for reveal.js < 3.0.0)
beige: Beige background, dark text, brown links
sky: Blue background, thin dark text, blue links
night: Black background, thick white text, orange links
serif: Cappuccino background, gray text, brown links
simple: White background, black text, blue links
solarized: Cream-colored background, dark green text, blue links

Each theme is available as a separate stylesheet. To change theme you will need to replace black below with your desired theme name in index.html:
<link rel=""stylesheet"" href=""css/theme/black.css"" id=""theme"">
If you want to add a theme of your own see the instructions here: /css/theme/README.md.
Speaker Notes
reveal.js comes with a speaker notes plugin which can be used to present per-slide notes in a separate browser window. The notes window also gives you a preview of the next upcoming slide so it may be helpful even if you haven't written any notes. Press the »S« key on your keyboard to open the notes window.
A speaker timer starts as soon as the speaker view is opened. You can reset it to 00:00:00 at any time by simply clicking/tapping on it.
Notes are defined by appending an <aside> element to a slide as seen below. You can add the data-markdown attribute to the aside element if you prefer writing notes using Markdown.
Alternatively you can add your notes in a data-notes attribute on the slide. Like <section data-notes=""Something important""></section>.
When used locally, this feature requires that reveal.js runs from a local web server.
<section>
	<h2>Some Slide</h2>

	<aside class=""notes"">
		Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit »S« on your keyboard).
	</aside>
</section>
If you're using the external Markdown plugin, you can add notes with the help of a special delimiter:
<section data-markdown=""example.md"" data-separator=""^\n\n\n"" data-separator-vertical=""^\n\n"" data-separator-notes=""^Note:""></section>

# Title
## Sub-title

Here is some content...

Note:
This will only display in the notes window.
Share and Print Speaker Notes
Notes are only visible to the speaker inside of the speaker view. If you wish to share your notes with others you can initialize reveal.js with the showNotes configuration value set to true. Notes will appear along the bottom of the presentations.
When showNotes is enabled notes are also included when you export to PDF. By default, notes are printed in a box on top of the slide. If you'd rather print them on a separate page, after the slide, set showNotes: ""separate-page"".
Speaker notes clock and timers
The speaker notes window will also show:

Time elapsed since the beginning of the presentation.  If you hover the mouse above this section, a timer reset button will appear.
Current wall-clock time
(Optionally) a pacing timer which indicates whether the current pace of the presentation is on track for the right timing (shown in green), and if not, whether the presenter should speed up (shown in red) or has the luxury of slowing down (blue).

The pacing timer can be enabled by configuring by the defaultTiming parameter in the Reveal configuration block, which specifies the number of seconds per slide.  120 can be a reasonable rule of thumb.  Timings can also be given per slide <section> by setting the data-timing attribute.  Both values are in numbers of seconds.
Server Side Speaker Notes
In some cases it can be desirable to run notes on a separate device from the one you're presenting on. The Node.js-based notes plugin lets you do this using the same note definitions as its client side counterpart. Include the required scripts by adding the following dependencies:
Reveal.initialize({
	// ...

	dependencies: [
		{ src: 'socket.io/socket.io.js', async: true },
		{ src: 'plugin/notes-server/client.js', async: true }
	]
});
Then:

Install Node.js (4.0.0 or later)
Run npm install
Run node plugin/notes-server

Plugins
Plugins should register themselves with reveal.js by calling Reveal.registerPlugin( 'myPluginID', MyPlugin ). Registered plugin instances can optionally expose an ""init"" function that reveal.js will call to initialize them.
When reveal.js is booted up via Reveal.initialize(), it will go through all registered plugins and invoke their ""init"" methods. If the ""init"" method returns a Promise, reveal.js will wait for that promise to be fullfilled before finshing the startup sequence and firing the ready event. Here's an example of a plugin that does some asynchronous work before reveal.js can proceed:
let MyPlugin = {
	init: () =>  new Promise( resolve => setTimeout( resolve, 3000 ) )
};
Reveal.registerPlugin( 'myPlugin', MyPlugin );
Reveal.addEventListener( 'ready', () => console.log( 'Three seconds later...' ) );
Reveal.initialize();
If the init method does not return a Promise, the plugin is considered ready right away and will not hold up the reveal.js startup sequence.
Retrieving Plugins
If you want to check if a specific plugin is registered you can use the Reveal.hasPlugin method and pass in a plugin ID, for example: Reveal.hasPlugin( 'myPlugin' ). If you want to retrieve a plugin instance you can use Reveal.getPlugin( 'myPlugin' ).
Multiplexing
The multiplex plugin allows your audience to view the slides of the presentation you are controlling on their own phone, tablet or laptop. As the master presentation navigates the slides, all client presentations will update in real time. See a demo at https://reveal-js-multiplex-ccjbegmaii.now.sh/.
The multiplex plugin needs the following 3 things to operate:

Master presentation that has control
Client presentations that follow the master
Socket.io server to broadcast events from the master to the clients

Master presentation
Served from a static file server accessible (preferably) only to the presenter. This need only be on your (the presenter's) computer. (It's safer to run the master presentation from your own computer, so if the venue's Internet goes down it doesn't stop the show.) An example would be to execute the following commands in the directory of your master presentation:

npm install node-static
static

If you want to use the speaker notes plugin with your master presentation then make sure you have the speaker notes plugin configured correctly along with the configuration shown below, then execute node plugin/notes-server in the directory of your master presentation. The configuration below will cause it to connect to the socket.io server as a master, as well as launch your speaker-notes/static-file server.
You can then access your master presentation at http://localhost:1947
Example configuration:
Reveal.initialize({
	// other options...

	multiplex: {
		// Example values. To generate your own, see the socket.io server instructions.
		secret: '13652805320794272084', // Obtained from the socket.io server. Gives this (the master) control of the presentation
		id: '1ea875674b17ca76', // Obtained from socket.io server
		url: 'https://reveal-js-multiplex-ccjbegmaii.now.sh' // Location of socket.io server
	},

	// Don't forget to add the dependencies
	dependencies: [
		{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js', async: true },
		{ src: 'plugin/multiplex/master.js', async: true },

		// and if you want speaker notes
		{ src: 'plugin/notes-server/client.js', async: true }

		// other dependencies...
	]
});
Client presentation
Served from a publicly accessible static file server. Examples include: GitHub Pages, Amazon S3, Dreamhost, Akamai, etc. The more reliable, the better. Your audience can then access the client presentation via http://example.com/path/to/presentation/client/index.html, with the configuration below causing them to connect to the socket.io server as clients.
Example configuration:
Reveal.initialize({
	// other options...

	multiplex: {
		// Example values. To generate your own, see the socket.io server instructions.
		secret: null, // null so the clients do not have control of the master presentation
		id: '1ea875674b17ca76', // id, obtained from socket.io server
		url: 'https://reveal-js-multiplex-ccjbegmaii.now.sh' // Location of socket.io server
	},

	// Don't forget to add the dependencies
	dependencies: [
		{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js', async: true },
		{ src: 'plugin/multiplex/client.js', async: true }

		// other dependencies...
	]
});
Socket.io server
Server that receives the slideChanged events from the master presentation and broadcasts them out to the connected client presentations. This needs to be publicly accessible. You can run your own socket.io server with the commands:

npm install
node plugin/multiplex

Or you can use the socket.io server at https://reveal-js-multiplex-ccjbegmaii.now.sh/.
You'll need to generate a unique secret and token pair for your master and client presentations. To do so, visit http://example.com/token, where http://example.com is the location of your socket.io server. Or if you're going to use the socket.io server at https://reveal-js-multiplex-ccjbegmaii.now.sh/, visit https://reveal-js-multiplex-ccjbegmaii.now.sh/token.
You are very welcome to point your presentations at the Socket.io server running at https://reveal-js-multiplex-ccjbegmaii.now.sh/, but availability and stability are not guaranteed.
For anything mission critical I recommend you run your own server. The easiest way to do this is by installing now. With that installed, deploying your own Multiplex server is as easy running the following command from the reveal.js folder: now plugin/multiplex.
socket.io server as file static server
The socket.io server can play the role of static file server for your client presentation, as in the example at https://reveal-js-multiplex-ccjbegmaii.now.sh/. (Open https://reveal-js-multiplex-ccjbegmaii.now.sh/ in two browsers. Navigate through the slides on one, and the other will update to match.)
Example configuration:
Reveal.initialize({
	// other options...

	multiplex: {
		// Example values. To generate your own, see the socket.io server instructions.
		secret: null, // null so the clients do not have control of the master presentation
		id: '1ea875674b17ca76', // id, obtained from socket.io server
		url: 'example.com:80' // Location of your socket.io server
	},

	// Don't forget to add the dependencies
	dependencies: [
		{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js', async: true },
		{ src: 'plugin/multiplex/client.js', async: true }

		// other dependencies...
	]
It can also play the role of static file server for your master presentation and client presentations at the same time (as long as you don't want to use speaker notes). (Open https://reveal-js-multiplex-ccjbegmaii.now.sh/ in two browsers. Navigate through the slides on one, and the other will update to match. Navigate through the slides on the second, and the first will update to match.) This is probably not desirable, because you don't want your audience to mess with your slides while you're presenting. ;)
Example configuration:
Reveal.initialize({
	// other options...

	multiplex: {
		// Example values. To generate your own, see the socket.io server instructions.
		secret: '13652805320794272084', // Obtained from the socket.io server. Gives this (the master) control of the presentation
		id: '1ea875674b17ca76', // Obtained from socket.io server
		url: 'example.com:80' // Location of your socket.io server
	},

	// Don't forget to add the dependencies
	dependencies: [
		{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js', async: true },
		{ src: 'plugin/multiplex/master.js', async: true },
		{ src: 'plugin/multiplex/client.js', async: true }

		// other dependencies...
	]
});
MathJax
If you want to display math equations in your presentation you can easily do so by including this plugin. The plugin is a very thin wrapper around the MathJax library. To use it you'll need to include it as a reveal.js dependency, find our more about dependencies here.
The plugin defaults to using LaTeX but that can be adjusted through the math configuration object. Note that MathJax is loaded from a remote server. If you want to use it offline you'll need to download a copy of the library and adjust the mathjax configuration value.
Below is an example of how the plugin can be configured. If you don't intend to change these values you do not need to include the math config object at all.
Reveal.initialize({
	// other options ...

	math: {
		mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
		config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
		// pass other options into `MathJax.Hub.Config()`
		TeX: { Macros: macros }
	},

	dependencies: [
		{ src: 'plugin/math/math.js', async: true }
	]
});
Read MathJax's documentation if you need HTTPS delivery or serving of specific versions for stability.
MathJax in Markdown
If you want to include math inside of a presentation written in Markdown you need to wrap the formula in backticks. This prevents syntax conflicts between LaTeX and Markdown. For example:
`$$ J(\theta_0,\theta_1) = \sum_{i=0} $$`

License
MIT licensed
Copyright (C) 2019 Hakim El Hattab, http://hakim.se
"
21,JavaScript,"socket.io
 






Features
Socket.IO enables real-time bidirectional event-based communication. It consists of:

a Node.js server (this repository)
a Javascript client library for the browser (or a Node.js client)

Some implementations in other languages are also available:

Java
C++
Swift
Dart

Its main features are:
Reliability
Connections are established even in the presence of:

proxies and load balancers.
personal firewall and antivirus software.

For this purpose, it relies on Engine.IO, which first establishes a long-polling connection, then tries to upgrade to better transports that are ""tested"" on the side, like WebSocket. Please see the Goals section for more information.
Auto-reconnection support
Unless instructed otherwise a disconnected client will try to reconnect forever, until the server is available again. Please see the available reconnection options here.
Disconnection detection
A heartbeat mechanism is implemented at the Engine.IO level, allowing both the server and the client to know when the other one is not responding anymore.
That functionality is achieved with timers set on both the server and the client, with timeout values (the pingInterval and pingTimeout parameters) shared during the connection handshake. Those timers require any subsequent client calls to be directed to the same server, hence the sticky-session requirement when using multiples nodes.
Binary support
Any serializable data structures can be emitted, including:

ArrayBuffer and Blob in the browser
ArrayBuffer and Buffer in Node.js

Simple and convenient API
Sample code:
io.on('connection', socket => {
  socket.emit('request', /* … */); // emit an event to the socket
  io.emit('broadcast', /* … */); // emit an event to all connected sockets
  socket.on('reply', () => { /* … */ }); // listen to the event
});
Cross-browser
Browser support is tested in Saucelabs:

Multiplexing support
In order to create separation of concerns within your application (for example per module, or based on permissions), Socket.IO allows you to create several Namespaces, which will act as separate communication channels but will share the same underlying connection.
Room support
Within each Namespace, you can define arbitrary channels, called Rooms, that sockets can join and leave. You can then broadcast to any given room, reaching every socket that has joined it.
This is a useful feature to send notifications to a group of users, or to a given user connected on several devices for example.
Note: Socket.IO is not a WebSocket implementation. Although Socket.IO indeed uses WebSocket as a transport when possible, it adds some metadata to each packet: the packet type, the namespace and the ack id when a message acknowledgement is needed. That is why a WebSocket client will not be able to successfully connect to a Socket.IO server, and a Socket.IO client will not be able to connect to a WebSocket server (like ws://echo.websocket.org) either. Please see the protocol specification here.
Installation
npm install socket.io
How to use
The following example attaches socket.io to a plain Node.JS
HTTP server listening on port 3000.
const server = require('http').createServer();
const io = require('socket.io')(server);
io.on('connection', client => {
  client.on('event', data => { /* … */ });
  client.on('disconnect', () => { /* … */ });
});
server.listen(3000);
Standalone
const io = require('socket.io')();
io.on('connection', client => { ... });
io.listen(3000);
In conjunction with Express
Starting with 3.0, express applications have become request handler
functions that you pass to http or http Server instances. You need
to pass the Server to socket.io, and not the express application
function. Also make sure to call .listen on the server, not the app.
const app = require('express')();
const server = require('http').createServer(app);
const io = require('socket.io')(server);
io.on('connection', () => { /* … */ });
server.listen(3000);
In conjunction with Koa
Like Express.JS, Koa works by exposing an application as a request
handler function, but only by calling the callback method.
const app = require('koa')();
const server = require('http').createServer(app.callback());
const io = require('socket.io')(server);
io.on('connection', () => { /* … */ });
server.listen(3000);
Documentation
Please see the documentation here. Contributions are welcome!
Debug / logging
Socket.IO is powered by debug.
In order to see all the debug output, run your app with the environment variable
DEBUG including the desired scope.
To see the output from all of Socket.IO's debugging scopes you can use:
DEBUG=socket.io* node myapp

Testing
npm test

This runs the gulp task test. By default the test will be run with the source code in lib directory.
Set the environmental variable TEST_VERSION to compat to test the transpiled es5-compat version of the code.
The gulp task test will always transpile the source code into es5 and export to dist first before running the test.
Backers
Support us with a monthly donation and help us continue our activities. [Become a backer]






























Sponsors
Become a sponsor and get your logo on our README on Github with a link to your site. [Become a sponsor]






























License
MIT
"
22,JavaScript,"
Semantic UI


Semantic is a UI framework designed for theming.
Key Features

50+ UI elements
3000 + CSS variables
3 Levels of variable inheritance (similar to SublimeText)
Built with EM values for responsive design
Flexbox friendly

Semantic allows developers to build beautiful websites fast, with concise HTML, intuitive javascript, and simplified debugging, helping make front-end development a delightful experience. Semantic is responsively designed allowing your website to scale on multiple devices. Semantic is production ready and partnered with frameworks such as React, Angular, Meteor, and Ember, which means you can integrate it with any of these frameworks to organize your UI layer alongside your application logic.
2.4.0 Release (Sep 17th, 2018)
Semantic UI 2.4 is now available. Read up on what's new in the docs.
Migration info from 1.x can be found in the 2.0 release notes
User Support
Please help us keep the issue tracker organized. For technical questions that do not include a specific JSFiddle test case (bug reports), or feature request please use StackOverflow to find a solution.
Visit our contributing guide for more on what should be posted to GitHub Issues.
Install
Recommended Install
npm install semantic-ui  # Use themes, import build/watch tasks into your own gulpfile.
Semantic UI includes an interactive installer to help setup your project.

For more details on setup visit our getting started guide.
To learn more about theming please read our theming guide

Additional Versions



Environment
Install Script
Repo




CSS Only
npm install semantic-ui-css
CSS Repo


LESS Only
npm install semantic-ui-less
LESS Repo


LESS plugin
npm install less-plugin-semantic-ui
LESS Plugin Repo


EmberJS
ember install:addon semantic-ui-ember
Ember Repo


Meteor - LESS
meteor add semantic:ui
Meteor Repo


Meteor - CSS
meteor add semantic:ui-css
CSS Repo


Bower
bower install semantic-ui




Check out our integration wiki for more options.
Browser Support

Last 2 Versions FF, Chrome, Safari Mac
IE 11+
Android 4.4+, Chrome for Android 44+
iOS Safari 7+
Microsoft Edge 12+

Although some components will work in IE9, grids and other flexbox components are not supported by IE9 and may not appear correctly.
Community
Getting Help
Please do not post usage questions to GitHub Issues. For these types of questions use our [Gitter chatroom] or StackOverflow.
Submitting Bugs and Enhancements
GitHub Issues is for suggesting enhancements and reporting bugs. Before submiting a bug make sure you do the following:

Check out our contributing guide for info on our release cycle.
Fork this boilerplate JSFiddle to create a test case for your bug. If a bug is apparent in the docs, that's ok as a test case, just make it clear exactly how to reproduce the issue. Only bugs that include a test case can be triaged.

Pull Requests
When adding pull requests, be sure to merge into the next branch. If you need to demonstrate a fix in next release, you can use this JSFiddle
International

Chinese A Chinese mirror site is available at http://www.semantic-ui.cn.
Right-to-Left (RTL) An RTL version can be created using our build tools by selecting rtl from the install script.
Translation To help translate see the Wiki Guide for translations.

Resources



Resource
Description




Bugs & Feature Requests
All bug submission require a link to a test case, and a set of steps to reproduce the issue. You can make a test case by forking this JSFiddle, then submit your bug report on GitHub Issues


Live Chat
Join our Gitter.im Room


Newsletter Updates
Sign up for updates at semantic-ui.com


Additional Resources
Submit a question on StackOverflow or ask our Google Group



Places to Help



Project
How To Help
Next Step




Localization
Help us translate Semantic UI into your language
Join our Translation Community


SCSS
SASS needs PR to support variables inside @import
Add Pull Request for #739


Angular
Help develop angular bindings
Reach Out on GitHub Issues


Guides & Tutorials
Help write guides and tutorials
Join the discussion



Reaching Out
If you'd like to start a conversation about Semantic feel free to e-mail me at jack@semantic-ui.com


"
23,JavaScript,"
Fast, unopinionated, minimalist web framework for node.





const express = require('express')
const app = express()

app.get('/', function (req, res) {
  res.send('Hello World')
})

app.listen(3000)
Installation
This is a Node.js module available through the
npm registry.
Before installing, download and install Node.js.
Node.js 0.10 or higher is required.
Installation is done using the
npm install command:
$ npm install express
Follow our installing guide
for more information.
Features

Robust routing
Focus on high performance
Super-high test coverage
HTTP helpers (redirection, caching, etc)
View system supporting 14+ template engines
Content negotiation
Executable for generating applications quickly

Docs & Community

Website and Documentation - [website repo]
#express on freenode IRC
GitHub Organization for Official Middleware & Modules
Visit the Wiki
Google Group for discussion
Gitter for support and discussion

PROTIP Be sure to read Migrating from 3.x to 4.x as well as New features in 4.x.
Security Issues
If you discover a security vulnerability in Express, please see Security Policies and Procedures.
Quick Start
The quickest way to get started with express is to utilize the executable express(1) to generate an application as shown below:
Install the executable. The executable's major version will match Express's:
$ npm install -g express-generator@4
Create the app:
$ express /tmp/foo && cd /tmp/foo
Install dependencies:
$ npm install
Start the server:
$ npm start
View the website at: http://localhost:3000
Philosophy
The Express philosophy is to provide small, robust tooling for HTTP servers, making
it a great solution for single page applications, web sites, hybrids, or public
HTTP APIs.
Express does not force you to use any specific ORM or template engine. With support for over
14 template engines via Consolidate.js,
you can quickly craft your perfect framework.
Examples
To view the examples, clone the Express repo and install the dependencies:
$ git clone git://github.com/expressjs/express.git --depth 1
$ cd express
$ npm install
Then run whichever example you want:
$ node examples/content-negotiation
Tests
To run the test suite, first install the dependencies, then run npm test:
$ npm install
$ npm test
Contributing
Contributing Guide
People
The original author of Express is TJ Holowaychuk
The current lead maintainer is Douglas Christopher Wilson
List of all contributors
License
MIT
"
24,JavaScript,"

    Simple yet flexible JavaScript charting for designers & developers








Documentation

Introduction
Getting Started
General
Configuration
Charts
Axes
Developers
Popular Extensions
Samples

Contributing
Instructions on building and testing Chart.js can be found in the documentation. Before submitting an issue or a pull request, please take a moment to look over the contributing guidelines first. For support, please post questions on Stack Overflow with the chartjs tag.
License
Chart.js is available under the MIT license.
"
25,JavaScript,"JSON Server  
Get a full fake REST API with zero coding in less than 30 seconds (seriously)
Created with <3 for front-end developers who need a quick back-end for prototyping and mocking.

Egghead.io free video tutorial - Creating demo APIs with json-server
JSONPlaceholder - Live running version
My JSON Server - no installation required, use your own data

See also:

🐶 husky - Git hooks made easy
🏨 hotel - developer tool with local .localhost domain and https out of the box

 
Gold sponsors 🥇
 





 
Bronze sponsors 🥉
 





 
Become a sponsor and have your company logo here
Table of contents

Getting started
Routes

Plural routes
Singular routes
Filter
Paginate
Sort
Slice
Operators
Full-text search
Relationships
Database
Homepage


Extras

Static file server
Alternative port
Access from anywhere
Remote schema
Generate random data
HTTPS
Add custom routes
Add middlewares
CLI usage
Module

Simple example
Custom routes example
Access control example
Custom output example
Rewriter example
Mounting JSON Server on another endpoint example
API


Deployment


Links

Video
Articles
Third-party tools


License

Getting started
Install JSON Server
npm install -g json-server

Create a db.json file with some data
{
  ""posts"": [
    { ""id"": 1, ""title"": ""json-server"", ""author"": ""typicode"" }
  ],
  ""comments"": [
    { ""id"": 1, ""body"": ""some comment"", ""postId"": 1 }
  ],
  ""profile"": { ""name"": ""typicode"" }
}
Start JSON Server
json-server --watch db.json
Now if you go to http://localhost:3000/posts/1, you'll get
{ ""id"": 1, ""title"": ""json-server"", ""author"": ""typicode"" }
Also when doing requests, it's good to know that:

If you make POST, PUT, PATCH or DELETE requests, changes will be automatically and safely saved to db.json using lowdb.
Your request body JSON should be object enclosed, just like the GET output. (for example {""name"": ""Foobar""})
Id values are not mutable. Any id value in the body of your PUT or PATCH request will be ignored. Only a value set in a POST request will be respected, but only if not already taken.
A POST, PUT or PATCH request should include a Content-Type: application/json header to use the JSON in the request body. Otherwise it will result in a 200 OK but without changes being made to the data.

Routes
Based on the previous db.json file, here are all the default routes. You can also add other routes using --routes.
Plural routes
GET    /posts
GET    /posts/1
POST   /posts
PUT    /posts/1
PATCH  /posts/1
DELETE /posts/1

Singular routes
GET    /profile
POST   /profile
PUT    /profile
PATCH  /profile

Filter
Use . to access deep properties
GET /posts?title=json-server&author=typicode
GET /posts?id=1&id=2
GET /comments?author.name=typicode

Paginate
Use _page and optionally _limit to paginate returned data.
In the Link header you'll get first, prev, next and last links.
GET /posts?_page=7
GET /posts?_page=7&_limit=20

10 items are returned by default
Sort
Add _sort and _order (ascending order by default)
GET /posts?_sort=views&_order=asc
GET /posts/1/comments?_sort=votes&_order=asc

For multiple fields, use the following format:
GET /posts?_sort=user,views&_order=desc,asc

Slice
Add _start and _end or _limit (an X-Total-Count header is included in the response)
GET /posts?_start=20&_end=30
GET /posts/1/comments?_start=20&_end=30
GET /posts/1/comments?_start=20&_limit=10

Works exactly as Array.slice (i.e. _start is inclusive and _end exclusive)
Operators
Add _gte or _lte for getting a range
GET /posts?views_gte=10&views_lte=20

Add _ne to exclude a value
GET /posts?id_ne=1

Add _like to filter (RegExp supported)
GET /posts?title_like=server

Full-text search
Add q
GET /posts?q=internet

Relationships
To include children resources, add _embed
GET /posts?_embed=comments
GET /posts/1?_embed=comments

To include parent resource, add _expand
GET /comments?_expand=post
GET /comments/1?_expand=post

To get or create nested resources (by default one level, add custom routes for more)
GET  /posts/1/comments
POST /posts/1/comments

Database
GET /db

Homepage
Returns default index file or serves ./public directory
GET /

Extras
Static file server
You can use JSON Server to serve your HTML, JS and CSS, simply create a ./public directory
or use --static to set a different static files directory.
mkdir public
echo 'hello world' > public/index.html
json-server db.json
json-server db.json --static ./some-other-dir
Alternative port
You can start JSON Server on other ports with the --port flag:
$ json-server --watch db.json --port 3004
Access from anywhere
You can access your fake API from anywhere using CORS and JSONP.
Remote schema
You can load remote schemas.
$ json-server http://example.com/file.json
$ json-server http://jsonplaceholder.typicode.com/db
Generate random data
Using JS instead of a JSON file, you can create data programmatically.
// index.js
module.exports = () => {
  const data = { users: [] }
  // Create 1000 users
  for (let i = 0; i < 1000; i++) {
    data.users.push({ id: i, name: `user${i}` })
  }
  return data
}
$ json-server index.js
Tip use modules like Faker, Casual, Chance or JSON Schema Faker.
HTTPS
There are many ways to set up SSL in development. One simple way is to use hotel.
Add custom routes
Create a routes.json file. Pay attention to start every route with /.
{
  ""/api/*"": ""/$1"",
  ""/:resource/:id/show"": ""/:resource/:id"",
  ""/posts/:category"": ""/posts?category=:category"",
  ""/articles\\?id=:id"": ""/posts/:id""
}
Start JSON Server with --routes option.
json-server db.json --routes routes.json
Now you can access resources using additional routes.
/api/posts # → /posts
/api/posts/1  # → /posts/1
/posts/1/show # → /posts/1
/posts/javascript # → /posts?category=javascript
/articles?id=1 # → /posts/1
Add middlewares
You can add your middlewares from the CLI using --middlewares option:
// hello.js
module.exports = (req, res, next) => {
  res.header('X-Hello', 'World')
  next()
}
json-server db.json --middlewares ./hello.js
json-server db.json --middlewares ./first.js ./second.js
CLI usage
json-server [options] <source>

Options:
  --config, -c       Path to config file           [default: ""json-server.json""]
  --port, -p         Set port                                    [default: 3000]
  --host, -H         Set host                             [default: ""localhost""]
  --watch, -w        Watch file(s)                                     [boolean]
  --routes, -r       Path to routes file
  --middlewares, -m  Paths to middleware files                           [array]
  --static, -s       Set static files directory
  --read-only, --ro  Allow only GET requests                           [boolean]
  --no-cors, --nc    Disable Cross-Origin Resource Sharing             [boolean]
  --no-gzip, --ng    Disable GZIP Content-Encoding                     [boolean]
  --snapshots, -S    Set snapshots directory                      [default: "".""]
  --delay, -d        Add delay to responses (ms)
  --id, -i           Set database id property (e.g. _id)         [default: ""id""]
  --foreignKeySuffix, --fks  Set foreign key suffix, (e.g. _id as in post_id)
                                                                 [default: ""Id""]
  --quiet, -q        Suppress log messages from output                 [boolean]
  --help, -h         Show help                                         [boolean]
  --version, -v      Show version number                               [boolean]

Examples:
  json-server db.json
  json-server file.js
  json-server http://example.com/db.json

https://github.com/typicode/json-server

You can also set options in a json-server.json configuration file.
{
  ""port"": 3000
}
Module
If you need to add authentication, validation, or any behavior, you can use the project as a module in combination with other Express middlewares.
Simple example
$ npm install json-server --save-dev
// server.js
const jsonServer = require('json-server')
const server = jsonServer.create()
const router = jsonServer.router('db.json')
const middlewares = jsonServer.defaults()

server.use(middlewares)
server.use(router)
server.listen(3000, () => {
  console.log('JSON Server is running')
})
$ node server.js
The path you provide to the jsonServer.router function  is relative to the directory from where you launch your node process. If you run the above code from another directory, it’s better to use an absolute path:
const path = require('path')
const router = jsonServer.router(path.join(__dirname, 'db.json'))
For an in-memory database, simply pass an object to jsonServer.router().
Please note also that jsonServer.router() can be used in existing Express projects.
Custom routes example
Let's say you want a route that echoes query parameters and another one that set a timestamp on every resource created.
const jsonServer = require('json-server')
const server = jsonServer.create()
const router = jsonServer.router('db.json')
const middlewares = jsonServer.defaults()

// Set default middlewares (logger, static, cors and no-cache)
server.use(middlewares)

// Add custom routes before JSON Server router
server.get('/echo', (req, res) => {
  res.jsonp(req.query)
})

// To handle POST, PUT and PATCH you need to use a body-parser
// You can use the one used by JSON Server
server.use(jsonServer.bodyParser)
server.use((req, res, next) => {
  if (req.method === 'POST') {
    req.body.createdAt = Date.now()
  }
  // Continue to JSON Server router
  next()
})

// Use default router
server.use(router)
server.listen(3000, () => {
  console.log('JSON Server is running')
})
Access control example
const jsonServer = require('json-server')
const server = jsonServer.create()
const router = jsonServer.router('db.json')
const middlewares = jsonServer.defaults()

server.use(middlewares)
server.use((req, res, next) => {
 if (isAuthorized(req)) { // add your authorization logic here
   next() // continue to JSON Server router
 } else {
   res.sendStatus(401)
 }
})
server.use(router)
server.listen(3000, () => {
  console.log('JSON Server is running')
})
Custom output example
To modify responses, overwrite router.render method:
// In this example, returned resources will be wrapped in a body property
router.render = (req, res) => {
  res.jsonp({
    body: res.locals.data
  })
}
You can set your own status code for the response:
// In this example we simulate a server side error response
router.render = (req, res) => {
  res.status(500).jsonp({
    error: ""error message here""
  })
}
Rewriter example
To add rewrite rules, use jsonServer.rewriter():
// Add this before server.use(router)
server.use(jsonServer.rewriter({
  '/api/*': '/$1',
  '/blog/:resource/:id/show': '/:resource/:id'
}))
Mounting JSON Server on another endpoint example
Alternatively, you can also mount the router on /api.
server.use('/api', router)
API
jsonServer.create()
Returns an Express server.
jsonServer.defaults([options])
Returns middlewares used by JSON Server.

options

static path to static files
logger enable logger middleware (default: true)
bodyParser enable body-parser middleware (default: true)
noCors disable CORS (default: false)
readOnly accept only GET requests (default: false)



jsonServer.router([path|object])
Returns JSON Server router.
Deployment
You can deploy JSON Server. For example, JSONPlaceholder is an online fake API powered by JSON Server and running on Heroku.
Links
Video

Creating Demo APIs with json-server on egghead.io

Articles

Node Module Of The Week - json-server
Mock up your REST API with JSON Server
ng-admin: Add an AngularJS admin GUI to any RESTful API
Fast prototyping using Restangular and Json-server
Create a Mock REST API in Seconds for Prototyping your Frontend
No API? No Problem! Rapid Development via Mock APIs
Zero Code REST With json-server

Third-party tools

Grunt JSON Server
Docker JSON Server
JSON Server GUI
JSON file generator
JSON Server extension

License
MIT
Supporters ✨
"
26,JavaScript,"HTML5 Boilerplate





HTML5 Boilerplate is a professional front-end template for building
fast, robust, and adaptable web apps or sites.
This project is the product of years of iterative development and
community knowledge. It does not impose a specific development
philosophy or framework, so you're free to architect your code in the
way that you want.

Homepage: https://html5boilerplate.com/
Source: https://github.com/h5bp/html5-boilerplate
Twitter: @h5bp

Quick start
Choose one of the following options:


Download the latest stable release from
html5boilerplate.com. This zip file is a snapshot of the dist folder. On Windows, Mac and from the file manager on Linux unzipping this folder will output to a folder named something like html5-boilerplate_v7.3.0. From the command line will need to create a folder and unzip the contents into that folder.
mkdir html5-boilerplate
unzip html5-boilerplate*.zip -d html5-boilerplate


Clone the git repo — git clone https://github.com/h5bp/html5-boilerplate.git - and checkout the
tagged release
you'd like to use. The dist folder represents the latest version of the project for end users.


Install with npm: npm install html5-boilerplate or yarn: yarn add html5-boilerplate. The resulting node_modules/html5-boilerplate/dist folder represents the latest version of the project for end users. Depending on what you want to use and how you want to use it, you may have to copy and paste the contents of that folder into your project directory.


Features

HTML5 ready. Use the new elements with confidence.
Designed with progressive enhancement in mind.
Includes:

Normalize.css
for CSS normalizations and common bug fixes
jQuery via CDN with SRI Hash and a local fallback
A custom build of Modernizr for feature
detection
Apache Server Configs
that, among other, improve the web site's performance and security


Placeholder CSS Media Queries.
Useful CSS helper classes.
Default print styles, performance optimized.
An optimized version of the Google Universal Analytics snippet.
Protection against any stray console statements causing JavaScript
errors in older browsers.
""Delete-key friendly."" Easy to strip out parts you don't need.
Extensive inline and accompanying documentation.

Browser support

Chrome (latest 2)
Edge (latest 2)
Firefox (latest 2)
Internet Explorer 11
Opera (latest 2)
Safari (latest 2)

This doesn't mean that HTML5 Boilerplate cannot be used in older browsers,
just that we'll ensure compatibility with the ones mentioned above.
If you need legacy browser support you can use HTML5 Boilerplate v6 (IE9/IE10)
or HTML5 Boilerplate v5 (IE 8). They are no longer actively developed.
Documentation
Take a look at the documentation table of contents.
This documentation is bundled with the project which makes it
available for offline reading and provides a useful starting point for
any documentation you want to write about your project.
Contributing
Hundreds of developers have helped to make the HTML5 Boilerplate. Anyone is welcome to contribute,
however, if you decide to get involved, please take a moment to review
the guidelines:

Bug reports
Feature requests
Pull requests

License
The code is available under the MIT license.
"
27,JavaScript,"
















    Visit https://nextjs.org/learn to get started with Next.js.
  


The below readme is the documentation for the canary (prerelease) branch. To view the documentation for the latest stable Next.js version visit nextjs.org/docs.


How to use

Setup

Quick Start
Manual Setup


Automatic code splitting
CSS

Built-in CSS support
CSS-in-JS
Importing CSS / Sass / Less / Stylus files


Static file serving (e.g.: images)
Dynamic Routing
Populating <head>
Fetching data and component lifecycle
Routing

With <Link>

With URL object
Replace instead of push url
Using a component that supports onClick
Forcing the Link to expose href to its child
Disabling the scroll changes to top on page


Imperatively
Intercepting popstate

With URL object
Router Events
Shallow Routing


useRouter
Using a Higher Order Component


Prefetching Pages

With <Link>
Imperatively


API Routes

Dynamic routes support
API Middlewares
Helper Functions


Custom server and routing

Disabling file-system routing
Dynamic assetPrefix
Changing x-powered-by


Dynamic Import

Basic Usage (Also does SSR)
With named exports
With Custom Loading Component
With No SSR


Custom <App>
Custom <Document>

Customizing renderPage


Custom error handling
Reusing the built-in error page
Custom configuration

Setting a custom build directory
Disabling etag generation
Configuring the onDemandEntries
Configuring extensions looked for when resolving pages in pages
Configuring the build ID
Configuring next process script


Customizing webpack config
Customizing babel config
Exposing configuration to the server / client side

Build-time configuration
Runtime configuration


Starting the server on alternative hostname
CDN support with Asset Prefix


Automatic Static Optimization
Automatic Static Optimization Indicator
Production deployment

Compression
Serverless deployment

One Level Lower
Summary




Browser support
TypeScript

Exported types


AMP Support

Enabling AMP Support
AMP First Page
Hybrid AMP Page
AMP Page Modes
AMP Behavior with next export
Adding AMP Components
AMP Validation
TypeScript Support


Static HTML export

Usage
Limitation


Multi Zones

How to define a zone
How to merge them


FAQ
Contributing
Authors

How to use
Setup
Quick Start
npx create-next-app
(npx comes with npm 5.2+ and higher, see instructions for older npm versions)
Manual Setup
Install it in your project:
npm install --save next react react-dom
and add a script to your package.json like this:
{
  ""scripts"": {
    ""dev"": ""next"",
    ""build"": ""next build"",
    ""start"": ""next start""
  }
}
After that, the file-system is the main API. Every .js file becomes a route that gets automatically processed and rendered.
Populate ./pages/index.js inside your project:
function Home() {
  return <div>Welcome to Next.js!</div>
}

export default Home
and then just run npm run dev and go to http://localhost:3000. To use another port, you can run npm run dev -- -p <your port here>.
So far, we get:

Automatic transpilation and bundling (with webpack and babel)
Hot code reloading
Server rendering and indexing of ./pages/
Static file serving. ./public/ is mapped to / (given you create a ./public/ directory inside your project)

Automatic code splitting
Every import you declare gets bundled and served with each page. That means pages never load unnecessary code!
import cowsay from 'cowsay-browser'

function CowsayHi() {
  return <pre>{cowsay.say({ text: 'hi there!' })}</pre>
}

export default CowsayHi
CSS
Built-in CSS support

Examples

Basic css


We bundle styled-jsx to provide support for isolated scoped CSS. The aim is to support ""shadow CSS"" similar to Web Components, which unfortunately do not support server-rendering and are JS-only.
function HelloWorld() {
  return (
    <div>
      Hello world
      <p>scoped!</p>
      <style jsx>{`
        p {
          color: blue;
        }
        div {
          background: red;
        }
        @media (max-width: 600px) {
          div {
            background: blue;
          }
        }
      `}</style>
      <style global jsx>{`
        body {
          background: black;
        }
      `}</style>
    </div>
  )
}

export default HelloWorld
Please see the styled-jsx documentation for more examples.
CSS-in-JS


Examples


Styled components
Styletron
Glamor
Cxs
Aphrodite
Fela


It's possible to use any existing CSS-in-JS solution. The simplest one is inline styles:
function HiThere() {
  return <p style={{ color: 'red' }}>hi there</p>
}

export default HiThere
To use more sophisticated CSS-in-JS solutions, you typically have to implement style flushing for server-side rendering. We enable this by allowing you to define your own custom <Document> component that wraps each page.
Importing CSS / Sass / Less / Stylus files
To support importing .css, .scss, .less or .styl files you can use these modules, which configure sensible defaults for server rendered applications.

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-stylus

Static file serving (e.g.: images)
Create a folder called public in your project root directory. From your code you can then reference those files starting from the baseURL /
function MyImage() {
  return <img src=""/my-image.png"" alt=""my image"" />
}

export default MyImage
Note: Don't name the public directory anything else. The name can't be changed and is the only directory that Next.js uses for serving static assets.
Dynamic Routing

Examples

Dynamic routing


Defining routes by using predefined paths is not always enough for complex applications, in Next.js you can add brackets to a page ([param]) to create a dynamic route (a.k.a. url slugs, pretty urls, et al).
Consider the following page pages/post/[pid].js:
import { useRouter } from 'next/router'

const Post = () => {
  const router = useRouter()
  const { pid } = router.query

  return <p>Post: {pid}</p>
}

export default Post
Any route like /post/1, /post/abc, etc will be matched by pages/post/[pid].js.
The matched path parameter will be sent as a query parameter to the page.
For example, the route /post/abc will have the following query object: { pid: 'abc' }.
Similarly, the route /post/abc?foo=bar will have the query object: { foo: 'bar', pid: 'abc' }.

Note: Multiple dynamic route segments work the same way.
For example, pages/post/[pid]/[comment].js would match /post/1/a-comment.
Its query object would be: { pid: '1', comment: 'a-comment' }.

A <Link> for /post/abc looks like so:
<Link href=""/post/[pid]"" as=""/post/abc"">
  <a>First Post</a>
</Link>

href: the path inside pages directory.
as: the path that will be rendered in the browser URL bar.

As href is a filesystem path, it shouldn't change at runtime, instead, you will probably need to change as
dynamically according to your needs. Here's an example to create a list of links:
const pids = ['id1', 'id2', 'id3']
{
  pids.map(pid => (
    <Link href=""/post/[pid]"" as={`/post/${pid}`}>
      <a>Post {pid}</a>
    </Link>
  ))
}

You can read more about <Link> here.

However, if a query and route param name are the same, route parameters will override the matching query params.
For example, /post/abc?pid=bcd will have the query object: { pid: 'abc' }.

Note: Predefined routes take precedence over dynamic routes.
For example, if you have pages/post/[pid].js and pages/post/create.js, the route /post/create will be matched by pages/post/create.js instead of the dynamic route ([pid]).


Note: Pages that are statically optimized by automatic static optimization will be hydrated without their route parameters provided (query will be empty, i.e. {}).
After hydration, Next.js will trigger an update to your application to provide the route parameters in the query object.
If your application cannot tolerate this behavior, you can opt-out of static optimization by capturing the query parameter in getInitialProps.


Note: If deploying to ZEIT Now dynamic routes will work out-of-the-box.
You do not need to configure custom routes in a now.json file.
If you are new to ZEIT Now, you can learn how to deploy a Next.js app to it in the Deploying a Next.js App Learn section.

Populating <head>

Examples

Head elements
Layout component


We expose a built-in component for appending elements to the <head> of the page.
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta name=""viewport"" content=""initial-scale=1.0, width=device-width"" />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
To avoid duplicate tags in your <head> you can use the key property, which will make sure the tag is only rendered once:
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta
          name=""viewport""
          content=""initial-scale=1.0, width=device-width""
          key=""viewport""
        />
      </Head>
      <Head>
        <meta
          name=""viewport""
          content=""initial-scale=1.2, width=device-width""
          key=""viewport""
        />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
In this case only the second <meta name=""viewport"" /> is rendered.
Note: The contents of <head> get cleared upon unmounting the component, so make sure each page completely defines what it needs in <head>, without making assumptions about what other pages added.
Note: <title> and <meta> elements need to be contained as direct children of the <Head> element, or wrapped into maximum one level of <React.Fragment>, otherwise the metatags won't be correctly picked up on clientside navigation.
Fetching data and component lifecycle

Examples

Data fetch


When you need state, lifecycle hooks or initial data population you can export a function component that uses Hooks or a class component.
Using a function component:
import fetch from 'isomorphic-unfetch'

function Page({ stars }) {
  return <div>Next stars: {stars}</div>
}

Page.getInitialProps = async ({ req }) => {
  const res = await fetch('https://api.github.com/repos/zeit/next.js')
  const json = await res.json()
  return { stars: json.stargazers_count }
}

export default Page
Using a class component:
import React from 'react'

class HelloUA extends React.Component {
  static async getInitialProps({ req }) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    return <div>Hello World {this.props.userAgent}</div>
  }
}

export default HelloUA
Notice that to load data when the page loads, we use getInitialProps which is an async static method. It can asynchronously fetch anything that resolves to a JavaScript plain Object, which populates props.
Data returned from getInitialProps is serialized when server rendering, similar to a JSON.stringify. Make sure the returned object from getInitialProps is a plain Object and not using Date, Map or Set.
For the initial page load, getInitialProps will execute on the server only. getInitialProps will only be executed on the client when navigating to a different route via the Link component or using the routing APIs.



getInitialProps can not be used in children components. Only in pages.
If you are using some server only modules inside getInitialProps, make sure to import them properly, otherwise, it'll slow down your app.



getInitialProps receives a context object with the following properties:

pathname - path section of URL
query - query string section of URL parsed as an object
asPath - String of the actual path (including the query) shows in the browser
req - HTTP request object (server only)
res - HTTP response object (server only)
err - Error object if any error is encountered during the rendering

Routing
Next.js does not ship a routes manifest with every possible route in the application, so the current page is not aware of any other pages on the client side. All subsequent routes get lazy-loaded, for scalability sake.
With <Link>

Examples

Hello World


Client-side transitions between routes can be enabled via a <Link> component.

This component is not required for navigations to static pages that require a hard refresh, like when using AMP.

Basic Example
Consider these two pages:
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <>
      <ul>
        <li>Home</li>
        <li>
          <Link href=""/about"">
            <a>About Us</a>
          </Link>
        </li>
      </ul>

      <h1>This is our homepage.</h1>
    </>
  )
}

export default Home
// pages/about.js
import Link from 'next/link'

function About() {
  return (
    <>
      <ul>
        <li>
          <Link href=""/"">
            <a>Home</a>
          </Link>
        </li>
        <li>About Us</li>
      </ul>

      <h1>About</h1>
      <p>We are a cool company.</p>
    </>
  )
}

export default About
Note: if passing a functional component as a child of <Link> you will need to wrap it in React.forwardRef
Example with React.forwardRef
import React from 'react'
import Link from 'next/link'

// `onClick`, `href`, and `ref` need to be passed to the DOM element
// for proper handling
const MyButton = React.forwardRef(({ onClick, href }, ref) => (
  <a href={href} onClick={onClick} ref={ref}>
    Click Me
  </a>
))

export default () => (
  <>
    <Link href=""/another"">
      <MyButton />
    </Link>
  </>
)
Custom routes (using props from URL)
If you find that your use case is not covered by Dynamic Routing then you can create a custom server and manually add dynamic routes.
Example:


Consider you have the URL /post/:slug.


You created pages/post.js:
import { useRouter } from 'next/router'

const Post = () => {
  const router = useRouter()
  const { slug } = router.query

  return <p>My Blog Post: {slug}</p>
}

export default Post


You add the route to express (or any other server) on server.js file (this is only for SSR). This will route the url /post/:slug to pages/post.js and provide slug as part of the query object to the page.
server.get('/post/:slug', (req, res) => {
  return app.render(req, res, '/post', { slug: req.params.slug })
})


For client side routing, use next/link:
<Link href=""/post?slug=something"" as=""/post/something"">

href: the path inside pages directory
as: the path used by your server routes



Client-side routing behaves exactly like the browser:

The component is fetched.
If it defines getInitialProps, data is fetched. If an error occurs, _error.js is rendered.
After 1 and 2 complete, pushState is performed and the new component is rendered.

To inject the pathname, query or asPath in your component, you can use the useRouter hook, or withRouter for class components.
With URL object

Examples

With URL Object Routing


The component <Link> can also receive a URL object and it will automatically format it to create the URL string.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href={{ pathname: '/about', query: { name: 'Zeit' } }}>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
That will generate the URL string /about?name=Zeit, you can use every property as defined in the Node.js URL module documentation.
Replace instead of push url
The default behaviour for the <Link> component is to push a new url into the stack. You can use the replace prop to prevent adding a new entry.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"" replace>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
Using a component that supports onClick
<Link> supports any component that supports the onClick event. In case you don't provide an <a> tag, it will only add the onClick event handler and won't pass the href property.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"">
        <img src=""/static/image.png"" alt=""image"" />
      </Link>
    </div>
  )
}

export default Home
Forcing the Link to expose href to its child
If child is an <a> tag and doesn't have a href attribute we specify it so that the repetition is not needed by the user. However, sometimes, you’ll want to pass an <a> tag inside of a wrapper and the Link won’t recognize it as a hyperlink, and, consequently, won’t transfer its href to the child. In cases like that, you should define a boolean passHref property to the Link, forcing it to expose its href property to the child.
Please note: using a tag other than a and failing to pass passHref may result in links that appear to navigate correctly, but, when being crawled by search engines, will not be recognized as links (owing to the lack of href attribute). This may result in negative effects on your site’s SEO.
import Link from 'next/link'
import Unexpected_A from 'third-library'

function NavLink({ href, name }) {
  return (
    <Link href={href} passHref>
      <Unexpected_A>{name}</Unexpected_A>
    </Link>
  )
}

export default NavLink
Disabling the scroll changes to top on page
The default behaviour of <Link> is to scroll to the top of the page. When there is a hash defined it will scroll to the specific id, just like a normal <a> tag. To prevent scrolling to the top / hash scroll={false} can be added to <Link>:
<Link scroll={false} href=""/?counter=10""><a>Disables scrolling</a></Link>
<Link href=""/?counter=10""><a>Changes with scrolling to top</a></Link>
Imperatively

Examples

Basic routing
With a page loading indicator


You can also do client-side page transitions using next/router:
import Router from 'next/router'

function ReadMore() {
  return (
    <div>
      Click <span onClick={() => Router.push('/about')}>here</span> to read more
    </div>
  )
}

export default ReadMore
Intercepting popstate
In some cases (for example, if using a custom router), you may wish
to listen to popstate and react before the router acts on it.
For example, you could use this to manipulate the request, or force an SSR refresh.
import Router from 'next/router'

Router.beforePopState(({ url, as, options }) => {
  // I only want to allow these two routes!
  if (as !== '/' && as !== '/other') {
    // Have SSR render bad routes as a 404.
    window.location.href = as
    return false
  }

  return true
})
If the function you pass into beforePopState returns false, Router will not handle popstate;
you'll be responsible for handling it, in that case.
See Disabling File-System Routing.
Above Router object comes with the following API:

route - String of the current route
pathname - String of the current path excluding the query string
query - Object with the parsed query string. Defaults to {}.
asPath - String of the actual path (including the query) shows in the browser
push(url, as=url) - performs a pushState call with the given url
replace(url, as=url) - performs a replaceState call with the given url
beforePopState(cb=function) - intercept popstate before router processes the event

The second as parameter for push and replace is an optional decoration of the URL. Useful if you configured custom routes on the server.
With URL object
You can use a URL object the same way you use it in a <Link> component to push and replace a URL.
import Router from 'next/router'

const handler = () => {
  Router.push({
    pathname: '/about',
    query: { name: 'Zeit' },
  })
}

function ReadMore() {
  return (
    <div>
      Click <span onClick={handler}>here</span> to read more
    </div>
  )
}

export default ReadMore
This uses the same exact parameters as in the <Link> component. The first parameter maps to href while the second parameter maps to as in the <Link> component as documented here.
Router Events
You can also listen to different events happening inside the Router.
Here's a list of supported events:

routeChangeStart(url) - Fires when a route starts to change
routeChangeComplete(url) - Fires when a route changed completely
routeChangeError(err, url) - Fires when there's an error when changing routes, or a route load is cancelled
beforeHistoryChange(url) - Fires just before changing the browser's history
hashChangeStart(url) - Fires when the hash will change but not the page
hashChangeComplete(url) - Fires when the hash has changed but not the page


Here url is the URL shown in the browser. If you call Router.push(url, as) (or similar), then the value of url will be as.

Here's how to properly listen to the router event routeChangeStart:
const handleRouteChange = url => {
  console.log('App is changing to: ', url)
}

Router.events.on('routeChangeStart', handleRouteChange)
If you no longer want to listen to that event, you can unsubscribe with the off method:
Router.events.off('routeChangeStart', handleRouteChange)
If a route load is cancelled (for example by clicking two links rapidly in succession), routeChangeError will fire. The passed err will contain a cancelled property set to true.
Router.events.on('routeChangeError', (err, url) => {
  if (err.cancelled) {
    console.log(`Route to ${url} was cancelled!`)
  }
})

Note: Using router events in getInitialProps is discouraged as it may result in unexpected behavior.
Router events should be registered when a component mounts (useEffect or componentDidMount/componentWillUnmount) or imperatively when an event happens.
useEffect(() => {
  const handleRouteChange = url => {
    console.log('App is changing to: ', url)
  }

  Router.events.on('routeChangeStart', handleRouteChange)
  return () => {
    Router.events.off('routeChangeStart', handleRouteChange)
  }
}, [])

Shallow Routing

Examples

Shallow Routing


Shallow routing allows you to change the URL without running getInitialProps. You'll receive the updated pathname and the query via the router prop (injected by using useRouter or withRouter), without losing state.
You can do this by invoking either Router.push or Router.replace with the shallow: true option. Here's an example:
// Current URL is ""/""
const href = '/?counter=10'
const as = href
Router.push(href, as, { shallow: true })
Now, the URL is updated to /?counter=10. You can see the updated URL with this.props.router.query inside the Component (make sure you are using withRouter around your Component to inject the router prop).
You can watch for URL changes via componentDidUpdate hook as shown below:
componentDidUpdate(prevProps) {
  const { pathname, query } = this.props.router
  // verify props have changed to avoid an infinite loop
  if (query.id !== prevProps.router.query.id) {
    // fetch data based on the new query
  }
}

NOTES:
Shallow routing works only for same page URL changes. For an example, let's assume we have another page called about, and you run this:
Router.push('/?counter=10', '/about?counter=10', { shallow: true })
Since that's a new page, it'll unload the current page, load the new one and call getInitialProps even though we asked to do shallow routing.

useRouter

Examples

Dynamic routing


If you want to access the router object inside any functional component in your app, you can use the useRouter hook, here's how to use it:
import { useRouter } from 'next/router'

export default function ActiveLink({ children, href }) {
  const router = useRouter()
  const style = {
    marginRight: 10,
    color: router.pathname === href ? 'red' : 'black',
  }

  const handleClick = e => {
    e.preventDefault()
    router.push(href)
  }

  return (
    <a href={href} onClick={handleClick} style={style}>
      {children}
    </a>
  )
}

Note: useRouter is a React hook, meaning it cannot be used with classes.
You can either use withRouter (a higher order component) or wrap your class in a functional component.

The above router object comes with an API similar to next/router.
Using a Higher Order Component

Examples

Using the `withRouter` utility


If useRouter is not the best fit for you, withRouter can also add the same router object to any component, here's how to use it:
import { withRouter } from 'next/router'

function Page({ router }) {
  return <p>{router.pathname}</p>
}

export default withRouter(Page)
Prefetching Pages
⚠️ This is a production only feature ⚠️

Examples

Prefetching


Next.js has an API which allows you to prefetch pages.
Since Next.js server-renders your pages, this allows all the future interaction paths of your app to be instant. Effectively Next.js gives you the great initial download performance of a website, with the ahead-of-time download capabilities of an app. Read more.

With prefetching Next.js only downloads JS code. When the page is getting rendered, you may need to wait for the data.


Automatic prefetching is disabled if your device is connected with 2G network or Save-Data header is on.


<link rel=""preload""> is used for prefetching. Sometimes browsers will show a warning if the resource is not used within 3 seconds, these warnings can be ignored as per https://github.com/zeit/next.js/issues/6517#issuecomment-469063892.

With <Link>
<Link> will automatically prefetch pages in the background as they appear in the view. If certain pages are rarely visited you can manually set prefetch to false, here's how:
<Link href=""/about"" prefetch={false}>
  <a>About</a>
</Link>
Imperatively
Most prefetching needs are addressed by <Link />, but we also expose an imperative API for advanced usage:
import { useRouter } from 'next/router'

export default function MyLink() {
  const router = useRouter()

  return (
    <>
      <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
        A route transition will happen after 100ms
      </a>
      {// and we can prefetch it!
      router.prefetch('/dynamic')}
    </>
  )
}
router methods should be only used inside the client side of your app though. In order to prevent any error regarding this subject use the imperatively prefetch method in the useEffect() hook:
import { useRouter } from 'next/router'

export default function MyLink() {
  const router = useRouter()

  useEffect(() => {
    router.prefetch('/dynamic')
  })

  return (
    <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
      A route transition will happen after 100ms
    </a>
  )
}
You can also add it to the componentDidMount() lifecycle method when using React.Component:
import React from 'react'
import { withRouter } from 'next/router'

class MyLink extends React.Component {
  componentDidMount() {
    const { router } = this.props
    router.prefetch('/dynamic')
  }

  render() {
    const { router } = this.props

    return (
      <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
        A route transition will happen after 100ms
      </a>
    )
  }
}

export default withRouter(MyLink)
API Routes

Examples

Basic API routes
API routes with micro
API routes with middleware
API routes with GraphQL server
API routes with REST


API routes provide a straightforward solution to build your API with Next.js.
Start by creating the api/ folder inside the ./pages/ folder.
Every file inside ./pages/api is mapped to /api/*.
For example, ./pages/api/posts.js is mapped to the route /api/posts.
Here's an example API route file:
export default (req, res) => {
  res.setHeader('Content-Type', 'application/json')
  res.statusCode = 200
  res.end(JSON.stringify({ name: 'Nextjs' }))
}


req refers to NextApiRequest which extends http.IncomingMessage


res refers to NextApiResponse which extends http.ServerResponse


For API routes there are built-in types NextApiRequest and NextApiResponse, which extend the Node.js request and response objects.
import { NextApiRequest, NextApiResponse } from 'next'

export default (req: NextApiRequest, res: NextApiResponse) => {
  res.status(200).json({ title: 'Next.js' })
}
To handle different HTTP methods for API calls you can access req.method in your resolver function:
export default (req, res) => {
  if (req.method === 'POST') {
    // Process your POST request
  } else {
    // Handle the rest of your HTTP methods
  }
}

Note: API Routes do not specify CORS headers, so they'll be same-origin only by default.
You can customize this behavior by wrapping your export with CORS middleware.
We provide an example of this below.

API Routes do not increase your client-side bundle size. They are server-side only bundles.
Dynamic routes support
API pages support dynamic routing, so you can use all benefits mentioned already above.
Consider the following page ./pages/api/post/[pid].js, here is how you get parameters inside the resolver method:
export default (req, res) => {
  const {
    query: { pid },
  } = req

  res.end(`Post: ${pid}`)
}
API Middlewares
API routes provides built in middlewares which parse the incoming req.
Those middlewares are:

req.cookies - an object containing the cookies sent by the request. Defaults to {}
req.query - an object containing the query string. Defaults to {}
req.body - an object containing the body parsed by content-type, or null if no body is sent

Body parsing is enabled by default with a size limit of 1mb for the parsed body.
You can opt-out of automatic body parsing if you need to consume it as a Stream:
// ./pages/api/my-endpoint.js
export default (req, res) => {
  // ...
}

export const config = {
  api: {
    bodyParser: false,
  },
}
You can adjust size of parsed body by adding sizeLimit key to bodyParser, supported values are by bytes library.
// ./pages/api/my-endpoint.js
export default (req, res) => {
  // ...
}

export const config = {
  api: {
    bodyParser: {
      sizeLimit: '1mb',
    },
  },
}
As an added bonus, you can also use any Micro compatible middleware!
For example, configuring CORS for your API endpoint can be done leveraging micro-cors.
First, install micro-cors:
npm i micro-cors
# or
yarn add micro-cors
Then, import micro-cors and configure it. Finally, wrap your exported function in the middleware:
import Cors from 'micro-cors'

const cors = Cors({
  allowMethods: ['GET', 'HEAD'],
})

function Endpoint(req, res) {
  res.json({ message: 'Hello Everyone!' })
}

export default cors(Endpoint)
Helper Functions
We're providing a set of Express.js-like methods to improve the developer experience and increase the speed of creating new API endpoints:
export default (req, res) => {
  res.status(200).json({ name: 'Next.js' })
}

res.status(code) - a function to set the status code. code must be a valid HTTP status code
res.json(json) - Sends a JSON response. json must be a valid JSON object
res.send(body) - Sends the HTTP response. body can be a string, an object or a Buffer

Custom server and routing

Examples

Basic custom server
Express integration
Hapi integration
Koa integration
SSR caching


Typically you start your next server with next start. It's possible, however, to start a server 100% programmatically in order to customize routes, use route patterns, etc.
When using a custom server with a server file, for example called server.js, make sure you update the scripts key in package.json to:
{
  ""scripts"": {
    ""dev"": ""node server.js"",
    ""build"": ""next build"",
    ""start"": ""NODE_ENV=production node server.js""
  }
}
This example makes /a resolve to ./pages/b, and /b resolve to ./pages/a:
// This file doesn't go through babel or webpack transformation.
// Make sure the syntax and sources this file requires are compatible with the current node version you are running
// See https://github.com/zeit/next.js/issues/1245 for discussions on Universal Webpack or universal Babel
const { createServer } = require('http')
const { parse } = require('url')
const next = require('next')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()

app.prepare().then(() => {
  createServer((req, res) => {
    // Be sure to pass `true` as the second argument to `url.parse`.
    // This tells it to parse the query portion of the URL.
    const parsedUrl = parse(req.url, true)
    const { pathname, query } = parsedUrl

    if (pathname === '/a') {
      app.render(req, res, '/b', query)
    } else if (pathname === '/b') {
      app.render(req, res, '/a', query)
    } else {
      handle(req, res, parsedUrl)
    }
  }).listen(3000, err => {
    if (err) throw err
    console.log('> Ready on http://localhost:3000')
  })
})
The next API is as follows:

next(opts: object)

Supported options:

dev (bool) whether to launch Next.js in dev mode - default false
dir (string) where the Next project is located - default '.'
quiet (bool) Hide error messages containing server information - default false
conf (object) the same object you would use in next.config.js - default {}

Then, change your start script to NODE_ENV=production node server.js.
Disabling file-system routing
By default, Next will serve each file in /pages under a pathname matching the filename (eg, /pages/some-file.js is served at site.com/some-file.
If your project uses custom routing, this behavior may result in the same content being served from multiple paths, which can present problems with SEO and UX.
To disable this behavior & prevent routing based on files in /pages, simply set the following option in your next.config.js:
// next.config.js
module.exports = {
  useFileSystemPublicRoutes: false,
}
Note that useFileSystemPublicRoutes simply disables filename routes from SSR; client-side routing may still access those paths. If using this option, you should guard against navigation to routes you do not want programmatically.
You may also wish to configure the client-side Router to disallow client-side redirects to filename routes; please refer to Intercepting popstate.
Dynamic assetPrefix
Sometimes we need to set the assetPrefix dynamically. This is useful when changing the assetPrefix based on incoming requests.
For that, we can use app.setAssetPrefix.
Here's an example usage of it:
const next = require('next')
const http = require('http')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handleNextRequests = app.getRequestHandler()

app.prepare().then(() => {
  const server = new http.Server((req, res) => {
    // Add assetPrefix support based on the hostname
    if (req.headers.host === 'my-app.com') {
      app.setAssetPrefix('http://cdn.com/myapp')
    } else {
      app.setAssetPrefix('')
    }

    handleNextRequests(req, res)
  })

  server.listen(port, err => {
    if (err) {
      throw err
    }

    console.log(`> Ready on http://localhost:${port}`)
  })
})
Changing x-powered-by
By default Next.js will add x-powered-by to the request headers. There's an optional way to opt-out of this:
// next.config.js
module.exports = {
  poweredByHeader: false,
}
Dynamic Import

Examples

With Dynamic Import


Next.js supports ES2020 dynamic import() for JavaScript.
With that, you could import JavaScript modules (inc. React Components) dynamically and work with them.
You can think dynamic imports as another way to split your code into manageable chunks.
Since Next.js supports dynamic imports with SSR, you could do amazing things with it.
Here are a few ways to use dynamic imports.
Basic Usage (Also does SSR)
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() => import('../components/hello'))

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With named exports
// components/hello.js
export function Hello() {
  return <p>Hello!</p>
}
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() =>
  import('../components/hello').then(mod => mod.Hello)
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With Custom Loading Component
import dynamic from 'next/dynamic'

const DynamicComponentWithCustomLoading = dynamic(
  () => import('../components/hello2'),
  { loading: () => <p>...</p> }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithCustomLoading />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With No SSR
import dynamic from 'next/dynamic'

const DynamicComponentWithNoSSR = dynamic(
  () => import('../components/hello3'),
  { ssr: false }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithNoSSR />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
Custom <App>

Examples

Using `_app.js` for layout
Using `_app.js` to override `componentDidCatch`


Next.js uses the App component to initialize pages. You can override it and control the page initialization. Which allows you to do amazing things like:

Persisting layout between page changes
Keeping state when navigating pages
Inject additional data into pages (for example by processing GraphQL queries)

To override, create the ./pages/_app.js file and override the App class as shown below:
function MyApp({ Component, pageProps }) {
  return <Component {...pageProps} />
}

// Only uncomment this method if you have blocking data requirements for
// every single page in your application. This disables the ability to
// perform automatic static optimization, causing every page in your app to
// be server-side rendered.
//
// MyApp.getInitialProps = async (appContext) => {
//   // calls page's `getInitialProps` and fills `appProps.pageProps`
//   const appProps = await App.getInitialProps(appContext);
//
//   return { ...appProps }
// }

export default MyApp

Note: Adding a custom getInitialProps in App will affect Automatic Static Optimization

Custom <Document>

Examples

Styled components custom document


A custom <Document> is commonly used to augment your application's <html> and <body> tags.
This is necessary because Next.js pages skip the definition of the surrounding document's markup.
This allows you to support Server-Side Rendering for CSS-in-JS libraries like
styled-components or emotion.
Note, styled-jsx is included in Next.js by default.
A custom <Document> can also include getInitialProps for expressing asynchronous server-rendering data requirements.

Note: <Document>'s getInitialProps function is not called during client-side transitions,
nor when a page is automatically statically optimized.


Note: Make sure to check if ctx.req / ctx.res are defined in getInitialProps.
These variables will be undefined when a page is being statically exported for next export or automatic static optimization.

To use a custom <Document>, you must create a file at ./pages/_document.js and extend the Document class:
// _document is only rendered on the server side and not on the client side
// Event handlers like onClick can't be added to this file

// ./pages/_document.js
import Document, { Html, Head, Main, NextScript } from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const initialProps = await Document.getInitialProps(ctx)
    return { ...initialProps }
  }

  render() {
    return (
      <Html>
        <Head />
        <body>
          <Main />
          <NextScript />
        </body>
      </Html>
    )
  }
}

export default MyDocument
All of <Html>, <Head />, <Main /> and <NextScript /> are required for page to be properly rendered.
Note: React-components outside of <Main /> will not be initialised by the browser. Do not add application logic here. If you need shared components in all your pages (like a menu or a toolbar), take a look at the <App> component instead.
The ctx object is equivalent to the one received in all getInitialProps hooks, with one addition:

renderPage (Function) a callback that executes the actual React rendering logic (synchronously). It's useful to decorate this function in order to support server-rendering wrappers like Aphrodite's renderStatic.

Customizing renderPage
🚧 It should be noted that the only reason you should be customizing renderPage is for usage with css-in-js libraries
that need to wrap the application to properly work with server-rendering. 🚧

It takes as argument an options object for further customization:

import Document from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const originalRenderPage = ctx.renderPage

    ctx.renderPage = () =>
      originalRenderPage({
        // useful for wrapping the whole react tree
        enhanceApp: App => App,
        // useful for wrapping in a per-page basis
        enhanceComponent: Component => Component,
      })

    // Run the parent `getInitialProps` using `ctx` that now includes our custom `renderPage`
    const initialProps = await Document.getInitialProps(ctx)

    return initialProps
  }
}

export default MyDocument
Custom error handling
404 or 500 errors are handled both client and server side by a default component error.js. If you wish to override it, define a _error.js in the pages folder:
⚠️ The pages/_error.js component is only used in production. In development you get an error with call stack to know where the error originated from. ⚠️
import React from 'react'

function Error({ statusCode }) {
  return (
    <p>
      {statusCode
        ? `An error ${statusCode} occurred on server`
        : 'An error occurred on client'}
    </p>
  )
}

Error.getInitialProps = ({ res, err }) => {
  const statusCode = res ? res.statusCode : err ? err.statusCode : 404
  return { statusCode }
}

export default Error
Reusing the built-in error page
If you want to render the built-in error page you can by using next/error:
import React from 'react'
import Error from 'next/error'
import fetch from 'isomorphic-unfetch'

const Page = ({ errorCode, stars }) => {
  if (errorCode) {
    return <Error statusCode={errorCode} />
  }

  return <div>Next stars: {stars}</div>
}

Page.getInitialProps = async () => {
  const res = await fetch('https://api.github.com/repos/zeit/next.js')
  const errorCode = res.statusCode > 200 ? res.statusCode : false
  const json = await res.json()

  return { errorCode, stars: json.stargazers_count }
}

export default Page

If you have created a custom error page you have to import your own _error component from ./_error instead of next/error.

The Error component also takes title as a property if you want to pass in a text message along with a statusCode.
Custom configuration
For custom advanced behavior of Next.js, you can create a next.config.js in the root of your project directory (next to pages/ and package.json).
Note: next.config.js is a regular Node.js module, not a JSON file. It gets used by the Next server and build phases, and not included in the browser build.
// next.config.js
module.exports = {
  /* config options here */
}
Or use a function:
module.exports = (phase, { defaultConfig }) => {
  return {
    /* config options here */
  }
}
phase is the current context in which the configuration is loaded. You can see all phases here: constants
Phases can be imported from next/constants:
const { PHASE_DEVELOPMENT_SERVER } = require('next/constants')
module.exports = (phase, { defaultConfig }) => {
  if (phase === PHASE_DEVELOPMENT_SERVER) {
    return {
      /* development only config options here */
    }
  }

  return {
    /* config options for all phases except development here */
  }
}
Setting a custom build directory
You can specify a name to use for a custom build directory. For example, the following config will create a build folder instead of a .next folder. If no configuration is specified then next will create a .next folder.
// next.config.js
module.exports = {
  distDir: 'build',
}
Disabling etag generation
You can disable etag generation for HTML pages depending on your cache strategy. If no configuration is specified then Next will generate etags for every page.
// next.config.js
module.exports = {
  generateEtags: false,
}
Configuring the onDemandEntries
Next exposes some options that give you some control over how the server will dispose or keep in memories pages built:
module.exports = {
  onDemandEntries: {
    // period (in ms) where the server will keep pages in the buffer
    maxInactiveAge: 25 * 1000,
    // number of pages that should be kept simultaneously without being disposed
    pagesBufferLength: 2,
  },
}
This is development-only feature. If you want to cache SSR pages in production, please see SSR-caching example.
Configuring extensions looked for when resolving pages in pages
Aimed at modules like @next/mdx, that add support for pages ending with .mdx. pageExtensions allows you to configure the extensions looked for in the pages directory when resolving pages.
// next.config.js
module.exports = {
  pageExtensions: ['mdx', 'jsx', 'js'],
}
Configuring the build ID
Next.js uses a constant generated at build time to identify which version of your application is being served. This can cause problems in multi-server deployments when next build is ran on every server. In order to keep a static build id between builds you can provide the generateBuildId function:
// next.config.js
module.exports = {
  generateBuildId: async () => {
    // For example get the latest git commit hash here
    return 'my-build-id'
  },
}
To fall back to the default of generating a unique id return null from the function:
module.exports = {
  generateBuildId: async () => {
    // When process.env.YOUR_BUILD_ID is undefined we fall back to the default
    if (process.env.YOUR_BUILD_ID) {
      return process.env.YOUR_BUILD_ID
    }

    return null
  },
}
Configuring next process script
You can pass any node arguments to next CLI command.
NODE_OPTIONS=""--throw-deprecation"" next
NODE_OPTIONS=""-r esm"" next
NODE_OPTIONS=""--inspect"" next
Customizing webpack config

Examples

Custom webpack bundle analyzer


Some commonly asked for features are available as modules:

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-preact
@next/mdx


Warning: The webpack function is executed twice, once for the server and once for the client. This allows you to distinguish between client and server configuration using the isServer property.

Multiple configurations can be combined together with function composition. For example:
const withMDX = require('@next/mdx')
const withSass = require('@zeit/next-sass')

module.exports = withMDX(
  withSass({
    webpack(config, options) {
      // Further custom configuration here
      return config
    },
  })
)
In order to extend our usage of webpack, you can define a function that extends its config via next.config.js.
// next.config.js is not transformed by Babel. So you can only use javascript features supported by your version of Node.js.

module.exports = {
  webpack: (config, { buildId, dev, isServer, defaultLoaders, webpack }) => {
    // Note: we provide webpack above so you should not `require` it
    // Perform customizations to webpack config
    // Important: return the modified config

    // Example using webpack option
    config.plugins.push(new webpack.IgnorePlugin(/\/__tests__\//))
    return config
  },
  webpackDevMiddleware: config => {
    // Perform customizations to webpack dev middleware config
    // Important: return the modified config
    return config
  },
}
The second argument to webpack is an object containing properties useful when customizing its configuration:

buildId - String the build id used as a unique identifier between builds
dev - Boolean shows if the compilation is done in development mode
isServer - Boolean shows if the resulting configuration will be used for server side (true), or client side compilation (false)
defaultLoaders - Object Holds loader objects Next.js uses internally, so that you can use them in custom configuration

babel - Object the babel-loader configuration for Next.js



Example usage of defaultLoaders.babel:
// Example next.config.js for adding a loader that depends on babel-loader
// This source was taken from the @next/mdx plugin source:
// https://github.com/zeit/next.js/tree/canary/packages/next-mdx
module.exports = {
  webpack: (config, options) => {
    config.module.rules.push({
      test: /\.mdx/,
      use: [
        options.defaultLoaders.babel,
        {
          loader: '@mdx-js/loader',
          options: pluginOptions.options,
        },
      ],
    })

    return config
  },
}
Customizing babel config

Examples

Custom babel configuration


In order to extend our usage of babel, you can simply define a .babelrc file at the root of your app. This file is optional.
If found, we're going to consider it the source of truth, therefore it needs to define what next needs as well, which is the next/babel preset.
This is designed so that you are not surprised by modifications we could make to the babel configurations.
Here's an example .babelrc file:
{
  ""presets"": [""next/babel""],
  ""plugins"": []
}
The next/babel preset includes everything needed to transpile React applications. This includes:

preset-env
preset-react
preset-typescript
plugin-proposal-class-properties
plugin-proposal-object-rest-spread
plugin-transform-runtime
styled-jsx

These presets / plugins should not be added to your custom .babelrc. Instead, you can configure them on the next/babel preset:
{
  ""presets"": [
    [
      ""next/babel"",
      {
        ""preset-env"": {},
        ""transform-runtime"": {},
        ""styled-jsx"": {},
        ""class-properties"": {}
      }
    ]
  ],
  ""plugins"": []
}
The modules option on ""preset-env"" should be kept to false otherwise webpack code splitting is disabled.
Exposing configuration to the server / client side
There is a common need in applications to provide configuration values.
Next.js supports 2 ways of providing configuration:

Build-time configuration
Runtime configuration

Build-time configuration
The way build-time configuration works is by inlining the provided values into the Javascript bundle.
You can add the env key in next.config.js:
// next.config.js
module.exports = {
  env: {
    customKey: 'value',
  },
}
This will allow you to use process.env.customKey in your code. For example:
// pages/index.js
function Index() {
  return <h1>The value of customKey is: {process.env.customKey}</h1>
}

export default Index

Warning: Note that it is not possible to destructure process.env variables due to the webpack DefinePlugin replacing process.env.XXXX inline at build time.

// Will not work
const { CUSTOM_KEY, CUSTOM_SECRET } = process.env
AuthMethod({ key: CUSTOM_KEY, secret: CUSTOM_SECRET })

// Will work as replaced inline
AuthMethod({ key: process.env.CUSTOM_KEY, secret: process.env.CUSTOM_SECRET })
Runtime configuration

Warning: Note that these options are not available when using target: 'serverless'


Warning: Generally you want to use build-time configuration to provide your configuration.
The reason for this is that runtime configuration adds rendering / initialization overhead and is incompatible with automatic static optimization.

The next/config module gives your app access to the publicRuntimeConfig and serverRuntimeConfig stored in your next.config.js.
Place any server-only runtime config under a serverRuntimeConfig property.
Anything accessible to both client and server-side code should be under publicRuntimeConfig.

Note: A page that relies on publicRuntimeConfig must use getInitialProps to opt-out of automatic static optimization.
You can also de-optimize your entire application by creating a Custom <App> with getInitialProps.

// next.config.js
module.exports = {
  serverRuntimeConfig: {
    // Will only be available on the server side
    mySecret: 'secret',
    secondSecret: process.env.SECOND_SECRET, // Pass through env variables
  },
  publicRuntimeConfig: {
    // Will be available on both server and client
    staticFolder: '/static',
  },
}
// pages/index.js
import getConfig from 'next/config'
// Only holds serverRuntimeConfig and publicRuntimeConfig from next.config.js nothing else.
const { serverRuntimeConfig, publicRuntimeConfig } = getConfig()

console.log(serverRuntimeConfig.mySecret) // Will only be available on the server side
console.log(publicRuntimeConfig.staticFolder) // Will be available on both server and client

function MyImage() {
  return (
    <div>
      <img src={`${publicRuntimeConfig.staticFolder}/logo.png`} alt=""logo"" />
    </div>
  )
}

export default MyImage
Starting the server on alternative hostname
To start the development server using a different default hostname you can use --hostname hostname_here or -H hostname_here option with next dev. This will start a TCP server listening for connections on the provided host.
CDN support with Asset Prefix
To set up a CDN, you can set up the assetPrefix setting and configure your CDN's origin to resolve to the domain that Next.js is hosted on.
const isProd = process.env.NODE_ENV === 'production'
module.exports = {
  // You may only need to add assetPrefix in the production.
  assetPrefix: isProd ? 'https://cdn.mydomain.com' : '',
}
Note: Next.js will automatically use that prefix in the scripts it loads, but this has no effect whatsoever on /static. If you want to serve those assets over the CDN, you'll have to introduce the prefix yourself. One way of introducing a prefix that works inside your components and varies by environment is documented in this example.
If your CDN is on a separate domain and you would like assets to be requested using a CORS aware request you can set a config option for that.
// next.config.js
module.exports = {
  crossOrigin: 'anonymous',
}
Automatic Static Optimization
Next.js automatically determines that a page is static (can be prerendered) if it has no blocking data requirements.
This determination is made by the absence of getInitialProps in the page.
If getInitialProps is present, Next.js will not statically optimize the page.
Instead, Next.js will use its default behavior and render the page on-demand, per-request (meaning Server-Side Rendering).
If getInitialProps is absent, Next.js will statically optimize your page automatically by prerendering it to static HTML. During prerendering, the router's query object will be empty since we do not have query information to provide during this phase. Any query values will be populated client side after hydration.
This feature allows Next.js to emit hybrid applications that contain both server-rendered and statically generated pages.
This ensures Next.js always emits applications that are fast by default.

Note: Statically generated pages are still reactive: Next.js will hydrate your application client-side to give it full interactivity.

This feature provides many benefits.
For example, optimized pages require no server-side computation and can be instantly streamed to the end-user from CDN locations.
The result is an ultra fast loading experience for your users.
next build will emit .html files for statically optimized pages.
The result will be a file named .next/server/static/${BUILD_ID}/about.html instead of .next/server/static/${BUILD_ID}/about.js.
This behavior is similar for target: 'serverless'.
The built-in Next.js server (next start) and programmatic API (app.getRequestHandler()) both support this build output transparently.
There is no configuration or special handling required.

Note: If you have a custom <App> with getInitialProps then this optimization will be disabled.


Note: If you have a custom <Document> with getInitialProps be sure you check if ctx.req is defined before assuming the page is server-side rendered.
ctx.req will be undefined for pages that are prerendered.

Automatic Static Optimization Indicator
When a page qualifies for automatic static optimization we show an indicator to let you know.
This is helpful since the automatic static optimization can be very beneficial and knowing immediately in development if it qualifies can be useful.
See above for information on the benefits of this optimization.
In some cases this indicator might not be as useful like when working on electron applications. For these cases you can disable the indicator in your next.config.js by setting
module.exports = {
  devIndicators: {
    autoPrerender: false,
  },
}
Production deployment
To deploy, instead of running next, you want to build for production usage ahead of time. Therefore, building and starting are separate commands:
next build
next start
To deploy Next.js with ZEIT Now see the ZEIT Guide for Deploying Next.js or the Next.js Learn section about deploying on ZEIT Now.
Next.js can be deployed to other hosting solutions too. Please have a look at the 'Deployment' section of the wiki.
Note: NODE_ENV is properly configured by the next subcommands, if absent, to maximize performance. if you’re using Next.js programmatically, it’s your responsibility to set NODE_ENV=production manually!
Note: we recommend putting .next, or your custom dist folder, in .gitignore or .npmignore. Otherwise, use files or now.files to opt-into a whitelist of files you want to deploy, excluding .next or your custom dist folder.
Compression
Next.js provides gzip compression to compress rendered content and static files. Compression only works with the server target. In general you will want to enable compression on a HTTP proxy like nginx, to offload load from the Node.js process.
To disable compression in Next.js, set compress to false in next.config.js:
// next.config.js
module.exports = {
  compress: false,
}
Serverless deployment

Examples

now.sh
anna-artemov.now.sh
We encourage contributing more examples to this section


Serverless deployment dramatically improves reliability and scalability by splitting your application into smaller parts (also called lambdas).
In the case of Next.js, each page in the pages directory becomes a serverless lambda.
There are a number of benefits to serverless.
The referenced link talks about some of them in the context of Express, but the principles apply universally:
serverless allows for distributed points of failure, infinite scalability, and is incredibly affordable with a ""pay for what you use"" model.
To enable serverless mode in Next.js, add the serverless build target in next.config.js:
// next.config.js
module.exports = {
  target: 'serverless',
}
The serverless target will output a single lambda or HTML file per page.
This file is completely standalone and doesn't require any dependencies to run:

pages/index.js => .next/serverless/pages/index.js
pages/about.js => .next/serverless/pages/about.js
pages/blog.js => .next/serverless/pages/blog.html

The signature of the Next.js Serverless function is similar to the Node.js HTTP server callback:
export function render(req: http.IncomingMessage, res: http.ServerResponse) => void

http.IncomingMessage
http.ServerResponse
void refers to the function not having a return value and is equivalent to JavaScript's undefined. Calling the function will finish the request.

The static HTML files are ready to be served as-is.
You can read more about this feature, including how to opt-out, in the Automatic Static Optimization section.
Using the serverless target, you can deploy Next.js to ZEIT Now with all of the benefits and added ease of control like for example; custom routes and caching headers. See the ZEIT Guide for Deploying Next.js with Now for more information.
One Level Lower
Next.js provides low-level APIs for serverless deployments as hosting platforms have different function signatures. In general you will want to wrap the output of a Next.js serverless build with a compatibility layer.
For example if the platform supports the Node.js http.Server class:
const http = require('http')
const page = require('./.next/serverless/pages/about.js')
const server = new http.Server((req, res) => page.render(req, res))
server.listen(3000, () => console.log('Listening on http://localhost:3000'))
For specific platform examples see the examples section above.
Summary

Low-level API for implementing serverless deployment
Every page in the pages directory becomes a serverless function (lambda)
Creates the smallest possible serverless function (50Kb base zip size)
Optimized for fast cold start of the function
The serverless function has 0 dependencies (they are included in the function bundle)
Uses the http.IncomingMessage and http.ServerResponse from Node.js
opt-in using target: 'serverless' in next.config.js
Does not load next.config.js when executing the function, note that this means publicRuntimeConfig / serverRuntimeConfig are not supported

Browser support
Next.js supports IE11 and all modern browsers out of the box using @babel/preset-env. In order to support IE11 Next.js adds a global Promise polyfill. In cases where your own code or any external NPM dependencies you are using requires features not supported by your target browsers you will need to implement polyfills.
The polyfills example demonstrates the recommended approach to implement polyfills.
TypeScript
Next.js provides an integrated TypeScript experience out of the box, similar to an IDE.
To get started, create a empty tsconfig.json file in the root of your project:
touch tsconfig.json
Next.js will automatically configure this file with default values (providing your own tsconfig.json is also supported).
Then, run next dev (normally npm run dev) and Next.js will guide you through installing the necessary packages to complete setup.
npm run dev

# You'll see instructions like these:
#
# Please install typescript, @types/react, and @types/node by running:
#
#         yarn add --dev typescript @types/react @types/node
#
# ...
You're now ready to start converting files from .js to .tsx and leveraging the benefits TypeScript provides!
To learn more about TypeScript checkout its documentation.

Note: Next.js will create a file named next-env.d.ts in the root of your project.
This file ensures Next.js' types are picked up by the TypeScript compiler.
You cannot remove this file, however, you can edit it (but don't need to).


Note: Next.js does not enable TypeScript's strict mode by default.
When you feel comfortable with TypeScript, you may turn this option on in your tsconfig.json.


Note: By default, Next.js reports TypeScript errors during development for pages you are actively working on.
TypeScript errors for inactive pages do not block the development process.
If you don't want to leverage this behavior and instead, e.g. prefer your editor's integration, you can set the following option in next.config.js:
// next.config.js
module.exports = {
  typescript: {
    ignoreDevErrors: true,
  },
}
Next.js will still fail your production build (next build) when TypeScript errors are present in your project.
If you'd like Next.js to dangerously produce production code even when your application is broken, you can set the following option in your next.config.js.
Be sure you are running type checks as part of your build or deploy process!
// next.config.js
module.exports = {
  typescript: {
    // !! WARN !!
    // Dangerously allow production builds to successfully complete even if
    // your project has type errors.
    //
    // This option is rarely needed, and should be reserved for advanced
    // setups. You may be looking for `ignoreDevErrors` instead.
    // !! WARN !!
    ignoreBuildErrors: true,
  },
}

Exported types
Next.js provides NextPage type that can be used for pages in the pages directory. NextPage adds definitions for getInitialProps so that it can be used without any extra typing needed.
import { NextPage } from 'next'

interface Props {
  userAgent?: string
}

const Page: NextPage<Props> = ({ userAgent }) => (
  <main>Your user agent: {userAgent}</main>
)

Page.getInitialProps = async ({ req }) => {
  const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
  return { userAgent }
}

export default Page
For React.Component you can use NextPageContext:
import React from 'react'
import { NextPageContext } from 'next'

interface Props {
  userAgent?: string
}

export default class Page extends React.Component<Props> {
  static async getInitialProps({ req }: NextPageContext) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    const { userAgent } = this.props
    return <main>Your user agent: {userAgent}</main>
  }
}
AMP Support

Examples

amp


Enabling AMP Support
To enable AMP support for a page, add export const config = { amp: true } to your page.
AMP First Page
// pages/about.js
export const config = { amp: true }

export default function AboutPage(props) {
  return <h3>My AMP About Page!</h3>
}
Hybrid AMP Page
// pages/hybrid-about.js
import { useAmp } from 'next/amp'

export const config = { amp: 'hybrid' }

export default function AboutPage(props) {
  return (
    <div>
      <h3>My AMP Page</h3>
      {useAmp() ? (
        <amp-img
          width=""300""
          height=""300""
          src=""/my-img.jpg""
          alt=""a cool image""
          layout=""responsive""
        />
      ) : (
        <img width=""300"" height=""300"" src=""/my-img.jpg"" alt=""a cool image"" />
      )}
    </div>
  )
}
AMP Page Modes
AMP pages can specify two modes:

AMP-only (default)

Pages have no Next.js or React client-side runtime
Pages are automatically optimized with AMP Optimizer, an optimizer that applies the same transformations as AMP caches (improves performance by up to 42%)
Pages have a user-accessible (optimized) version of the page and a search-engine indexable (unoptimized) version of the page
Opt-in via export const config = { amp: true }


Hybrid

Pages are able to be rendered as traditional HTML (default) and AMP HTML (by adding ?amp=1 to the URL)
The AMP version of the page only has valid optimizations applied with AMP Optimizer so that it is indexable by search-engines
Opt-in via export const config = { amp: 'hybrid' }
Able to differentiate between modes using useAmp from next/amp



Both of these page modes provide a consistently fast experience for users accessing pages through search engines.
AMP Behavior with next export
When using next export to statically prerender pages Next.js will detect if the page supports AMP and change the exporting behavior based on that.
Hybrid AMP (pages/about.js) would output:

out/about.html - with client-side React runtime
out/about.amp.html - AMP page

AMP-only (pages/about.js) would output:

out/about.html - Optimized AMP page

During export Next.js automatically detects if a page is hybrid AMP and outputs the AMP version to page.amp.html. We also automatically insert the <link rel=""amphtml"" href=""/page.amp"" /> and <link rel=""canonical"" href=""/"" /> tags for you.

Note: When using exportTrailingSlash: true in next.config.js, output will be different. For Hybrid AMP pages, output will be out/page/index.html and out/page.amp/index.html, and for AMP-only pages, output will be out/page/index.html

Adding AMP Components
The AMP community provides many components to make AMP pages more interactive. You can add these components to your page by using next/head:
// pages/hello.js
import Head from 'next/head'

export const config = { amp: true }

export default function MyAmpPage() {
  return (
    <div>
      <Head>
        <script
          async
          key=""amp-timeago""
          custom-element=""amp-timeago""
          src=""https://cdn.ampproject.org/v0/amp-timeago-0.1.js""
        />
      </Head>

      <p>Some time: {date.toJSON()}</p>
      <amp-timeago
        width=""0""
        height=""15""
        datetime={date.toJSON()}
        layout=""responsive""
      >
        .
      </amp-timeago>
    </div>
  )
}
AMP Validation
AMP pages are automatically validated with amphtml-validator during development. Errors and warnings will appear in the terminal where you started Next.js.
Pages are also validated during next export and any warnings / errors will be printed to the terminal.
Any AMP errors will cause next export to exit with status code 1 because the export is not valid AMP.
TypeScript Support
AMP currently doesn't have built-in types for TypeScript, but it's in their roadmap (#13791). As a workaround you can manually add the types to amp.d.ts like here.
Static HTML export

Examples

Static export


next export is a way to run your Next.js app as a standalone static app without the need for a Node.js server.
The exported app supports almost every feature of Next.js, including dynamic urls, prefetching, preloading and dynamic imports.
The way next export works is by prerendering all pages possible to HTML. It does so based on a mapping of pathname key to page object. This mapping is called the exportPathMap.
The page object has 2 values:

page - String the page inside the pages directory to render
query - Object the query object passed to getInitialProps when prerendering. Defaults to {}

Usage
Simply develop your app as you normally do with Next.js. Then run:
next build
next export

By default next export doesn't require any configuration. It will generate a default exportPathMap containing the routes to pages inside the pages directory. This default mapping is available as defaultPathMap in the example below.
If your application has dynamic routes you can add a dynamic exportPathMap in next.config.js.
This function is asynchronous and gets the default exportPathMap as a parameter.
// next.config.js
module.exports = {
  exportPathMap: async function(
    defaultPathMap,
    { dev, dir, outDir, distDir, buildId }
  ) {
    return {
      '/': { page: '/' },
      '/about': { page: '/about' },
      '/readme.md': { page: '/readme' },
      '/p/hello-nextjs': { page: '/post', query: { title: 'hello-nextjs' } },
      '/p/learn-nextjs': { page: '/post', query: { title: 'learn-nextjs' } },
      '/p/deploy-nextjs': { page: '/post', query: { title: 'deploy-nextjs' } },
    }
  },
}
The pages will be exported as html files, i.e. /about will become /about.html.
It is possible to configure Next.js to export pages as index.html files and require trailing slashes, i.e. /about becomes /about/index.html and is routable via /about/.
This was the default behavior prior to Next.js 9.
You can use the following next.config.js to switch back to this behavior:
// next.config.js
module.exports = {
  exportTrailingSlash: true,
}

Note: If the export path is a filename (e.g. /readme.md) and is different than .html, you may need to set the Content-Type header to text/html when serving this content.

The second argument is an object with:

dev - true when exportPathMap is being called in development. false when running next export. In development exportPathMap is used to define routes.
dir - Absolute path to the project directory
outDir - Absolute path to the out/ directory (configurable with -o or --outdir). When dev is true the value of outDir will be null.
distDir - Absolute path to the .next/ directory (configurable using the distDir config key)
buildId - The buildId the export is running for

Then simply run these commands:
next build
next export
For that you may need to add a NPM script to package.json like this:
{
  ""scripts"": {
    ""build"": ""next build"",
    ""export"": ""npm run build && next export""
  }
}
And run it at once with:
npm run export
Then you have a static version of your app in the out directory.

You can also customize the output directory. For that run next export -h for the help.

Now you can deploy the out directory to any static hosting service. Note that there is an additional step for deploying to GitHub Pages, documented here.
For an example, simply visit the out directory and run following command to deploy your app to ZEIT Now.
now
Limitation
With next export, we build a HTML version of your app. At export time we will run getInitialProps of your pages.
The req and res fields of the context object passed to getInitialProps are empty objects during export as there is no server running.

Note: If your pages don't have getInitialProps you may not need next export at all, next build is already enough thanks to automatic static optimization.


You won't be able to render HTML dynamically when static exporting, as we pre-build the HTML files. If you want to do dynamic rendering use next start or the custom server API

Multi Zones

Examples

With Zones


A zone is a single deployment of a Next.js app. Just like that, you can have multiple zones and then you can merge them as a single app.
For an example, you can have two zones like this:

An app for serving /blog/**
Another app for serving all other pages

With multi zones support, you can merge both these apps into a single one allowing your customers to browse it using a single URL, but you can develop and deploy both apps independently.

This is exactly the same concept of microservices, but for frontend apps.

How to define a zone
There are no special zones related APIs. You only need to do following:

Make sure to keep only the pages you need in your app, meaning that an app can't have pages from another app, if app A has /blog then app B shouldn't have it too.
Make sure to add an assetPrefix to avoid conflicts with static files.

How to merge them
You can merge zones using any HTTP proxy.
You can use now dev as your local development server. It allows you to easily define routing routes for multiple apps like below:
{
  ""version"": 2,
  ""builds"": [
    { ""src"": ""docs/next.config.js"", ""use"": ""@now/next"" },
    { ""src"": ""home/next.config.js"", ""use"": ""@now/next"" }
  ],
  ""routes"": [
    { ""src"": ""/docs(.*)"", ""dest"": ""docs$1"", ""continue"": true },
    { ""src"": ""(?!/?docs)(.*)"", ""dest"": ""home$1"", ""continue"": true }
  ]
}
For the production deployment, you can use the same configuration and run now to do the deployment with ZEIT Now. Otherwise you can also configure a proxy server to route using a set of routes like the ones above, e.g deploy the docs app to https://docs.example.com and the home app to https://home.example.com and then add a proxy server for both apps in https://example.com.
FAQ

Is this production ready?
  Next.js has been powering https://zeit.co since its inception.
We’re ecstatic about both the developer experience and end-user performance, so we decided to share it with the community.


How big is it?
The client side bundle size should be measured in a per-app basis.
A small Next main bundle is around 65kb gzipped.


Is this like `create-react-app`?
Yes and No.
Yes in that both make your life easier.
No in that it enforces a structure so that we can do more advanced things like:

Server side rendering
Automatic code splitting

In addition, Next.js provides two built-in features that are critical for every single website:

Routing with lazy component loading: <Link> (by importing next/link)
A way for components to alter <head>: <Head> (by importing next/head)

If you want to create re-usable React components that you can embed in your Next.js app or other React applications, using create-react-app is a great idea. You can later import it and keep your codebase clean!


How do I use CSS-in-JS solutions?
Next.js bundles styled-jsx supporting scoped css. However you can use any CSS-in-JS solution in your Next app by just including your favorite library as mentioned before in the document.


What syntactic features are transpiled? How do I change them?
We track V8. Since V8 has wide support for ES6 and async and await, we transpile those. Since V8 doesn’t support class decorators, we don’t transpile those.
See the documentation about customizing the babel config and next/preset for more information.


Why a new Router?
Next.js is special in that:

Routes don’t need to be known ahead of time
Routes are always lazy-loadable
Top-level components can define getInitialProps that should block the loading of the route (either when server-rendering or lazy-loading)

As a result, we were able to introduce a very simple approach to routing that consists of two pieces:

Every top level component receives a url object to inspect the url or perform modifications to the history
A <Link /> component is used to wrap elements like anchors (<a/>) to perform client-side transitions



How do I define a custom fancy route?
Next.js provide dynamic routing solution out of the box. This allows to use pretty links in url.
You can check an example to see how it works.


How do I fetch data?
It’s up to you. getInitialProps is an async function (or a regular function that returns a Promise). It can retrieve data from anywhere.


Can I use it with GraphQL?
Yes! Here's an example with Apollo.


Can I use it with Redux and thunk?
Yes! Here's an example.


Can I use it with Redux?
Yes! Here's an example.


Can I use Next with my favorite Javascript library or toolkit?
Since our first release we've had many example contributions, you can check them out in the examples directory.


What is this inspired by?
Many of the goals we set out to accomplish were the ones listed in The 7 principles of Rich Web Applications by Guillermo Rauch.
The ease-of-use of PHP is a great inspiration. We feel Next.js is a suitable replacement for many scenarios where you otherwise would use PHP to output HTML.
Unlike PHP, we benefit from the ES6 module system and every file exports a component or function that can be easily imported for lazy evaluation or testing.
As we were researching options for server-rendering React that didn’t involve a large number of steps, we came across react-page (now deprecated), a similar approach to Next.js by the creator of React Jordan Walke.

Contributing
Please see our contributing.md.
Authors

Arunoda Susiripala (@arunoda) – ZEIT
Tim Neutkens (@timneutkens) – ZEIT
Naoyuki Kanezawa (@nkzawa) – ZEIT
Tony Kovanen (@tonykovanen) – ZEIT
Guillermo Rauch (@rauchg) – ZEIT
Dan Zajdband (@impronunciable) – Knight-Mozilla / Coral Project

"
28,JavaScript,"
   



A lightweight JavaScript date library for parsing, validating, manipulating, and formatting dates.
Documentation
Port to ECMAScript 6 (version 2.10.0)
Moment 2.10.0 does not bring any new features, but the code is now written in
ECMAScript 6 modules and placed inside src/. Previously moment.js, locale/*.js and
test/moment/*.js, test/locale/*.js contained the source of the project. Now
the source is in src/, temporary build (ECMAScript 5) files are placed under
build/umd/ (for running tests during development), and the moment.js and
locale/*.js files are updated only on release.
If you want to use a particular revision of the code, make sure to run
grunt transpile update-index, so moment.js and locales/*.js are synced
with src/*. We might place that in a commit hook in the future.
Upgrading to 2.0.0
There are a number of small backwards incompatible changes with version 2.0.0. See the full descriptions here


Changed language ordinal method to return the number + ordinal instead of just the ordinal.


Changed two digit year parsing cutoff to match strptime.


Removed moment#sod and moment#eod in favor of moment#startOf and moment#endOf.


Removed moment.humanizeDuration() in favor of moment.duration().humanize().


Removed the lang data objects from the top level namespace.


Duplicate Date passed to moment() instead of referencing it.


Changelog
Contributing 
We're looking for co-maintainers! If you want to become a master of time please
write to ichernev.
In addition to contributing code, you can help to triage issues. This can include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to moment/moment on CodeTriage.
License
Moment.js is freely distributable under the terms of the MIT license.

"
29,JavaScript,"GitHub Résumé
A service that creates a résumé based on your GitHub repos/activity.
GitHub Résumé is opt-in. To make your resume visible, just star this project. To view your résumé, go to https://resume.github.io/?yourusername or follow the instructions on the home page.
Great for all the tech-savy bosses who want to have a quick view of person's git/github activity, before the interview.
Development
To run the app in development mode:
$ rackup config.ru

(You must have Ruby and the rack gem installed.)
"
30,Python,"English ∙ 日本語 ∙ 简体中文 ∙ 繁體中文 | العَرَبِيَّة‎ ∙ বাংলা ∙ Português do Brasil ∙ Deutsch ∙ ελληνικά ∙ עברית ∙ Italiano ∙ 韓國語 ∙ فارسی ∙ Polski ∙ русский язык ∙ Español ∙ ภาษาไทย ∙ Türkçe ∙ tiếng Việt ∙ Français | Add Translation
The System Design Primer




Motivation

Learn how to design large-scale systems.
Prep for the system design interview.

Learn how to design large-scale systems
Learning how to design scalable systems will help you become a better engineer.
System design is a broad topic.  There is a vast amount of resources scattered throughout the web on system design principles.
This repo is an organized collection of resources to help you learn how to build systems at scale.
Learn from the open source community
This is a continually updated, open source project.
Contributions are welcome!
Prep for the system design interview
In addition to coding interviews, system design is a required component of the technical interview process at many tech companies.
Practice common system design interview questions and compare your results with sample solutions: discussions, code, and diagrams.
Additional topics for interview prep:

Study guide
How to approach a system design interview question
System design interview questions, with solutions
Object-oriented design interview questions, with solutions
Additional system design interview questions

Anki flashcards




The provided Anki flashcard decks use spaced repetition to help you retain key system design concepts.

System design deck
System design exercises deck
Object oriented design exercises deck

Great for use while on-the-go.
Coding Resource: Interactive Coding Challenges
Looking for resources to help you prep for the Coding Interview?




Check out the sister repo Interactive Coding Challenges, which contains an additional Anki deck:

Coding deck

Contributing

Learn from the community.

Feel free to submit pull requests to help:

Fix errors
Improve sections
Add new sections
Translate

Content that needs some polishing is placed under development.
Review the Contributing Guidelines.
Index of system design topics

Summaries of various system design topics, including pros and cons.  Everything is a trade-off.
Each section contains links to more in-depth resources.






System design topics: start here

Step 1: Review the scalability video lecture
Step 2: Review the scalability article
Next steps


Performance vs scalability
Latency vs throughput
Availability vs consistency

CAP theorem

CP - consistency and partition tolerance
AP - availability and partition tolerance




Consistency patterns

Weak consistency
Eventual consistency
Strong consistency


Availability patterns

Fail-over
Replication
Availability in numbers


Domain name system
Content delivery network

Push CDNs
Pull CDNs


Load balancer

Active-passive
Active-active
Layer 4 load balancing
Layer 7 load balancing
Horizontal scaling


Reverse proxy (web server)

Load balancer vs reverse proxy


Application layer

Microservices
Service discovery


Database

Relational database management system (RDBMS)

Master-slave replication
Master-master replication
Federation
Sharding
Denormalization
SQL tuning


NoSQL

Key-value store
Document store
Wide column store
Graph Database


SQL or NoSQL


Cache

Client caching
CDN caching
Web server caching
Database caching
Application caching
Caching at the database query level
Caching at the object level
When to update the cache

Cache-aside
Write-through
Write-behind (write-back)
Refresh-ahead




Asynchronism

Message queues
Task queues
Back pressure


Communication

Transmission control protocol (TCP)
User datagram protocol (UDP)
Remote procedure call (RPC)
Representational state transfer (REST)


Security
Appendix

Powers of two table
Latency numbers every programmer should know
Additional system design interview questions
Real world architectures
Company architectures
Company engineering blogs


Under development
Credits
Contact info
License

Study guide

Suggested topics to review based on your interview timeline (short, medium, long).


Q: For interviews, do I need to know everything here?
A: No, you don't need to know everything here to prepare for the interview.
What you are asked in an interview depends on variables such as:

How much experience you have
What your technical background is
What positions you are interviewing for
Which companies you are interviewing with
Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.
Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

Short timeline - Aim for breadth with system design topics.  Practice by solving some interview questions.
Medium timeline - Aim for breadth and some depth with system design topics.  Practice by solving many interview questions.
Long timeline - Aim for breadth and more depth with system design topics.  Practice by solving most interview questions.





Short
Medium
Long




Read through the System design topics to get a broad understanding of how systems work
👍
👍
👍


Read through a few articles in the Company engineering blogs for the companies you are interviewing with
👍
👍
👍


Read through a few Real world architectures
👍
👍
👍


Review How to approach a system design interview question
👍
👍
👍


Work through System design interview questions with solutions
Some
Many
Most


Work through Object-oriented design interview questions with solutions
Some
Many
Most


Review Additional system design interview questions
Some
Many
Most



How to approach a system design interview question

How to tackle a system design interview question.

The system design interview is an open-ended conversation.  You are expected to lead it.
You can use the following steps to guide the discussion.  To help solidify this process, work through the System design interview questions with solutions section using the following steps.
Step 1: Outline use cases, constraints, and assumptions
Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

Who is going to use it?
How are they going to use it?
How many users are there?
What does the system do?
What are the inputs and outputs of the system?
How much data do we expect to handle?
How many requests per second do we expect?
What is the expected read to write ratio?

Step 2: Create a high level design
Outline a high level design with all important components.

Sketch the main components and connections
Justify your ideas

Step 3: Design core components
Dive into details for each core component.  For example, if you were asked to design a url shortening service, discuss:

Generating and storing a hash of the full url

MD5 and Base62
Hash collisions
SQL or NoSQL
Database schema


Translating a hashed url to the full url

Database lookup


API and object-oriented design

Step 4: Scale the design
Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

Load balancer
Horizontal scaling
Caching
Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using principles of scalable system design.
Back-of-the-envelope calculations
You might be asked to do some estimates by hand.  Refer to the Appendix for the following resources:

Use back of the envelope calculations
Powers of two table
Latency numbers every programmer should know

Source(s) and further reading
Check out the following links to get a better idea of what to expect:

How to ace a systems design interview
The system design interview
Intro to Architecture and Systems Design Interviews

System design interview questions with solutions

Common system design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.




Question





Design Pastebin.com (or Bit.ly)
Solution


Design the Twitter timeline and search (or Facebook feed and search)
Solution


Design a web crawler
Solution


Design Mint.com
Solution


Design the data structures for a social network
Solution


Design a key-value store for a search engine
Solution


Design Amazon's sales ranking by category feature
Solution


Design a system that scales to millions of users on AWS
Solution


Add a system design question
Contribute



Design Pastebin.com (or Bit.ly)
View exercise and solution

Design the Twitter timeline and search (or Facebook feed and search)
View exercise and solution

Design a web crawler
View exercise and solution

Design Mint.com
View exercise and solution

Design the data structures for a social network
View exercise and solution

Design a key-value store for a search engine
View exercise and solution

Design Amazon's sales ranking by category feature
View exercise and solution

Design a system that scales to millions of users on AWS
View exercise and solution

Object-oriented design interview questions with solutions

Common object-oriented design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.


Note: This section is under development




Question





Design a hash map
Solution


Design a least recently used cache
Solution


Design a call center
Solution


Design a deck of cards
Solution


Design a parking lot
Solution


Design a chat server
Solution


Design a circular array
Contribute


Add an object-oriented design question
Contribute



System design topics: start here
New to system design?
First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.
Step 1: Review the scalability video lecture
Scalability Lecture at Harvard

Topics covered:

Vertical scaling
Horizontal scaling
Caching
Load balancing
Database replication
Database partitioning



Step 2: Review the scalability article
Scalability

Topics covered:

Clones
Databases
Caches
Asynchronism



Next steps
Next, we'll look at high-level trade-offs:

Performance vs scalability
Latency vs throughput
Availability vs consistency

Keep in mind that everything is a trade-off.
Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.
Performance vs scalability
A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.1
Another way to look at performance vs scalability:

If you have a performance problem, your system is slow for a single user.
If you have a scalability problem, your system is fast for a single user but slow under heavy load.

Source(s) and further reading

A word on scalability
Scalability, availability, stability, patterns

Latency vs throughput
Latency is the time to perform some action or to produce some result.
Throughput is the number of such actions or results per unit of time.
Generally, you should aim for maximal throughput with acceptable latency.
Source(s) and further reading

Understanding latency vs throughput

Availability vs consistency
CAP theorem



Source: CAP theorem revisited

In a distributed computer system, you can only support two of the following guarantees:

Consistency - Every read receives the most recent write or an error
Availability - Every request receives a response, without guarantee that it contains the most recent version of the information
Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures

Networks aren't reliable, so you'll need to support partition tolerance.  You'll need to make a software tradeoff between consistency and availability.
CP - consistency and partition tolerance
Waiting for a response from the partitioned node might result in a timeout error.  CP is a good choice if your business needs require atomic reads and writes.
AP - availability and partition tolerance
Responses return the most recent version of the data available on a node, which might not be the latest.  Writes might take some time to propagate when the partition is resolved.
AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.
Source(s) and further reading

CAP theorem revisited
A plain english introduction to CAP theorem
CAP FAQ

Consistency patterns
With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data.  Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error.
Weak consistency
After a write, reads may or may not see it.  A best effort approach is taken.
This approach is seen in systems such as memcached.  Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games.  For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.
Eventual consistency
After a write, reads will eventually see it (typically within milliseconds).  Data is replicated asynchronously.
This approach is seen in systems such as DNS and email.  Eventual consistency works well in highly available systems.
Strong consistency
After a write, reads will see it.  Data is replicated synchronously.
This approach is seen in file systems and RDBMSes.  Strong consistency works well in systems that need transactions.
Source(s) and further reading

Transactions across data centers

Availability patterns
There are two main patterns to support high availability: fail-over and replication.
Fail-over
Active-passive
With active-passive fail-over, heartbeats are sent between the active and the passive server on standby.  If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.
The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby.  Only the active server handles traffic.
Active-passive failover can also be referred to as master-slave failover.
Active-active
In active-active, both servers are managing traffic, spreading the load between them.
If the servers are public-facing, the DNS would need to know about the public IPs of both servers.  If the servers are internal-facing, application logic would need to know about both servers.
Active-active failover can also be referred to as master-master failover.
Disadvantage(s): failover

Fail-over adds more hardware and additional complexity.
There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.

Replication
Master-slave and master-master
This topic is further discussed in the Database section:

Master-slave replication
Master-master replication

Availability in numbers
Availability is often quantified by uptime (or downtime) as a percentage of time the service is available.  Availability is generally measured in number of 9s--a service with 99.99% availability is described as having four 9s.
99.9% availability - three 9s



Duration
Acceptable downtime




Downtime per year
8h 45min 57s


Downtime per month
43m 49.7s


Downtime per week
10m 4.8s


Downtime per day
1m 26.4s



99.99% availability - four 9s



Duration
Acceptable downtime




Downtime per year
52min 35.7s


Downtime per month
4m 23s


Downtime per week
1m 5s


Downtime per day
8.6s



Availability in parallel vs in sequence
If a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.
In sequence
Overall availability decreases when two components with availability < 100% are in sequence:
Availability (Total) = Availability (Foo) * Availability (Bar)

If both Foo and Bar each had 99.9% availability, their total availability in sequence would be 99.8%.
In parallel
Overall availability increases when two components with availability < 100% are in parallel:
Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))

If both Foo and Bar each had 99.9% availability, their total availability in parallel would be 99.9999%.
Domain name system



Source: DNS security presentation

A Domain Name System (DNS) translates a domain name such as www.example.com to an IP address.
DNS is hierarchical, with a few authoritative servers at the top level.  Your router or ISP provides information about which DNS server(s) to contact when doing a lookup.  Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays.  DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL).

NS record (name server) - Specifies the DNS servers for your domain/subdomain.
MX record (mail exchange) - Specifies the mail servers for accepting messages.
A record (address) - Points a name to an IP address.
CNAME (canonical) - Points a name to another name or CNAME (example.com to www.example.com) or to an A record.

Services such as CloudFlare and Route 53 provide managed DNS services.  Some DNS services can route traffic through various methods:

Weighted round robin

Prevent traffic from going to servers under maintenance
Balance between varying cluster sizes
A/B testing


Latency-based
Geolocation-based

Disadvantage(s): DNS

Accessing a DNS server introduces a slight delay, although mitigated by caching described above.
DNS server management could be complex and is generally managed by governments, ISPs, and large companies.
DNS services have recently come under DDoS attack, preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).

Source(s) and further reading

DNS architecture
Wikipedia
DNS articles

Content delivery network



Source: Why use a CDN

A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.  Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content.  The site's DNS resolution will tell clients which server to contact.
Serving content from CDNs can significantly improve performance in two ways:

Users receive content at data centers close to them
Your servers do not have to serve requests that the CDN fulfills

Push CDNs
Push CDNs receive new content whenever changes occur on your server.  You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN.  You can configure when content expires and when it is updated.  Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.
Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs.  Content is placed on the CDNs once, instead of being re-pulled at regular intervals.
Pull CDNs
Pull CDNs grab new content from your server when the first user requests the content.  You leave the content on your server and rewrite URLs to point to the CDN.  This results in a slower request until the content is cached on the CDN.
A time-to-live (TTL) determines how long content is cached.  Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.
Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.
Disadvantage(s): CDN

CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.
Content might be stale if it is updated before the TTL expires it.
CDNs require changing URLs for static content to point to the CDN.

Source(s) and further reading

Globally distributed content delivery
The differences between push and pull CDNs
Wikipedia

Load balancer



Source: Scalable system design patterns

Load balancers distribute incoming client requests to computing resources such as application servers and databases.  In each case, the load balancer returns the response from the computing resource to the appropriate client.  Load balancers are effective at:

Preventing requests from going to unhealthy servers
Preventing overloading resources
Helping eliminate single points of failure

Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.
Additional benefits include:

SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Session persistence - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions

To protect against failures, it's common to set up multiple load balancers, either in active-passive or active-active mode.
Load balancers can route traffic based on various metrics, including:

Random
Least loaded
Session/cookies
Round robin or weighted round robin
Layer 4
Layer 7

Layer 4 load balancing
Layer 4 load balancers look at info at the transport layer to decide how to distribute requests.  Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet.  Layer 4 load balancers forward network packets to and from the upstream server, performing Network Address Translation (NAT).
Layer 7 load balancing
Layer 7 load balancers look at the application layer to decide how to distribute requests.  This can involve contents of the header, message, and cookies.  Layer 7 load balancers terminates network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server.  For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.
At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.
Horizontal scaling
Load balancers can also help with horizontal scaling, improving performance and availability.  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling.  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.
Disadvantage(s): horizontal scaling

Scaling horizontally introduces complexity and involves cloning servers

Servers should be stateless: they should not contain any user-related data like sessions or profile pictures
Sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached)


Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out

Disadvantage(s): load balancer

The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.
Introducing a load balancer to help eliminate single points of failure results in increased complexity.
A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.

Source(s) and further reading

NGINX architecture
HAProxy architecture guide
Scalability
Wikipedia
Layer 4 load balancing
Layer 7 load balancing
ELB listener config

Reverse proxy (web server)



Source: Wikipedia


A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public.  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.
Additional benefits include:

Increased security - Hide information about backend servers, blacklist IPs, limit number of connections per client
Increased scalability and flexibility - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration
SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Compression - Compress server responses
Caching - Return the response for cached requests
Static content - Serve static content directly

HTML/CSS/JS
Photos
Videos
Etc



Load balancer vs reverse proxy

Deploying a load balancer is useful when you have multiple servers.  Often, load balancers  route traffic to a set of servers serving the same function.
Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.
Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.

Disadvantage(s): reverse proxy

Introducing a reverse proxy results in increased complexity.
A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a failover) further increases complexity.

Source(s) and further reading

Reverse proxy vs load balancer
NGINX architecture
HAProxy architecture guide
Wikipedia

Application layer



Source: Intro to architecting systems for scale

Separating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently.  Adding a new API results in adding application servers without necessarily adding additional web servers.  The single responsibility principle advocates for small and autonomous services that work together.  Small teams with small services can plan more aggressively for rapid growth.
Workers in the application layer also help enable asynchronism.
Microservices
Related to this discussion are microservices, which can be described as a suite of independently deployable, small, modular services.  Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. 1
Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.
Service Discovery
Systems such as Consul, Etcd, and Zookeeper can help services find each other by keeping track of registered names, addresses, and ports.  Health checks help verify service integrity and are often done using an HTTP endpoint.  Both Consul and Etcd have a built in key-value store that can be useful for storing config values and other shared data.
Disadvantage(s): application layer

Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).
Microservices can add complexity in terms of deployments and operations.

Source(s) and further reading

Intro to architecting systems for scale
Crack the system design interview
Service oriented architecture
Introduction to Zookeeper
Here's what you need to know about building microservices

Database



Source: Scaling up to your first 10 million users

Relational database management system (RDBMS)
A relational database like SQL is a collection of data items organized in tables.
ACID is a set of properties of relational database transactions.

Atomicity - Each transaction is all or nothing
Consistency - Any transaction will bring the database from one valid state to another
Isolation - Executing transactions concurrently has the same results as if the transactions were executed serially
Durability - Once a transaction has been committed, it will remain so

There are many techniques to scale a relational database: master-slave replication, master-master replication, federation, sharding, denormalization, and SQL tuning.
Master-slave replication
The master serves reads and writes, replicating writes to one or more slaves, which serve only reads.  Slaves can also replicate to additional slaves in a tree-like fashion.  If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-slave replication

Additional logic is needed to promote a slave to a master.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Master-master replication
Both masters serve reads and writes and coordinate with each other on writes.  If either master goes down, the system can continue to operate with both reads and writes.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-master replication

You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.
Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.
Conflict resolution comes more into play as more write nodes are added and as latency increases.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Disadvantage(s): replication

There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.
Writes are replayed to the read replicas.  If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.
The more read slaves, the more you have to replicate, which leads to greater replication lag.
On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.
Replication adds more hardware and additional complexity.

Source(s) and further reading: replication

Scalability, availability, stability, patterns
Multi-master replication

Federation



Source: Scaling up to your first 10 million users

Federation (or functional partitioning) splits up databases by function.  For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.  Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality.  With no single central master serializing writes you can write in parallel, increasing throughput.
Disadvantage(s): federation

Federation is not effective if your schema requires huge functions or tables.
You'll need to update your application logic to determine which database to read and write.
Joining data from two databases is more complex with a server link.
Federation adds more hardware and additional complexity.

Source(s) and further reading: federation

Scaling up to your first 10 million users

Sharding



Source: Scalability, availability, stability, patterns

Sharding distributes data across different databases such that each database can only manage a subset of the data.  Taking a users database as an example, as the number of users increases, more shards are added to the cluster.
Similar to the advantages of federation, sharding results in less read and write traffic, less replication, and more cache hits.  Index size is also reduced, which generally improves performance with faster queries.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.  Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.
Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.
Disadvantage(s): sharding

You'll need to update your application logic to work with shards, which could result in complex SQL queries.
Data distribution can become lopsided in a shard.  For example, a set of power users on a shard could result in increased load to that shard compared to others.

Rebalancing adds additional complexity.  A sharding function based on consistent hashing can reduce the amount of transferred data.


Joining data from multiple shards is more complex.
Sharding adds more hardware and additional complexity.

Source(s) and further reading: sharding

The coming of the shard
Shard database architecture
Consistent hashing

Denormalization
Denormalization attempts to improve read performance at the expense of some write performance.  Redundant copies of the data are written in multiple tables to avoid expensive joins.  Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent.
Once data becomes distributed with techniques such as federation and sharding, managing joins across data centers further increases complexity.  Denormalization might circumvent the need for such complex joins.
In most systems, reads can heavily outnumber writes 100:1 or even 1000:1.  A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.
Disadvantage(s): denormalization

Data is duplicated.
Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.
A denormalized database under heavy write load might perform worse than its normalized counterpart.

Source(s) and further reading: denormalization

Denormalization

SQL tuning
SQL tuning is a broad topic and many books have been written as reference.
It's important to benchmark and profile to simulate and uncover bottlenecks.

Benchmark - Simulate high-load situations with tools such as ab.
Profile - Enable tools such as the slow query log to help track performance issues.

Benchmarking and profiling might point you to the following optimizations.
Tighten up the schema

MySQL dumps to disk in contiguous blocks for fast access.
Use CHAR instead of VARCHAR for fixed-length fields.

CHAR effectively allows for fast, random access, whereas with VARCHAR, you must find the end of a string before moving onto the next one.


Use TEXT for large blocks of text such as blog posts.  TEXT also allows for boolean searches.  Using a TEXT field results in storing a pointer on disk that is used to locate the text block.
Use INT for larger numbers up to 2^32 or 4 billion.
Use DECIMAL for currency to avoid floating point representation errors.
Avoid storing large BLOBS, store the location of where to get the object instead.
VARCHAR(255) is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.
Set the NOT NULL constraint where applicable to improve search performance.

Use good indices

Columns that you are querying (SELECT, GROUP BY, ORDER BY, JOIN) could be faster with indices.
Indices are usually represented as self-balancing B-tree that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.
Placing an index can keep the data in memory, requiring more space.
Writes could also be slower since the index also needs to be updated.
When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.

Avoid expensive joins

Denormalize where performance demands it.

Partition tables

Break up a table by putting hot spots in a separate table to help keep it in memory.

Tune the query cache

In some cases, the query cache could lead to performance issues.

Source(s) and further reading: SQL tuning

Tips for optimizing MySQL queries
Is there a good reason i see VARCHAR(255) used so often?
How do null values affect performance?
Slow query log

NoSQL
NoSQL is a collection of data items represented in a key-value store, document store, wide column store, or a graph database.  Data is denormalized, and joins are generally done in the application code.  Most NoSQL stores lack true ACID transactions and favor eventual consistency.
BASE is often used to describe the properties of NoSQL databases.  In comparison with the CAP Theorem, BASE chooses availability over consistency.

Basically available - the system guarantees availability.
Soft state - the state of the system may change over time, even without input.
Eventual consistency - the system will become consistent over a period of time, given that the system doesn't receive input during that period.

In addition to choosing between SQL or NoSQL, it is helpful to understand which type of NoSQL database best fits your use case(s).  We'll review key-value stores, document stores, wide column stores, and graph databases in the next section.
Key-value store

Abstraction: hash table

A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD.  Data stores can maintain keys in lexicographic order, allowing efficient retrieval of key ranges.  Key-value stores can allow for storing of metadata with a value.
Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer.  Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.
A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.
Source(s) and further reading: key-value store

Key-value database
Disadvantages of key-value stores
Redis architecture
Memcached architecture

Document store

Abstraction: key-value store with documents stored as values

A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.  Document stores provide APIs or a query language to query based on the internal structure of the document itself.  Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.
Based on the underlying implementation, documents are organized by collections, tags, metadata, or directories.  Although documents can be organized or grouped together, documents may have fields that are completely different from each other.
Some document stores like MongoDB and CouchDB also provide a SQL-like language to perform complex queries.  DynamoDB supports both key-values and documents.
Document stores provide high flexibility and are often used for working with occasionally changing data.
Source(s) and further reading: document store

Document-oriented database
MongoDB architecture
CouchDB architecture
Elasticsearch architecture

Wide column store



Source: SQL & NoSQL, a brief history


Abstraction: nested map ColumnFamily<RowKey, Columns<ColKey, Value, Timestamp>>

A wide column store's basic unit of data is a column (name/value pair).  A column can be grouped in column families (analogous to a SQL table).  Super column families further group column families.  You can access each column independently with a row key, and columns with the same row key form a row.  Each value contains a timestamp for versioning and for conflict resolution.
Google introduced Bigtable as the first wide column store, which influenced the open-source HBase often-used in the Hadoop ecosystem, and Cassandra from Facebook.  Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.
Wide column stores offer high availability and high scalability.  They are often used for very large data sets.
Source(s) and further reading: wide column store

SQL & NoSQL, a brief history
Bigtable architecture
HBase architecture
Cassandra architecture

Graph database



Source: Graph database


Abstraction: graph

In a graph database, each node is a record and each arc is a relationship between two nodes.  Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.
Graphs databases offer high performance for data models with complex relationships, such as a social network.  They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.  Many graphs can only be accessed with REST APIs.
Source(s) and further reading: graph

Graph database
Neo4j
FlockDB

Source(s) and further reading: NoSQL

Explanation of base terminology
NoSQL databases a survey and decision guidance
Scalability
Introduction to NoSQL
NoSQL patterns

SQL or NoSQL



Source: Transitioning from RDBMS to NoSQL

Reasons for SQL:

Structured data
Strict schema
Relational data
Need for complex joins
Transactions
Clear patterns for scaling
More established: developers, community, code, tools, etc
Lookups by index are very fast

Reasons for NoSQL:

Semi-structured data
Dynamic or flexible schema
Non-relational data
No need for complex joins
Store many TB (or PB) of data
Very data intensive workload
Very high throughput for IOPS

Sample data well-suited for NoSQL:

Rapid ingest of clickstream and log data
Leaderboard or scoring data
Temporary data, such as a shopping cart
Frequently accessed ('hot') tables
Metadata/lookup tables

Source(s) and further reading: SQL or NoSQL

Scaling up to your first 10 million users
SQL vs NoSQL differences

Cache



Source: Scalable system design patterns

Caching improves page load times and can reduce the load on your servers and databases.  In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.
Databases often benefit from a uniform distribution of reads and writes across its partitions.  Popular items can skew the distribution, causing bottlenecks.  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.
Client caching
Caches can be located on the client side (OS or browser), server side, or in a distinct cache layer.
CDN caching
CDNs are considered a type of cache.
Web server caching
Reverse proxies and caches such as Varnish can serve static and dynamic content directly.  Web servers can also cache requests, returning responses without having to contact application servers.
Database caching
Your database usually includes some level of caching in a default configuration, optimized for a generic use case.  Tweaking these settings for specific usage patterns can further boost performance.
Application caching
In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage.  Since the data is held in RAM, it is much faster than typical databases where data is stored on disk.  RAM is more limited than disk, so cache invalidation algorithms such as least recently used (LRU) can help invalidate 'cold' entries and keep 'hot' data in RAM.
Redis has the following additional features:

Persistence option
Built-in data structures such as sorted sets and lists

There are multiple levels you can cache that fall into two general categories: database queries and objects:

Row level
Query-level
Fully-formed serializable objects
Fully-rendered HTML

Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.
Caching at the database query level
Whenever you query the database, hash the query as a key and store the result to the cache.  This approach suffers from expiration issues:

Hard to delete a cached result with complex queries
If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell

Caching at the object level
See your data as an object, similar to what you do with your application code.  Have your application assemble the dataset from the database into a class instance or a data structure(s):

Remove the object from cache if its underlying data has changed
Allows for asynchronous processing: workers assemble objects by consuming the latest cached object

Suggestions of what to cache:

User sessions
Fully rendered web pages
Activity streams
User graph data

When to update the cache
Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.
Cache-aside



Source: From cache to in-memory data grid

The application is responsible for reading and writing from storage.  The cache does not interact with storage directly.  The application does the following:

Look for entry in cache, resulting in a cache miss
Load entry from the database
Add entry to cache
Return entry

def get_user(self, user_id):
    user = cache.get(""user.{0}"", user_id)
    if user is None:
        user = db.query(""SELECT * FROM users WHERE user_id = {0}"", user_id)
        if user is not None:
            key = ""user.{0}"".format(user_id)
            cache.set(key, json.dumps(user))
    return user
Memcached is generally used in this manner.
Subsequent reads of data added to cache are fast.  Cache-aside is also referred to as lazy loading.  Only requested data is cached, which avoids filling up the cache with data that isn't requested.
Disadvantage(s): cache-aside

Each cache miss results in three trips, which can cause a noticeable delay.
Data can become stale if it is updated in the database.  This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.
When a node fails, it is replaced by a new, empty node, increasing latency.

Write-through



Source: Scalability, availability, stability, patterns

The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:

Application adds/updates entry in cache
Cache synchronously writes entry to data store
Return

Application code:
set_user(12345, {""foo"":""bar""})
Cache code:
def set_user(user_id, values):
    user = db.query(""UPDATE Users WHERE id = {0}"", user_id, values)
    cache.set(user_id, user)
Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast.  Users are generally more tolerant of latency when updating data than reading data.  Data in the cache is not stale.
Disadvantage(s): write through

When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database.  Cache-aside in conjunction with write through can mitigate this issue.
Most data written might never be read, which can be minimized with a TTL.

Write-behind (write-back)



Source: Scalability, availability, stability, patterns

In write-behind, the application does the following:

Add/update entry in cache
Asynchronously write entry to the data store, improving write performance

Disadvantage(s): write-behind

There could be data loss if the cache goes down prior to its contents hitting the data store.
It is more complex to implement write-behind than it is to implement cache-aside or write-through.

Refresh-ahead



Source: From cache to in-memory data grid

You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.
Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.
Disadvantage(s): refresh-ahead

Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.

Disadvantage(s): cache

Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.
Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.
Need to make application changes such as adding Redis or memcached.

Source(s) and further reading

From cache to in-memory data grid
Scalable system design patterns
Introduction to architecting systems for scale
Scalability, availability, stability, patterns
Scalability
AWS ElastiCache strategies
Wikipedia

Asynchronism



Source: Intro to architecting systems for scale

Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line.  They can also help by doing time-consuming work in advance, such as periodic aggregation of data.
Message queues
Message queues receive, hold, and deliver messages.  If an operation is too slow to perform inline, you can use a message queue with the following workflow:

An application publishes a job to the queue, then notifies the user of job status
A worker picks up the job from the queue, processes it, then signals the job is complete

The user is not blocked and the job is processed in the background.  During this time, the client might optionally do a small amount of processing to make it seem like the task has completed.  For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.
Redis is useful as a simple message broker but messages can be lost.
RabbitMQ is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.
Amazon SQS is hosted but can have high latency and has the possibility of messages being delivered twice.
Task queues
Tasks queues receive tasks and their related data, runs them, then delivers their results.  They can support scheduling and can be used to run computationally-intensive jobs in the background.
Celery has support for scheduling and primarily has python support.
Back pressure
If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.  Back pressure can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue.  Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later.  Clients can retry the request at a later time, perhaps with exponential backoff.
Disadvantage(s): asynchronism

Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.

Source(s) and further reading

It's all a numbers game
Applying back pressure when overloaded
Little's law
What is the difference between a message queue and a task queue?

Communication



Source: OSI 7 layer model

Hypertext transfer protocol (HTTP)
HTTP is a method for encoding and transporting data between a client and a server.  It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.  HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.
A basic HTTP request consists of a verb (method) and a resource (endpoint).  Below are common HTTP verbs:



Verb
Description
Idempotent*
Safe
Cacheable




GET
Reads a resource
Yes
Yes
Yes


POST
Creates a resource or trigger a process that handles data
No
No
Yes if response contains freshness info


PUT
Creates or replace a resource
Yes
No
No


PATCH
Partially updates a resource
No
No
Yes if response contains freshness info


DELETE
Deletes a resource
Yes
No
No



*Can be called many times without different outcomes.
HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP.
Source(s) and further reading: HTTP

What is HTTP?
Difference between HTTP and TCP
Difference between PUT and PATCH

Transmission control protocol (TCP)



Source: How to make a multiplayer game

TCP is a connection-oriented protocol over an IP network.  Connection is established and terminated using a handshake.  All packets sent are guaranteed to reach the destination in the original order and without corruption through:

Sequence numbers and checksum fields for each packet
Acknowledgement packets and automatic retransmission

If the sender does not receive a correct response, it will resend the packets.  If there are multiple timeouts, the connection is dropped.  TCP also implements flow control and congestion control.  These guarantees cause delays and generally result in less efficient transmission than UDP.
To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage.  It can be expensive to have a large number of open connections between web server threads and say, a memcached server.  Connection pooling can help in addition to switching to UDP where applicable.
TCP is useful for applications that require high reliability but are less time critical.  Some examples include web servers, database info, SMTP, FTP, and SSH.
Use TCP over UDP when:

You need all of the data to arrive intact
You want to automatically make a best estimate use of the network throughput

User datagram protocol (UDP)



Source: How to make a multiplayer game

UDP is connectionless.  Datagrams (analogous to packets) are guaranteed only at the datagram level.  Datagrams might reach their destination out of order or not at all.  UDP does not support congestion control.  Without the guarantees that TCP support, UDP is generally more efficient.
UDP can broadcast, sending datagrams to all devices on the subnet.  This is useful with DHCP because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.
UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.
Use UDP over TCP when:

You need the lowest latency
Late data is worse than loss of data
You want to implement your own error correction

Source(s) and further reading: TCP and UDP

Networking for game programming
Key differences between TCP and UDP protocols
Difference between TCP and UDP
Transmission control protocol
User datagram protocol
Scaling memcache at Facebook

Remote procedure call (RPC)



Source: Crack the system design interview

In an RPC, a client causes a procedure to execute on a different address space, usually a remote server.  The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program.  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls.  Popular RPC frameworks include Protobuf, Thrift, and Avro.
RPC is a request-response protocol:

Client program - Calls the client stub procedure.  The parameters are pushed onto the stack like a local procedure call.
Client stub procedure - Marshals (packs) procedure id and arguments into a request message.
Client communication module - OS sends the message from the client to the server.
Server communication module - OS passes the incoming packets to the server stub procedure.
Server stub procedure -  Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.
The server response repeats the steps above in reverse order.

Sample RPC calls:
GET /someoperation?data=anId

POST /anotheroperation
{
  ""data"":""anId"";
  ""anotherdata"": ""another value""
}

RPC is focused on exposing behaviors.  RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.
Choose a native library (aka SDK) when:

You know your target platform.
You want to control how your ""logic"" is accessed.
You want to control how error control happens off your library.
Performance and end user experience is your primary concern.

HTTP APIs following REST tend to be used more often for public APIs.
Disadvantage(s): RPC

RPC clients become tightly coupled to the service implementation.
A new API must be defined for every new operation or use case.
It can be difficult to debug RPC.
You might not be able to leverage existing technologies out of the box.  For example, it might require additional effort to ensure RPC calls are properly cached on caching servers such as Squid.

Representational state transfer (REST)
REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server.  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources.  All communication must be stateless and cacheable.
There are four qualities of a RESTful interface:

Identify resources (URI in HTTP) - use the same URI regardless of any operation.
Change with representations (Verbs in HTTP) - use verbs, headers, and body.
Self-descriptive error message (status response in HTTP) - Use status codes, don't reinvent the wheel.
HATEOAS (HTML interface for HTTP) - your web service should be fully accessible in a browser.

Sample REST calls:
GET /someresources/anId

PUT /someresources/anId
{""anotherdata"": ""another value""}

REST is focused on exposing data.  It minimizes the coupling between client/server and is often used for public HTTP APIs.  REST uses a more generic and uniform method of exposing resources through URIs, representation through headers, and actions through verbs such as GET, POST, PUT, DELETE, and PATCH.  Being stateless, REST is great for horizontal scaling and partitioning.
Disadvantage(s): REST

With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy.  For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path.  With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.
REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case.  For example, moving expired documents to the archive folder might not cleanly fit within these verbs.
Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.
Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.

RPC and REST calls comparison



Operation
RPC
REST




Signup
POST /signup
POST /persons


Resign
POST /resign{""personid"": ""1234""}
DELETE /persons/1234


Read a person
GET /readPerson?personid=1234
GET /persons/1234


Read a person’s items list
GET /readUsersItemsList?personid=1234
GET /persons/1234/items


Add an item to a person’s items
POST /addItemToUsersItemsList{""personid"": ""1234"";""itemid"": ""456""}
POST /persons/1234/items{""itemid"": ""456""}


Update an item
POST /modifyItem{""itemid"": ""456"";""key"": ""value""}
PUT /items/456{""key"": ""value""}


Delete an item
POST /removeItem{""itemid"": ""456""}
DELETE /items/456




Source: Do you really know why you prefer REST over RPC

Source(s) and further reading: REST and RPC

Do you really know why you prefer REST over RPC
When are RPC-ish approaches more appropriate than REST?
REST vs JSON-RPC
Debunking the myths of RPC and REST
What are the drawbacks of using REST
Crack the system design interview
Thrift
Why REST for internal use and not RPC

Security
This section could use some updates.  Consider contributing!
Security is a broad topic.  Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:

Encrypt in transit and at rest.
Sanitize all user inputs or any input parameters exposed to user to prevent XSS and SQL injection.
Use parameterized queries to prevent SQL injection.
Use the principle of least privilege.

Source(s) and further reading

API security checklist
Security guide for developers
OWASP top ten

Appendix
You'll sometimes be asked to do 'back-of-the-envelope' estimates.  For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take.  The Powers of two table and Latency numbers every programmer should know are handy references.
Powers of two table
Power           Exact Value         Approx Value        Bytes
---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB

Source(s) and further reading

Powers of two

Latency numbers every programmer should know
Latency Comparison Numbers
--------------------------
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy            10,000   ns       10 us
Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us
Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD
Read 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD
Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms

Notes
-----
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns

Handy metrics based on numbers above:

Read sequentially from disk at 30 MB/s
Read sequentially from 1 Gbps Ethernet at 100 MB/s
Read sequentially from SSD at 1 GB/s
Read sequentially from main memory at 4 GB/s
6-7 world-wide round trips per second
2,000 round trips per second within a data center

Latency numbers visualized

Source(s) and further reading

Latency numbers every programmer should know - 1
Latency numbers every programmer should know - 2
Designs, lessons, and advice from building large distributed systems
Software Engineering Advice from Building Large-Scale Distributed Systems

Additional system design interview questions

Common system design interview questions, with links to resources on how to solve each.




Question
Reference(s)




Design a file sync service like Dropbox
youtube.com


Design a search engine like Google
queue.acm.orgstackexchange.comardendertat.comstanford.edu


Design a scalable web crawler like Google
quora.com


Design Google docs
code.google.comneil.fraser.name


Design a key-value store like Redis
slideshare.net


Design a cache system like Memcached
slideshare.net


Design a recommendation system like Amazon's
hulu.comijcai13.org


Design a tinyurl system like Bitly
n00tc0d3r.blogspot.com


Design a chat app like WhatsApp
highscalability.com


Design a picture sharing system like Instagram
highscalability.comhighscalability.com


Design the Facebook news feed function
quora.comquora.comslideshare.net


Design the Facebook timeline function
facebook.comhighscalability.com


Design the Facebook chat function
erlang-factory.comfacebook.com


Design a graph search function like Facebook's
facebook.comfacebook.comfacebook.com


Design a content delivery network like CloudFlare
figshare.com


Design a trending topic system like Twitter's
michael-noll.comsnikolov .wordpress.com


Design a random ID generation system
blog.twitter.comgithub.com


Return the top k requests during a time interval
cs.ucsb.eduwpi.edu


Design a system that serves data from multiple data centers
highscalability.com


Design an online multiplayer card game
indieflashblog.combuildnewgames.com


Design a garbage collection system
stuffwithstuff.comwashington.edu


Design an API rate limiter
https://stripe.com/blog/


Add a system design question
Contribute



Real world architectures

Articles on how real world systems are designed.




Source: Twitter timelines at scale

Don't focus on nitty gritty details for the following articles, instead:

Identify shared principles, common technologies, and patterns within these articles
Study what problems are solved by each component, where it works, where it doesn't
Review the lessons learned




Type
System
Reference(s)




Data processing
MapReduce - Distributed data processing from Google
research.google.com


Data processing
Spark - Distributed data processing from Databricks
slideshare.net


Data processing
Storm - Distributed data processing from Twitter
slideshare.net







Data store
Bigtable - Distributed column-oriented database from Google
harvard.edu


Data store
HBase - Open source implementation of Bigtable
slideshare.net


Data store
Cassandra - Distributed column-oriented database from Facebook
slideshare.net


Data store
DynamoDB - Document-oriented database from Amazon
harvard.edu


Data store
MongoDB - Document-oriented database
slideshare.net


Data store
Spanner - Globally-distributed database from Google
research.google.com


Data store
Memcached - Distributed memory caching system
slideshare.net


Data store
Redis - Distributed memory caching system with persistence and value types
slideshare.net







File system
Google File System (GFS) - Distributed file system
research.google.com


File system
Hadoop File System (HDFS) - Open source implementation of GFS
apache.org







Misc
Chubby - Lock service for loosely-coupled distributed systems from Google
research.google.com


Misc
Dapper - Distributed systems tracing infrastructure
research.google.com


Misc
Kafka - Pub/sub message queue from LinkedIn
slideshare.net


Misc
Zookeeper - Centralized infrastructure and services enabling synchronization
slideshare.net



Add an architecture
Contribute



Company architectures



Company
Reference(s)




Amazon
Amazon architecture


Cinchcast
Producing 1,500 hours of audio every day


DataSift
Realtime datamining At 120,000 tweets per second


DropBox
How we've scaled Dropbox


ESPN
Operating At 100,000 duh nuh nuhs per second


Google
Google architecture


Instagram
14 million users, terabytes of photosWhat powers Instagram


Justin.tv
Justin.Tv's live video broadcasting architecture


Facebook
Scaling memcached at FacebookTAO: Facebook’s distributed data store for the social graphFacebook’s photo storageHow Facebook Live Streams To 800,000 Simultaneous Viewers


Flickr
Flickr architecture


Mailbox
From 0 to one million users in 6 weeks


Netflix
A 360 Degree View Of The Entire Netflix StackNetflix: What Happens When You Press Play?


Pinterest
From 0 To 10s of billions of page views a month18 million visitors, 10x growth, 12 employees


Playfish
50 million monthly users and growing


PlentyOfFish
PlentyOfFish architecture


Salesforce
How they handle 1.3 billion transactions a day


Stack Overflow
Stack Overflow architecture


TripAdvisor
40M visitors, 200M dynamic page views, 30TB data


Tumblr
15 billion page views a month


Twitter
Making Twitter 10000 percent fasterStoring 250 million tweets a day using MySQL150M active users, 300K QPS, a 22 MB/S firehoseTimelines at scaleBig and small data at TwitterOperations at Twitter: scaling beyond 100 million usersHow Twitter Handles 3,000 Images Per Second


Uber
How Uber scales their real-time market platformLessons Learned From Scaling Uber To 2000 Engineers, 1000 Services, And 8000 Git Repositories


WhatsApp
The WhatsApp architecture Facebook bought for $19 billion


YouTube
YouTube scalabilityYouTube architecture



Company engineering blogs

Architectures for companies you are interviewing with.
Questions you encounter might be from the same domain.


Airbnb Engineering
Atlassian Developers
AWS Blog
Bitly Engineering Blog
Box Blogs
Cloudera Developer Blog
Dropbox Tech Blog
Engineering at Quora
Ebay Tech Blog
Evernote Tech Blog
Etsy Code as Craft
Facebook Engineering
Flickr Code
Foursquare Engineering Blog
GitHub Engineering Blog
Google Research Blog
Groupon Engineering Blog
Heroku Engineering Blog
Hubspot Engineering Blog
High Scalability
Instagram Engineering
Intel Software Blog
Jane Street Tech Blog
LinkedIn Engineering
Microsoft Engineering
Microsoft Python Engineering
Netflix Tech Blog
Paypal Developer Blog
Pinterest Engineering Blog
Quora Engineering
Reddit Blog
Salesforce Engineering Blog
Slack Engineering Blog
Spotify Labs
Twilio Engineering Blog
Twitter Engineering
Uber Engineering Blog
Yahoo Engineering Blog
Yelp Engineering Blog
Zynga Engineering Blog

Source(s) and further reading
Looking to add a blog?  To avoid duplicating work, consider adding your company blog to the following repo:

kilimchoi/engineering-blogs

Under development
Interested in adding a section or helping complete one in-progress?  Contribute!

Distributed computing with MapReduce
Consistent hashing
Scatter gather
Contribute

Credits
Credits and sources are provided throughout this repo.
Special thanks to:

Hired in tech
Cracking the coding interview
High scalability
checkcheckzz/system-design-interview
shashank88/system_design
mmcgrana/services-engineering
System design cheat sheet
A distributed systems reading list
Cracking the system design interview

Contact info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/

"
31,Python,"Awesome Python 
A curated list of awesome Python frameworks, libraries, software and resources.
Inspired by awesome-php.

Awesome Python

Admin Panels
Algorithms and Design Patterns
Asynchronous Programming
Audio
Authentication
Build Tools
Built-in Classes Enhancement
Caching
ChatOps Tools
CMS
Code Analysis
Command-line Interface Development
Command-line Tools
Compatibility
Computer Vision
Concurrency and Parallelism
Configuration
Cryptography
Data Analysis
Data Validation
Data Visualization
Database Drivers
Database
Date and Time
Debugging Tools
Deep Learning
DevOps Tools
Distributed Computing
Distribution
Documentation
Downloader
E-commerce
Editor Plugins and IDEs
Email
Environment Management
Files
Foreign Function Interface
Forms
Functional Programming
Game Development
Geolocation
GUI Development
Hardware
HTML Manipulation
HTTP Clients
Image Processing
Implementations
Interactive Interpreter
Internationalization
Job Scheduler
Logging
Machine Learning
Miscellaneous
Natural Language Processing
Network Virtualization
News Feed
ORM
Package Management
Package Repositories
Permissions
Processes
Recommender Systems
RESTful API
Robotics
RPC Servers
Science
Search
Serialization
Serverless Frameworks
Specific Formats Processing
Static Site Generator
Tagging
Task Queues
Template Engine
Testing
Text Processing
Third-party APIs
URL Manipulation
Video
Web Asset Management
Web Content Extracting
Web Crawling
Web Frameworks
WebSocket
WSGI Servers


Resources

Podcasts
Twitter
Websites
Weekly


Contributing


Admin Panels
Libraries for administrative interfaces.

ajenti - The admin panel your servers deserve.
django-grappelli - A jazzy skin for the Django Admin-Interface.
django-jet - Modern responsive template for the Django admin interface with improved functionality.
django-suit - Alternative Django Admin-Interface (free only for Non-commercial use).
django-xadmin - Drop-in replacement of Django admin comes with lots of goodies.
jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django)
flask-admin - Simple and extensible administrative interface framework for Flask.
flower - Real-time monitor and web admin for Celery.
wooey - A Django app which creates automatic web UIs for Python scripts.

Algorithms and Design Patterns
Python implementation of algorithms and design patterns.

algorithms - Minimal examples of data structures and algorithms in Python.
PyPattyrn - A simple yet effective library for implementing common design patterns.
python-patterns - A collection of design patterns in Python.
sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.

Asynchronous Programming

asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.

awesome-asyncio


uvloop - Ultra fast asyncio event loop.
Twisted - An event-driven networking engine.

Audio
Libraries for manipulating audio and its metadata.

Audio

audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
dejavu - Audio fingerprinting and recognition.
mingus - An advanced music theory and notation package with MIDI file and playback support.
pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications.
pydub - Manipulate audio with a simple and easy high level interface.
TimeSide - Open web audio processing framework.


Metadata

beets - A music library manager and MusicBrainz tagger.
eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
mutagen - A Python module to handle audio metadata.
tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.



Authentication
Libraries for implementing authentications schemes.

OAuth

authlib - JavaScript Object Signing and Encryption draft implementation.
django-allauth - Authentication app for Django that ""just works.""
django-oauth-toolkit - OAuth 2 goodies for Django.
oauthlib - A generic and thorough implementation of the OAuth request-signing logic.
python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers.
python-social-auth - An easy-to-setup social authentication mechanism.


JWT

pyjwt - JSON Web Token implementation in Python.
python-jose - A JOSE implementation in Python.
python-jwt - A module for generating and verifying JSON Web Tokens.



Build Tools
Compile software from source code.

BitBake - A make-like build tool for embedded Linux.
buildout - A build system for creating, assembling and deploying applications from multiple parts.
PlatformIO - A console tool to build code with different development platforms.
pybuilder - A continuous build tool written in pure Python.
SCons - A software construction tool.

Built-in Classes Enhancement
Libraries for enhancing Python built-in classes.

dataclasses - (Python standard library) Data classes.
attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions.
bidict - Efficient, Pythonic bidirectional map data structures and related functionality..
Box - Python dictionaries with advanced dot notation access.
DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.

CMS
Content Management Systems.

wagtail - A Django content management system.
django-cms - An Open source enterprise CMS based on the Django.
feincms - One of the most advanced Content Management Systems built on Django.
Kotti - A high-level, Pythonic web application framework built on Pyramid.
mezzanine - A powerful, consistent, and flexible content management platform.
plone - A CMS built on top of the open source application server Zope.
quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.

Caching
Libraries for caching data.

beaker - A WSGI middleware for sessions and caching.
django-cache-machine - Automatic caching and invalidation for Django models.
django-cacheops - A slick ORM cache with automatic granular event-driven invalidation.
dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors.
HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention.
pylibmc - A Python wrapper around the libmemcached interface.
python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.

ChatOps Tools
Libraries for chatbot development.

errbot - The easiest and most popular chatbot to implement ChatOps.

Code Analysis
Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis.

Code Analysis

coala - Language independent and easily extendable code analysis application.
code2flow - Turn your Python and JavaScript code into DOT flowcharts.
prospector - A tool to analyse Python code.
pycallgraph - A library that visualises the flow (call graph) of your Python application.


Code Linters

flake8 - A wrapper around pycodestyle, pyflakes and McCabe.

awesome-flake8-extensions


pylint - A fully customizable source code analyzer.
pylama - A code audit tool for Python and JavaScript.
wemake-python-styleguide - The strictest and most opinionated python linter ever.


Code Formatters

black - The uncompromising Python code formatter.
yapf - Yet another Python code formatter from Google.


Static Type Checkers, also see awesome-python-typing

mypy - Check variable types during compile time.
pyre-check - Performant type checking.


Static Type Annotations Generators

MonkeyType - A system for Python that generates static type annotations by collecting runtime types



Command-line Interface Development
Libraries for building command-line applications.

Command-line Application Development

cement - CLI Application Framework for Python.
click - A package for creating beautiful command line interfaces in a composable way.
cliff - A framework for creating command-line programs with multi-level commands.
clint - Python Command-line Application Tools.
docopt - Pythonic command line arguments parser.
python-fire - A library for creating command line interfaces from absolutely any Python object.
python-prompt-toolkit - A library for building powerful interactive command lines.


Terminal Rendering

asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations).
bashplotlib - Making basic plots in the terminal.
colorama - Cross-platform colored terminal text.
tqdm - Fast, extensible progress bar for loops and CLI.



Command-line Tools
Useful CLI-based tools for productivity.

Productivity Tools

cookiecutter - A command-line utility that creates projects from cookiecutters (project templates).
doitlive - A tool for live presentations in the terminal.
howdoi - Instant coding answers via the command line.
PathPicker - Select files out of bash output.
percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX.
thefuck - Correcting your previous console command.
tmuxp - A tmux session manager.
try - A dead simple CLI to try out python packages - it's never been easier.


CLI Enhancements

httpie - A command line HTTP client, a user-friendly cURL replacement.
kube-shell - An integrated shell for working with the Kubernetes CLI.
mycli - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.
pgcli - Postgres CLI with autocompletion and syntax highlighting.
saws - A Supercharged aws-cli.



Compatibility
Libraries for migrating from Python 2 to 3.

python-future - The missing compatibility layer between Python 2 and Python 3.
python-modernize - Modernizes Python code for eventual Python 3 migration.
six - Python 2 and 3 compatibility utilities.

Computer Vision
Libraries for computer vision.

OpenCV - Open Source Computer Vision Library.
pytesseract - Another wrapper for Google Tesseract OCR.
SimpleCV - An open source framework for building computer vision applications.

Concurrency and Parallelism
Libraries for concurrent and parallel execution. Also see awesome-asyncio.

concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables.
multiprocessing - (Python standard library) Process-based parallelism.
eventlet - Asynchronous framework with WSGI support.
gevent - A coroutine-based Python networking library that uses greenlet.
uvloop - Ultra fast implementation of asyncio event loop on top of libuv.
scoop - Scalable Concurrent Operations in Python.

Configuration
Libraries for storing and parsing configuration options.

configobj - INI file parser with validation.
configparser - (Python standard library) INI file parser.
profig - Config from multiple formats with value conversion.
python-decouple - Strict separation of settings from code.

Cryptography

cryptography - A package designed to expose cryptographic primitives and recipes to Python developers.
paramiko - The leading native Python SSHv2 protocol library.
passlib - Secure password storage/hashing library, very high level.
pynacl - Python binding to the Networking and Cryptography (NaCl) library.

Data Analysis
Libraries for data analyzing.

Blaze - NumPy and Pandas interface to Big Data.
Open Mining - Business Intelligence (BI) in Pandas interface.
Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Optimus - Agile Data Science Workflows made easy with PySpark.

Data Validation
Libraries for validating data. Used for forms in many cases.

Cerberus - A lightweight and extensible data validation library.
colander - Validating and deserializing data obtained via XML, JSON, an HTML form post.
jsonschema - An implementation of JSON Schema for Python.
schema - A library for validating Python data structures.
Schematics - Data Structure Validation.
valideer - Lightweight extensible data validation and adaptation library.
voluptuous - A Python data validation library.

Data Visualization
Libraries for visualizing data. Also see awesome-javascript.

Altair - Declarative statistical visualization library for Python.
Bokeh - Interactive Web Plotting for Python.
bqplot - Interactive Plotting Library for the Jupyter Notebook
Dash - Built on top of Flask, React and Plotly aimed at analytical web applications.

awesome-dash


plotnine - A grammar of graphics for Python based on ggplot2.
Matplotlib - A Python 2D plotting library.
Pygal - A Python SVG Charts Creator.
PyGraphviz - Python interface to Graphviz.
PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.
Seaborn - Statistical data visualization using Matplotlib.
VisPy - High-performance scientific visualization based on OpenGL.

Database
Databases implemented in Python.

pickleDB - A simple and lightweight key-value store for Python.
tinydb - A tiny, document-oriented database.
ZODB - A native object database for Python. A key-value and object graph database.

Database Drivers
Libraries for connecting and operating databases.

MySQL - awesome-mysql

mysqlclient - MySQL connector with Python 3 support (mysql-python fork).
PyMySQL - A pure Python MySQL driver compatible to mysql-python.


PostgreSQL - awesome-postgres

psycopg2 - The most popular PostgreSQL adapter for Python.
queries - A wrapper of the psycopg2 library for interacting with PostgreSQL.


Other Relational Databases

pymssql - A simple database interface to Microsoft SQL Server.
SuperSQLite - A supercharged SQLite library built on top of apsw.


NoSQL Databases

cassandra-driver - The Python Driver for Apache Cassandra.
happybase - A developer-friendly library for Apache HBase.
kafka-python - The Python client for Apache Kafka.
py2neo - A client library and toolkit for working with Neo4j.
pymongo - The official Python client for MongoDB.
redis-py - The Python client for Redis.


Asynchronous Clients

motor - The async Python driver for MongoDB.



Date and Time
Libraries for working with dates and times.

Chronyk - A Python 3 library for parsing human-written times and dates.
dateutil - Extensions to the standard Python datetime module.
delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes.
moment - A Python library for dealing with dates/times. Inspired by Moment.js.
Pendulum - Python datetimes made easy.
PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string.
pytz - World timezone definitions, modern and historical. Brings the tz database into Python.
when.py - Providing user-friendly functions to help perform common date and time actions.
maya - Datetimes for Humans.

Debugging Tools
Libraries for debugging code.

pdb-like Debugger

ipdb - IPython-enabled pdb.
pdb++ - Another drop-in replacement for pdb.
pudb - A full-screen, console-based Python debugger.
wdb - An improbable web debugger through WebSockets.


Tracing

lptrace - strace for Python programs.
manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.
pyringe - Debugger capable of attaching to and injecting code into Python processes.
python-hunter - A flexible code tracing toolkit.


Profiler

line_profiler - Line-by-line profiling.
memory_profiler - Monitor Memory usage of Python code.
profiling - An interactive Python profiler.
py-spy - A sampling profiler for Python programs. Written in Rust.
pyflame - A ptracing profiler For Python.
vprof - Visual Python profiler.


Others

icecream - Inspect variables, expressions, and program execution with a single, simple function call.
django-debug-toolbar - Display various debug information for Django.
django-devserver - A drop-in replacement for Django's runserver.
flask-debugtoolbar - A port of the django-debug-toolbar to flask.
pyelftools - Parsing and analyzing ELF files and DWARF debugging information.



Deep Learning
Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning.

caffe - A fast open framework for deep learning..
keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.
mxnet - A deep learning framework designed for both efficiency and flexibility.
pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
SerpentAI - Game agent framework. Use any video game as a deep learning sandbox.
tensorflow - The most popular Deep Learning framework created by Google.
Theano - A library for fast numerical computation.

DevOps Tools
Software and libraries for DevOps.

ansible - A radically simple IT automation platform.
cloudinit - A multi-distribution package that handles early initialization of a cloud instance.
cuisine - Chef-like functionality for Fabric.
docker-compose - Fast, isolated development environments using Docker.
fabric - A simple, Pythonic tool for remote execution and deployment.
fabtools - Tools for writing awesome Fabric files.
honcho - A Python clone of Foreman, for managing Procfile-based applications.
OpenStack - Open source software for building private and public clouds.
pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect.
psutil - A cross-platform process and system utilities module.
saltstack - Infrastructure automation and management system.
supervisor - Supervisor process control system for UNIX.

Distributed Computing
Frameworks and libraries for Distributed Computing.

Batch Processing

PySpark - Apache Spark Python API.
dask - A flexible parallel computing library for analytic computing.
luigi - A module that helps you build complex pipelines of batch jobs.
mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services.
Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem.


Stream Processing

faust - A stream processing library, porting the ideas from Kafka Streams to Python.
streamparse - Run Python code against real-time streams of data via Apache Storm.



Distribution
Libraries to create packaged executables for release distribution.

dh-virtualenv - Build and distribute a virtualenv as a Debian package.
Nuitka - Compile scripts, modules, packages to an executable or extension module.
py2app - Freezes Python scripts (Mac OS X).
py2exe - Freezes Python scripts (Windows).
PyInstaller - Converts Python programs into stand-alone executables (cross-platform).
pynsist - A tool to build Windows installers, installers bundle Python itself.

Documentation
Libraries for generating project documentation.

sphinx - Python Documentation generator.

awesome-sphinxdoc


pdoc - Epydoc replacement to auto generate API documentation for Python libraries.
pycco - The literate-programming-style documentation generator.

Downloader
Libraries for downloading.

s3cmd - A command line tool for managing Amazon S3 and CloudFront.
s4cmd - Super S3 command line tool, good for higher performance.
you-get - A YouTube/Youku/Niconico video downloader written in Python 3.
youtube-dl - A small command-line program to download videos from YouTube.

E-commerce
Frameworks and libraries for e-commerce and payments.

alipay - Unofficial Alipay API for Python.
Cartridge - A shopping cart app built using the Mezzanine.
django-oscar - An open-source e-commerce framework for Django.
django-shop - A Django based shop system.
merchant - A Django app to accept payments from various payment processors.
money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.
python-currencies - Display money format and its filthy currencies.
forex-python - Foreign exchange rates, Bitcoin price index and currency conversion.
saleor - An e-commerce storefront for Django.
shoop - An open source E-Commerce platform based on Django.

Editor Plugins and IDEs

Emacs

elpy - Emacs Python Development Environment.


Sublime Text

anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.
SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi.


Vim

jedi-vim - Vim bindings for the Jedi auto-completion library for Python.
python-mode - An all in one plugin for turning Vim into a Python IDE.
YouCompleteMe - Includes Jedi-based completion engine for Python.


Visual Studio

PTVS - Python Tools for Visual Studio.


Visual Studio Code

Python - The official VSCode extension with rich support for Python.


IDE

PyCharm - Commercial Python IDE by JetBrains. Has free community edition available.
spyder - Open Source Python IDE.



Email
Libraries for sending and parsing email.

envelopes - Mailing for human beings.
flanker - An email address and Mime parsing library.
imbox - Python IMAP for Humans.
inbox.py - Python SMTP Server for Humans.
lamson - Pythonic SMTP Application Server.
Marrow Mailer - High-performance extensible mail delivery framework.
modoboa - A mail hosting and management platform including a modern and simplified Web UI.
Nylas Sync Engine - Providing a RESTful API on top of a powerful email sync platform.
yagmail - Yet another Gmail/SMTP client.

Environment Management
Libraries for Python version and virtual environment management.

pyenv - Simple Python version management.
pipenv - Python Development Workflow for Humans.
poetry - Python dependency management and packaging made easy.
virtualenv - A tool to create isolated Python environments.

Files
Libraries for file manipulation and MIME type detection.

mimetypes - (Python standard library) Map filenames to MIME types.
path.py - A module wrapper for os.path.
pathlib - (Python standard library) An cross-platform, object-oriented path library.
PyFilesystem2 - Python's filesystem abstraction layer.
python-magic - A Python interface to the libmagic file type identification library.
Unipath - An object-oriented approach to file/directory operations.
watchdog - API and shell utilities to monitor file system events.

Foreign Function Interface
Libraries for providing foreign function interface.

cffi - Foreign Function Interface for Python calling C code.
ctypes - (Python standard library) Foreign Function Interface for Python calling C code.
PyCUDA - A Python wrapper for Nvidia's CUDA API.
SWIG - Simplified Wrapper and Interface Generator.

Forms
Libraries for working with forms.

Deform - Python HTML form generation library influenced by the formish form generation library.
django-bootstrap3 - Bootstrap 3 integration with Django.
django-bootstrap4 - Bootstrap 4 integration with Django.
django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way.
django-remote-forms - A platform independent Django form serializer.
WTForms - A flexible forms validation and rendering library.

Functional Programming
Functional Programming with Python.

Coconut - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.
CyToolz - Cython implementation of Toolz: High performance functional utilities.
fn.py - Functional programming in Python: implementation of missing features to enjoy FP.
funcy - A fancy and practical functional tools.
Toolz - A collection of functional utilities for iterators, functions, and dictionaries.

GUI Development
Libraries for working with graphical user interface applications.

curses - Built-in wrapper for ncurses used to create terminal GUI applications.
Eel - A library for making simple Electron-like offline HTML/JS GUI apps.
enaml - Creating beautiful user-interfaces with Declarative Syntax like QML.
Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.
Gooey - Turn command line programs into a full GUI application with one line.
kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.
pyglet - A cross-platform windowing and multimedia library for Python.
PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).
PyQt - Python bindings for the Qt cross-platform application and UI framework.
PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi.
pywebview - A lightweight cross-platform native wrapper around a webview component.
Tkinter - Tkinter is Python's de-facto standard GUI package.
Toga - A Python native, OS native GUI toolkit.
urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.
wxPython - A blending of the wxWidgets C++ class library with the Python.

Game Development
Awesome game development libraries.

Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.
Harfang3D - Python framework for 3D, VR and game development.
Panda3D - 3D game engine developed by Disney.
Pygame - Pygame is a set of Python modules designed for writing games.
PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.
PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs.
PySDL2 - A ctypes based wrapper for the SDL2 library.
RenPy - A Visual Novel engine.

Geolocation
Libraries for geocoding addresses and working with latitudes and longitudes.

django-countries - A Django app that provides a country field for models and forms.
GeoDjango - A world-class geographic web framework.
GeoIP - Python API for MaxMind GeoIP Legacy Database.
geojson - Python bindings and utilities for GeoJSON.
geopy - Python Geocoding Toolbox.
pygeoip - Pure Python GeoIP API.

HTML Manipulation
Libraries for working with HTML and XML.

BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.
bleach - A whitelist-based HTML sanitization and text linkification library.
cssutils - A CSS library for Python.
html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments.
lxml - A very fast, easy-to-use and versatile library for handling HTML and XML.
MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python.
pyquery - A jQuery-like library for parsing HTML.
untangle - Converts XML documents to Python objects for easy access.
WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF.
xmldataset - Simple XML Parsing.
xmltodict - Working with XML feel like you are working with JSON.

HTTP Clients
Libraries for working with HTTP.

grequests - requests + gevent for asynchronous HTTP requests.
httplib2 - Comprehensive HTTP client library.
requests - HTTP Requests for Humans.
treq - Python requests like API built on top of Twisted's HTTP client.
urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.

Hardware
Libraries for programming with hardware.

ino - Command line toolkit for working with Arduino.
keyboard - Hook and simulate global keyboard events on Windows and Linux.
mouse - Hook and simulate global mouse events on Windows and Linux.
Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.
PyUserInput - A module for cross-platform control of the mouse and keyboard.
scapy - A brilliant packet manipulation library.
wifi - A Python library and command line tool for working with WiFi on Linux.

Image Processing
Libraries for manipulating images.

hmap - Image histogram remapping.
imgSeek - A project for searching a collection of images using visual similarity.
nude.py - Nudity detection.
pagan - Retro identicon (Avatar) generation based on input string and hash.
pillow - Pillow is the friendly PIL fork.
pyBarcode - Create barcodes in Python without needing PIL.
pygram - Instagram-like image filters.
python-qrcode - A pure Python QR Code generator.
Quads - Computer art based on quadtrees.
scikit-image - A Python library for (scientific) image processing.
thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.
wand - Python bindings for MagickWand, C API for ImageMagick.

Implementations
Implementations of Python.

CPython - Default, most widely used implementation of the Python programming language written in C.
Cython - Optimizing Static Compiler for Python.
CLPython - Implementation of the Python programming language written in Common Lisp.
Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).
IronPython - Implementation of the Python programming language written in C#.
Jython - Implementation of Python programming language written in Java for the JVM.
MicroPython - A lean and efficient Python programming language implementation.
Numba - Python JIT compiler to LLVM aimed at scientific Python.
PeachPy - x86-64 assembler embedded in Python.
Pyjion - A JIT for Python based upon CoreCLR.
PyPy - A very fast and compliant implementation of the Python language.
Pyston - A Python implementation using JIT techniques.
Stackless Python - An enhanced version of the Python programming language.

Interactive Interpreter
Interactive Python interpreters (REPL).

bpython - A fancy interface to the Python interpreter.
Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively.

awesome-jupyter


ptpython - Advanced Python REPL built on top of the python-prompt-toolkit.

Internationalization
Libraries for working with i18n.

Babel - An internationalization library for Python.
PyICU - A wrapper of International Components for Unicode C++ library (ICU).

Job Scheduler
Libraries for scheduling jobs.

APScheduler - A light but powerful in-process task scheduler that lets you schedule functions.
django-schedule - A calendaring app for Django.
doit - A task runner and build tool.
gunnery - Multipurpose task execution tool for distributed systems with web-based interface.
Joblib - A set of tools to provide lightweight pipelining in Python.
Plan - Writing crontab file in Python like a charm.
schedule - Python job scheduling for humans.
Spiff - A powerful workflow engine implemented in pure Python.
TaskFlow - A Python library that helps to make task execution easy, consistent and reliable.
Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.

Logging
Libraries for generating and working with logs.

Eliot - Logging for complex & distributed systems.
logbook - Logging replacement for Python.
logging - (Python standard library) Logging facility for Python.
raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.

Machine Learning
Libraries for Machine Learning. Also see awesome-machine-learning.

H2O - Open Source Fast Scalable Machine Learning Platform.
Metrics - Machine learning evaluation metrics.
NuPIC - Numenta Platform for Intelligent Computing.
scikit-learn - The most popular Python library for Machine Learning.
Spark ML - Apache Spark's scalable Machine Learning library.
vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit.
xgboost - A scalable, portable, and distributed gradient boosting library.

Microsoft Windows
Python programming on Microsoft Windows.

Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.
pythonlibs - Unofficial Windows binaries for Python extension packages.
PythonNet - Python Integration with the .NET Common Language Runtime (CLR).
PyWin32 - Python Extensions for Windows.
WinPython - Portable development environment for Windows 7/8.

Miscellaneous
Useful libraries or tools that don't fit in the categories above.

blinker - A fast Python in-process signal/event dispatching system.
boltons - A set of pure-Python utilities.
itsdangerous - Various helpers to pass trusted data to untrusted environments.
pluginbase - A simple but flexible plugin system for Python.
tryton - A general purpose business framework.

Natural Language Processing
Libraries for working with human languages.

General

gensim - Topic Modeling for Humans.
langid.py - Stand-alone language identification system.
nltk - A leading platform for building Python programs to work with human language data.
pattern - A web mining module for the Python.
polyglot - Natural language pipeline supporting hundreds of languages.
pytext - A natural language modeling framework based on PyTorch.
PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research.
spacy - A library for industrial-strength natural language processing in Python and Cython.
stanfordnlp - The Stanford NLP Group's official Python library, supporting 50+ languages.


Chinese

jieba - The most popular Chinese text segmentation library.
pkuseg-python - A toolkit for Chinese word segmentation in various domains.
snownlp - A library for processing Chinese text.
funNLP - A collection of tools and datasets for Chinese NLP.



Network Virtualization
Tools and libraries for Virtual Networking and SDN (Software Defined Networking).

mininet - A popular network emulator and API written in Python.
napalm - Cross-vendor API to manipulate network devices.
pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.

News Feed
Libraries for building user's activities.

django-activity-stream - Generating generic activity streams from the actions on your site.
Stream Framework - Building news feed and notification systems using Cassandra and Redis.

ORM
Libraries that implement Object-Relational Mapping or data mapping techniques.

Relational Databases

Django Models - The Django ORM.
SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper.

awesome-sqlalchemy


dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.
orator -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.
orm - An async ORM.
peewee - A small, expressive ORM.
pony - ORM that provides a generator-oriented interface to SQL.
pydal - A pure Python Database Abstraction Layer.


NoSQL Databases

hot-redis - Rich Python data types for Redis.
mongoengine - A Python Object-Document-Mapper for working with MongoDB.
PynamoDB - A Pythonic interface for Amazon DynamoDB.
redisco - A Python Library for Simple Models and Containers Persisted in Redis.



Package Management
Libraries for package and dependency management.

pip - The Python package and dependency manager.

PyPI
pip-tools - A set of tools to keep your pinned Python dependencies fresh.


conda - Cross-platform, Python-agnostic binary package manager.

Package Repositories
Local PyPI repository server and proxies.

warehouse - Next generation Python Package Repository (PyPI).
bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA).
devpi - PyPI server and packaging/testing/release tool.
localshop - Local PyPI server (custom packages and auto-mirroring of pypi).

Permissions
Libraries that allow or deny users access to data or functionality.

django-guardian - Implementation of per object permissions for Django 1.2+
django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.

Processes
Libraries for starting and communicating with OS processes.

delegator.py - Subprocesses for Humans 2.0.
sarge - Yet another wrapper for subprocess.
sh - A full-fledged subprocess replacement for Python.

Recommender Systems
Libraries for building recommender systems.

annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage.
fastFM - A library for Factorization Machines.
implicit - A fast Python implementation of collaborative filtering for implicit datasets.
libffm - A library for Field-aware Factorization Machine (FFM).
lightfm - A Python implementation of a number of popular recommendation algorithms.
spotlight - Deep recommender models using PyTorch.
Surprise - A scikit for building and analyzing recommender systems.
tensorrec - A Recommendation Engine Framework in TensorFlow.

RESTful API
Libraries for building RESTful APIs.

Django

django-rest-framework - A powerful and flexible toolkit to build web APIs.
django-tastypie - Creating delicious APIs for Django apps.


Flask

eve - REST API framework powered by Flask, MongoDB and good intentions.
flask-api - Browsable Web APIs for Flask.
flask-restful - Quickly building REST APIs for Flask.


Pyramid

cornice - A RESTful framework for Pyramid.


Framework agnostic

apistar - A smart Web API framework, designed for Python 3.
falcon - A high-performance framework for building cloud APIs and web app backends.
fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.
hug - A Python 3 framework for cleanly exposing APIs.
sandman2 - Automated REST APIs for existing database-driven systems.
sanic - A Python 3.6+ web server and web framework that's written to go fast.
vibora - Fast, efficient and asynchronous Web framework inspired by Flask.



Robotics
Libraries for robotics.

PythonRobotics - This is a compilation of various robotics algorithms with visualizations.
rospy - This is a library for ROS (Robot Operating System).

RPC Servers
RPC-compatible servers.

zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack.

Science
Libraries for scientific computing. Also see Python-for-Scientists

astropy - A community Python library for Astronomy.
bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis.
bccb - Collection of useful code related to biological analysis.
Biopython - Biopython is a set of freely available tools for biological computation.
cclib - A library for parsing and interpreting the results of computational chemistry packages.
Colour - Implementing a comprehensive number of colour theory transformations and algorithms.
NetworkX - A high-productivity software for complex networks.
NIPY - A collection of neuroimaging toolkits.
NumPy - A fundamental package for scientific computing with Python.
Open Babel - A chemical toolbox designed to speak the many languages of chemical data.
ObsPy - A Python toolbox for seismology.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.
PyMC - Markov Chain Monte Carlo sampling toolkit.
QuTiP - Quantum Toolbox in Python.
RDKit - Cheminformatics and Machine Learning Software.
SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
statsmodels - Statistical modeling and econometrics in Python.
SymPy - A Python library for symbolic mathematics.
Zipline - A Pythonic algorithmic trading library.
SimPy -  A process-based discrete-event simulation framework.

Search
Libraries and software for indexing and performing search queries on data.

elasticsearch-py - The official low-level Python client for Elasticsearch.
elasticsearch-dsl-py - The official high-level Python client for Elasticsearch.
django-haystack - Modular search for Django.
pysolr - A lightweight Python wrapper for Apache Solr.
whoosh - A fast, pure Python search engine library.

Serialization
Libraries for serializing complex data types

marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes.
pysimdjson - A Python bindings for simdjson.
python-rapidjson - A Python wrapper around RapidJSON.
ultrajson - A fast JSON decoder and encoder written in C with Python bindings.

Serverless Frameworks
Frameworks for developing serverless Python code.

python-lambda - A toolkit for developing and deploying Python code in AWS Lambda.
Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.

Specific Formats Processing
Libraries for parsing and manipulating specific text formats.

General

tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML.


Office

openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.
pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.
python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files.
python-pptx - Python library for creating and updating PowerPoint (.pptx) files.
unoconv - Convert between any document format supported by LibreOffice/OpenOffice.
XlsxWriter - A Python module for creating Excel .xlsx files.
xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.
xlwt / xlrd - Writing and reading data and formatting information from Excel files.


PDF

PDFMiner - A tool for extracting information from PDF documents.
PyPDF2 - A library capable of splitting, merging and transforming PDF pages.
ReportLab - Allowing Rapid creation of rich PDF documents.


Markdown

Mistune - Fastest and full featured pure Python parsers of Markdown.
Python-Markdown - A Python implementation of John Gruber’s Markdown.


YAML

PyYAML - YAML implementations for Python.


CSV

csvkit - Utilities for converting to and working with CSV.


Archive

unp - A command line tool that can unpack archives easily.



Static Site Generator
Static site generator is a software that takes some text + templates as input and produces HTML files on the output.

mkdocs - Markdown friendly documentation generator.
pelican - Static site generator that supports Markdown and reST syntax.
lektor - An easy to use static CMS and blog engine.
nikola - A static website and blog generator.

Tagging
Libraries for tagging items.

django-taggit - Simple tagging for Django.

Task Queues
Libraries for working with task queues.

celery - An asynchronous task queue/job queue based on distributed message passing.
huey - Little multi-threaded task queue.
mrq - A distributed worker task queue in Python using Redis & gevent.
rq - Simple job queues for Python.

Template Engine
Libraries and tools for templating and lexing.

Jinja2 - A modern and designer friendly templating language.
Genshi - Python templating toolkit for generation of web-aware output.
Mako - Hyperfast and lightweight templating for the Python platform.

Testing
Libraries for testing codebases and generating test data.

Testing Frameworks

pytest - A mature full-featured Python testing tool.
hypothesis - Hypothesis is an advanced Quickcheck style property based testing library.
nose2 - The successor to nose, based on `unittest2.
Robot Framework - A generic test automation framework.
unittest - (Python standard library) Unit testing framework.


Test Runners

green - A clean, colorful test runner.
mamba - The definitive testing tool for Python. Born under the banner of BDD.
tox - Auto builds and tests distributions in multiple Python versions


GUI / Web Testing

locust - Scalable user load testing tool written in Python.
PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings.
Selenium - Python bindings for Selenium WebDriver.
sixpack - A language-agnostic A/B Testing framework.
splinter - Open source tool for testing web applications.


Mock

mock - (Python standard library) A mocking and patching library.
doublex - Powerful test doubles framework for Python.
freezegun - Travel through time by mocking the datetime module.
httmock - A mocking library for requests for Python 2.6+ and 3.2+.
httpretty - HTTP request mock tool for Python.
mocket - A socket mock framework with gevent/asyncio/SSL support.
responses - A utility library for mocking out the requests Python library.
VCR.py - Record and replay HTTP interactions on your tests.


Object Factories

factory_boy - A test fixtures replacement for Python.
mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.
model_mommy - Creating random fixtures for testing in Django.


Code Coverage

coverage - Code coverage measurement.


Fake Data

mimesis - is a Python library that help you generate fake data.
fake2db - Fake database generator.
faker - A Python package that generates fake data.
radar - Generate random datetime / time.



Text Processing
Libraries for parsing and manipulating plain texts.

General

chardet - Python 2/3 compatible character encoding detector.
difflib - (Python standard library) Helpers for computing deltas.
ftfy - Makes Unicode text less broken and more consistent automagically.
fuzzywuzzy - Fuzzy String Matching.
Levenshtein - Fast computation of Levenshtein distance and string similarity.
pangu.py - Paranoid text spacing.
pyfiglet - An implementation of figlet written in Python.
pypinyin - Convert Chinese hanzi (漢字) to pinyin (拼音).
textdistance - Compute distance between sequences with 30+ algorithms.
unidecode - ASCII transliterations of Unicode text.


Slugify

awesome-slugify - A Python slugify library that can preserve unicode.
python-slugify - A Python slugify library that translates unicode to ASCII.
unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency.


Unique identifiers

hashids - Implementation of hashids in Python.
shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs.


Parser

ply - Implementation of lex and yacc parsing tools for Python.
pygments - A generic syntax highlighter.
pyparsing - A general purpose framework for generating parsers.
python-nameparser - Parsing human names into their individual components.
python-phonenumbers - Parsing, formatting, storing and validating international phone numbers.
python-user-agents - Browser user agent parser.
sqlparse - A non-validating SQL parser.



Third-party APIs
Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries.

apache-libcloud - One Python library for all clouds.
boto3 - Python interface to Amazon Web Services.
django-wordpress - WordPress models and views for Django.
facebook-sdk - Facebook Platform Python SDK.
google-api-python-client - Google APIs Client Library for Python.
gspread - Google Spreadsheets Python API.
twython - A Python wrapper for the Twitter API.

URL Manipulation
Libraries for parsing URLs.

furl - A small Python library that makes parsing and manipulating URLs easy.
purl - A simple, immutable URL class with a clean API for interrogation and manipulation.
pyshorteners - A pure Python URL shortening lib.
webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.

Video
Libraries for manipulating video and GIFs.

moviepy - A module for script-based movie editing with many formats, including animated GIFs.
scikit-video - Video processing routines for SciPy.

Web Asset Management
Tools for managing, compressing and minifying website assets.

django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file.
django-pipeline - An asset packaging library for Django.
django-storages - A collection of custom storage back ends for Django.
fanstatic - Packages, optimizes, and serves static file dependencies as Python packages.
fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP.
flask-assets - Helps you integrate webassets into your Flask app.
webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.

Web Content Extracting
Libraries for extracting web contents.

html2text - Convert HTML to Markdown-formatted text.
lassie - Web Content Retrieval for Humans.
micawber - A small library for extracting rich content from URLs.
newspaper - News extraction, article extraction and content curation in Python.
python-readability - Fast Python port of arc90's readability tool.
requests-html - Pythonic HTML Parsing for Humans.
sumy - A module for automatic summarization of text documents and HTML pages.
textract - Extract text from any document, Word, PowerPoint, PDFs, etc.
toapi - Every web site provides APIs.

Web Crawling
Libraries to automate web scraping.

cola - A distributed crawling framework.
feedparser - Universal feed parser.
grab - Site scraping framework.
MechanicalSoup - A Python library for automating interaction with websites.
pyspider - A powerful spider system.
robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser.
scrapy - A fast high-level screen scraping and web crawling framework.
portia - Visual scraping for Scrapy.

Web Frameworks
Traditional full stack web frameworks. Also see RESTful API

Synchronous

Django - The most popular web framework in Python.

awesome-django


Flask - A microframework for Python.

awesome-flask


Pyramid - A small, fast, down-to-earth, open source Python web framework.

awesome-pyramid


Masonite - The modern and developer centric Python web framework.


Asynchronous

Tornado - A web framework and asynchronous networking library.



WebSocket
Libraries for working with WebSocket.

autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio.
channels - Developer-friendly asynchrony for Django.
websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity.

WSGI Servers
WSGI-compatible web servers.

bjoern - Asynchronous, very fast and written in C.
gunicorn - Pre-forked, partly written in C.
uWSGI - A project aims at developing a full stack for building hosting services, written in C.
waitress - Multi-threaded, powers Pyramid.
werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.

Resources
Where to discover new Python libraries.
Podcasts

From Python Import Podcast
Podcast.init
Python Bytes
Python Testing
Radio Free Python
Talk Python To Me
Test and Code

Twitter

@codetengu
@getpy
@importpython
@planetpython
@pycoders
@pypi
@pythontrending
@PythonWeekly
@TalkPython
@realpython

Websites

/r/CoolGithubProjects
/r/Python
Awesome Python @LibHunt
Django Packages
Full Stack Python
Python Cheatsheet
Python ZEEF
Python 开发社区
Real Python
Trending Python repositories on GitHub today
Сообщество Python Программистов

Weekly

CodeTengu Weekly 碼天狗週刊
Import Python Newsletter
Pycoder's Weekly
Python Weekly
Python Tricks

Contributing
Your contributions are always welcome! Please take a look at the contribution guidelines first.
I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding 👍 to them. Pull requests will be merged when their votes reach 20.

If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.
"
32,Python,"Public APIs 
A collective list of free APIs for use in software and web development.
Sponsor:

A public API for this project can be found here - thanks to DigitalOcean for helping us provide this service!
For information on contributing to this project, please see the contributing guide.
Please note a passing build status indicates all listed APIs are available since the last update. A failing build status indicates that 1 or more services may be unavailable at the moment.
Index

Animals
Anime
Anti-Malware
Art & Design
Books
Business
Calendar
Cloud Storage & File Sharing
Continuous Integration
Cryptocurrency
Currency Exchange
Data Validation
Development
Dictionaries
Documents & Productivity
Environment
Events
Finance
Food & Drink
Fraud Prevention
Games & Comics
Geocoding
Government
Health
Jobs
Machine Learning
Music
News
Open Data
Open Source Projects
Patent
Personality
Photography
Science & Math
Security
Shopping
Social
Sports & Fitness
Test Data
Text Analysis
Tracking
Transportation
URL Shorteners
Vehicle
Video
Weather

Animals



API
Description
Auth
HTTPS
CORS




Cat Facts
Daily cat facts
No
Yes
No


Cats
Pictures of cats from Tumblr
apiKey
Yes
Unknown


Dogs
Based on the Stanford Dogs Dataset
No
Yes
Yes


HTTPCat
Cat for every HTTP Status
No
Yes
Unknown


IUCN
IUCN Red List of Threatened Species
apiKey
No
Unknown


Movebank
Movement and Migration data of animals
No
Yes
Unknown


Petfinder
Adoption
OAuth
Yes
Yes


PlaceGOAT
Placeholder goat images
No
Yes
Unknown


RandomCat
Random pictures of cats
No
Yes
Yes


RandomDog
Random pictures of dogs
No
Yes
Yes


RandomFox
Random pictures of foxes
No
Yes
No


RescueGroups
Adoption
No
Yes
Unknown


Shibe.Online
Random pictures of Shibu Inu, cats or birds
No
No
No



⬆ Back to Index
Anime



API
Description
Auth
HTTPS
CORS




AniList
Anime discovery & tracking
OAuth
Yes
Unknown


AnimeNewsNetwork
Anime industry news
No
Yes
Yes


Jikan
Unofficial MyAnimeList API
No
Yes
Yes


Kitsu
Anime discovery platform
OAuth
Yes
Unknown


Studio Ghibli
Resources from Studio Ghibli films
No
Yes
Unknown



⬆ Back to Index
Anti-Malware



API
Description
Auth
HTTPS
CORS




AbuseIPDB
IP/domain/URL reputation
apiKey
Yes
Unknown


AlienVault Open Threat Exchange (OTX)
IP/domain/URL reputation
apiKey
Yes
Unknown


Google Safe Browsing
Google Link/Domain Flagging
apiKey
Yes
Unknown


Metacert
Metacert Link Flagging
apiKey
Yes
Unknown


VirusTotal
VirusTotal File/URL Analysis
apiKey
Yes
Unknown


Web Of Trust (WOT)
Website reputation
apiKey
Yes
Unknown



⬆ Back to Index
Art & Design



API
Description
Auth
HTTPS
CORS




Behance
Design
apiKey
Yes
Unknown


Cooper Hewitt
Smithsonian Design Museum
apiKey
Yes
Unknown


Dribbble
Design
OAuth
No
Unknown


Harvard Art Museums
Art
apiKey
No
Unknown


Iconfinder
Icons
apiKey
Yes
Unknown


Icons8
Icons
OAuth
Yes
Unknown


Noun Project
Icons
OAuth
No
Unknown


Rijksmuseum
Art
apiKey
Yes
Unknown



⬆ Back to Index
Books



API
Description
Auth
HTTPS
CORS




Bhagavad Gita
Bhagavad Gita text
OAuth
Yes
Yes


BookNomads
Books published in the Netherlands and Flanders (about 2.5 million), book covers and related data
No
Yes
Unknown


British National Bibliography
Books
No
No
Unknown


Goodreads
Books
apiKey
Yes
Unknown


Google Books
Books
OAuth
Yes
Unknown


LibGen
Library Genesis search engine
No
No
Unknown


Open Library
Books, book covers and related data
No
Yes
Unknown


Penguin Publishing
Books, book covers and related data
No
Yes
Unknown



⬆ Back to Index
Business



API
Description
Auth
HTTPS
CORS




Charity Search
Non-profit charity data
apiKey
No
Unknown


Clearbit Logo
Search for company logos and embed them in your projects
apiKey
Yes
Unknown


Domainsdb.info
Registered Domain Names Search
No
Yes
Unknown


Freelancer
Hire freelancers to get work done
OAuth
Yes
Unknown


Gmail
Flexible, RESTful access to the user's inbox
OAuth
Yes
Unknown


Google Analytics
Collect, configure and analyze your data to reach the right audience
OAuth
Yes
Unknown


MailboxValidator
Validate email address to improve deliverability
apiKey
Yes
Unknown


mailgun
Email Service
apiKey
Yes
Unknown


markerapi
Trademark Search
No
No
Unknown


Ticksel
Friendly website analytics made for humans
No
Yes
Unknown


Trello
Boards, lists and cards to help you organize and prioritize your projects
OAuth
Yes
Unknown



⬆ Back to Index
Calendar



API
Description
Auth
HTTPS
CORS




Calendar Index
Worldwide Holidays and Working Days
apiKey
Yes
Yes


Church Calendar
Catholic liturgical calendar
No
No
Unknown


Czech Namedays Calendar
Lookup for a name and returns nameday date
No
No
Unknown


Google Calendar
Display, create and modify Google calendar events
OAuth
Yes
Unknown


Hebrew Calendar
Convert between Gregorian and Hebrew, fetch Shabbat and Holiday times, etc
No
No
Unknown


Holidays
Historical data regarding holidays
apiKey
Yes
Unknown


LectServe
Protestant liturgical calendar
No
No
Unknown


Nager.Date
Public holidays for more than 90 countries
No
Yes
No


Namedays Calendar
Provides namedays for multiple countries
No
Yes
Yes


Non-Working Days
Database of ICS files for non working days
No
Yes
Unknown


Russian Calendar
Check if a date is a Russian holiday or not
No
Yes
No



⬆ Back to Index
Cloud Storage & File Sharing



API
Description
Auth
HTTPS
CORS




Box
File Sharing and Storage
OAuth
Yes
Unknown


Dropbox
File Sharing and Storage
OAuth
Yes
Unknown


Google Drive
File Sharing and Storage
OAuth
Yes
Unknown


OneDrive
File Sharing and Storage
OAuth
Yes
Unknown


Pastebin
Plain Text Storage
apiKey
Yes
Unknown


Temporal
IPFS based file storage and sharing with optional IPNS naming
apiKey
Yes
No


WeTransfer
File Sharing
apiKey
Yes
Yes



⬆ Back to Index
Continuous Integration



API
Description
Auth
HTTPS
CORS




CircleCI
Automate the software development process using continuous integration and continuous delivery
apiKey
Yes
Unknown


Codeship
Codeship is a Continuous Integration Platform in the cloud
apiKey
Yes
Unknown


Travis CI
Sync your GitHub projects with Travis CI to test your code in minutes
apiKey
Yes
Unknown



⬆ Back to Index
Cryptocurrency



API
Description
Auth
HTTPS
CORS




Binance
Exchange for Trading Cryptocurrencies based in China
apiKey
Yes
Unknown


BitcoinAverage
Digital Asset Price Data for the blockchain industry
apiKey
Yes
Unknown


BitcoinCharts
Financial and Technical Data related to the Bitcoin Network
No
Yes
Unknown


Bitfinex
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


Bitmex
Real-Time Cryptocurrency derivatives trading platform based in Hong Kong
apiKey
Yes
Unknown


Bittrex
Next Generation Crypto Trading Platform
apiKey
Yes
Unknown


Block
Bitcoin Payment, Wallet & Transaction Data
apiKey
Yes
Unknown


Blockchain
Bitcoin Payment, Wallet & Transaction Data
No
Yes
Unknown


CoinAPI
All Currency Exchanges integrate under a single api
apiKey
Yes
No


Coinbase
Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices
apiKey
Yes
Unknown


Coinbase Pro
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


CoinDesk
Bitcoin Price Index
No
No
Unknown


CoinGecko
Cryptocurrency Price, Market, and Developer/Social Data
No
Yes
Yes


Coinigy
Interacting with Coinigy Accounts and Exchange Directly
apiKey
Yes
Unknown


CoinLayer
Real-time Crypto Currency Exchange Rates
apiKey
Yes
Unknown


Coinlib
Crypto Currency Prices
apiKey
Yes
Unknown


Coinlore
Cryptocurrencies prices, volume and more
No
Yes
Unknown


CoinMarketCap
Cryptocurrencies Prices
apiKey
Yes
Unknown


Coinpaprika
Cryptocurrencies prices, volume and more
No
Yes
Yes


CoinRanking
Live Cryptocurrency data
No
Yes
Unknown


CryptoCompare
Cryptocurrencies Comparison
No
Yes
Unknown


Cryptonator
Cryptocurrencies Exchange Rates
No
Yes
Unknown


Gemini
Cryptocurrencies Exchange
No
Yes
Unknown


ICObench
Various information on listing, ratings, stats, and more
apiKey
Yes
Unknown


Livecoin
Cryptocurrency Exchange
No
Yes
Unknown


MercadoBitcoin
Brazilian Cryptocurrency Information
No
Yes
Unknown


Nexchange
Automated cryptocurrency exchange service
No
No
Yes


NiceHash
Largest Crypto Mining Marketplace
apiKey
Yes
Unknown


Poloniex
US based digital asset exchange
apiKey
Yes
Unknown


WorldCoinIndex
Cryptocurrencies Prices
apiKey
Yes
Unknown



⬆ Back to Index
Currency Exchange



API
Description
Auth
HTTPS
CORS




1Forge
Forex currency market data
apiKey
Yes
Unknown


Currencylayer
Exchange rates and currency conversion
apiKey
Yes
Unknown


Czech National Bank
A collection of exchange rates
No
Yes
Unknown


ExchangeRate-API
Free currency conversion
No
Yes
Yes


Exchangeratesapi.io
Exchange rates with currency conversion
No
Yes
Yes


Fixer.io
Exchange rates and currency conversion
apiKey
Yes
Unknown


Frankfurter
Exchange rates, currency conversion and time series
No
Yes
Yes


ratesapi
Free exchange rates and historical rates
No
Yes
Unknown



⬆ Back to Index
Data Validation



API
Description
Auth
HTTPS
CORS




Cloudmersive Validate
Validate email addresses, phone numbers, VAT numbers and domain names
apiKey
Yes
Yes


languagelayer
Language detection
No
Yes
Unknown


Lob.com
US Address Verification
apiKey
Yes
Unknown


mailboxlayer
Email address validation
No
Yes
Unknown


NumValidate
Open Source phone number validation
No
Yes
Unknown


numverify
Phone number validation
No
Yes
Unknown


PurgoMalum
Content validator against profanity & obscenity
No
No
Unknown


US Autocomplete
Enter address data quickly with real-time address suggestions
apiKey
Yes
Yes


US Extract
Extract postal addresses from any text including emails
apiKey
Yes
Yes


US Street Address
Validate and append data for any US postal address
apiKey
Yes
Yes


vatlayer
VAT number validation
No
Yes
Unknown



⬆ Back to Index
Development



API
Description
Auth
HTTPS
CORS




24 Pull Requests
Project to promote open source collaboration during December
No
Yes
Yes


Agify.io
Estimates the age from a first name
No
Yes
Yes


ApiFlash
Chrome based screenshot API for developers
apiKey
Yes
Unknown


Apility.io
IP, Domains and Emails anti-abuse API blocklist
No
Yes
Yes


APIs.guru
Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs
No
Yes
Unknown


BetterMeta
Return a site's meta tags in JSON format
X-Mashape-Key
Yes
Unknown


Bitbucket
Pull public information for a Bitbucket account
No
Yes
Unknown


Bored
Find random activities to fight boredom
No
Yes
Unknown


Browshot
Easily make screenshots of web pages in any screen size, as any device
apiKey
Yes
Unknown


CDNJS
Library info on CDNJS
No
Yes
Unknown


Changelogs.md
Structured changelog metadata from open source projects
No
Yes
Unknown


CountAPI
Free and simple counting service. You can use it to track page hits and specific events
No
Yes
Yes


DigitalOcean Status
Status of all DigitalOcean services
No
Yes
Unknown


DomainDb Info
Domain name search to find all domains containing particular words/phrases/etc
No
Yes
Unknown


Faceplusplus
A tool to detect face
OAuth
Yes
Unknown


Genderize.io
Estimates a gender from a first name
No
Yes
Yes


GitHub
Make use of GitHub repositories, code and user info programmatically
OAuth
Yes
Yes


Gitlab
Automate GitLab interaction programmatically
OAuth
Yes
Unknown


Gitter
Chat for GitHub
OAuth
Yes
Unknown


HTTP2.Pro
Test endpoints for client and server HTTP/2 protocol support
No
Yes
Unknown


IBM Text to Speech
Convert text to speech
apiKey
Yes
Yes


import.io
Retrieve structured data from a website or RSS feed
apiKey
Yes
Unknown


IPify
A simple IP Address API
No
Yes
Unknown


IPinfo
Another simple IP Address API
No
Yes
Unknown


JSON 2 JSONP
Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript
No
Yes
Unknown


JSONbin.io
Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps
apiKey
Yes
Yes


Judge0
Compile and run source code
No
Yes
Unknown


Let's Validate
Uncovers the technologies used on websites and URL to thumbnail
No
Yes
Unknown


License-API
Unofficial REST API for choosealicense.com
No
Yes
No


LiveEdu
Live Coding Streaming
OAuth
Yes
Unknown


MAC address vendor lookup
Retrieve vendor details and other information regarding a given MAC address or an OUI
apiKey
Yes
Yes


Myjson
A simple JSON store for your web or mobile app
No
No
Unknown


Nationalize.io
Estimate the nationality of a first name
No
Yes
Yes


OOPSpam
Multiple spam filtering service
No
Yes
Yes


Plino
Spam filtering system
No
Yes
Unknown


Postman
Tool for testing APIs
apiKey
Yes
Unknown


ProxyCrawl
Scraping and crawling anticaptcha service
apiKey
Yes
Unknown


Public APIs
A collective list of free JSON APIs for use in web development
No
Yes
Unknown


Pusher Beams
Push notifications for Android & iOS
apiKey
Yes
Unknown


QR code
Create an easy to read QR code and URL shortener
No
Yes
Yes


QR code
Generate and decode / read QR code graphics
No
Yes
Unknown


QuickChart
Generate chart and graph images
No
Yes
Yes


ReqRes
A hosted REST-API ready to respond to your AJAX requests
No
Yes
Unknown


Scrape Website Email
Grabs email addresses from a URL
X-Mashape-Key
Yes
Unknown


ScraperApi
Easily build scalable web scrapers
apiKey
Yes
Unknown


ScreenshotAPI.net
Create pixel-perfect website screenshots
apiKey
Yes
Yes


SHOUTCLOUD
ALL-CAPS AS A SERVICE
No
No
Unknown


StackExchange
Q&A forum for developers
OAuth
Yes
Unknown


Verse
Check what's the latest version of your favorite open-source project
No
Yes
Unknown


XML to JSON
Integration developer utility APIs
No
Yes
Unknown



⬆ Back to Index
Dictionaries



API
Description
Auth
HTTPS
CORS




Lingua Robot
Word definitions, pronunciations, synonyms, antonyms and others
apiKey
Yes
Yes


Merriam-Webster
Dictionary and Thesaurus Data
apiKey
Yes
Unknown


OwlBot
Definitions with example sentence and photo if available
apiKey
Yes
Yes


Oxford
Dictionary Data
apiKey
Yes
No


Wordnik
Dictionary Data
apiKey
No
Unknown


Words
Definitions and synonyms for more than 150,000 words
apiKey
Yes
Unknown



⬆ Back to Index
Documents & Productivity



API
Description
Auth
HTTPS
CORS




Cloudmersive Document and Data Conversion
HTML/URL to PDF/PNG, Office documents to PDF, image conversion
apiKey
Yes
Yes


File.io
File Sharing
No
Yes
Unknown


Mercury
Web parser
apiKey
Yes
Unknown


pdflayer
HTML/URL to PDF
apiKey
Yes
Unknown


Pocket
Bookmarking service
OAuth
Yes
Unknown


PrexView
Data from XML or JSON to PDF, HTML or Image
apiKey
Yes
Unknown


Restpack
Provides screenshot, HTML to PDF and content extraction APIs
apiKey
Yes
Unknown


Todoist
Todo Lists
OAuth
Yes
Unknown


Vector Express
Free vector file converting API
No
No
Yes


WakaTime
Automated time tracking leaderboards for programmers
No
Yes
Unknown


Wunderlist
Todo Lists
OAuth
Yes
Unknown



⬆ Back to Index
Environment



API
Description
Auth
HTTPS
CORS




AirVisual
Air quality and weather data
apiKey
Yes
Unknown


GrünstromIndex
Green Power Index for Germany (Grünstromindex/GSI)
No
No
Yes


OpenAQ
Open air quality data
apiKey
Yes
Unknown


PM25.in
Air quality of China
apiKey
No
Unknown


PVWatts
Energy production photovoltaic (PV) energy systems
apiKey
Yes
Unknown


UK Carbon Intensity
The Official Carbon Intensity API for Great Britain developed by National Grid
No
Yes
Unknown



⬆ Back to Index
Events



API
Description
Auth
HTTPS
CORS




Eventbrite
Find events
OAuth
Yes
Unknown


Picatic
Sell tickets anywhere
apiKey
Yes
Unknown


Ticketmaster
Search events, attractions, or venues
apiKey
Yes
Unknown



⬆ Back to Index
Finance



API
Description
Auth
HTTPS
CORS




Alpha Vantage
Realtime and historical stock data
apiKey
Yes
Unknown


Barchart OnDemand
Stock, Futures and Forex Market Data
apiKey
Yes
Unknown


Consumer Financial Protection Bureau
Financial services consumer complaint data
apiKey
Yes
Unknown


Financial Modeling Prep
Stock information and data
No
Yes
Unknown


IEX
Realtime stock data
No
Yes
Yes


IEX Cloud
Realtime & Historical Stock and Market Data
apiKey
Yes
Yes


IG
Spreadbetting and CFD Market Data
apiKey
Yes
Unknown


Plaid
Connect with users’ bank accounts and access transaction data
apiKey
Yes
Unknown


Razorpay IFSC
Indian Financial Systems Code (Bank Branch Codes)
No
Yes
Unknown


RoutingNumbers.info
ACH/NACHA Bank Routing Numbers
No
Yes
Unknown


Tradier
US equity/option market data (delayed, intraday, historical)
OAuth
Yes
Yes


VAT Rates
A collection of all VAT rates for EU countries
No
Yes
Unknown


YNAB
Budgeting & Planning
OAuth
Yes
Yes



⬆ Back to Index
Food & Drink



API
Description
Auth
HTTPS
CORS




Edamam
Recipe Search
apiKey
Yes
Unknown


LCBO
Alcohol
apiKey
Yes
Unknown


Open Brewery DB
Breweries, Cideries and Craft Beer Bottle Shops
No
Yes
Yes


Open Food Facts
Food Products Database
No
Yes
Unknown


PunkAPI
Brewdog Beer Recipes
No
Yes
Unknown


Recipe Puppy
Food
No
No
Unknown


TacoFancy
Community-driven taco database
No
No
Unknown


The Report of the Week
Food & Drink Reviews
No
Yes
Unknown


TheCocktailDB
Cocktail Recipes
apiKey
Yes
Yes


TheMealDB
Meal Recipes
apiKey
Yes
Yes


What's on the menu?
NYPL human-transcribed historical menu collection
apiKey
No
Unknown


Zomato
Discover restaurants
apiKey
Yes
Unknown



⬆ Back to Index
Fraud Prevention



API
Description
Auth
HTTPS
CORS




FraudLabs Pro
Screen order information using AI to detect frauds
apiKey
Yes
Unknown


Whitepages Pro
Global identity verification with phone, address, email and IP
apiKey
Yes
Unknown


Whitepages Pro
Phone reputation to detect spammy phones
apiKey
Yes
Unknown


Whitepages Pro
Get an owner’s name, address, demographics based on the phone number
apiKey
Yes
Unknown


Whitepages Pro
Phone number validation, line_type, carrier append
apiKey
Yes
Unknown


Whitepages Pro
Get normalized physical address, residents, address type and validity
apiKey
Yes
Unknown



⬆ Back to Index
Games & Comics



API
Description
Auth
HTTPS
CORS




Age of Empires II
Get information about Age of Empires II resources
No
Yes
Unknown


AmiiboAPI
Amiibo Information
No
No
Yes


Battle.net
Blizzard Entertainment
apiKey
Yes
Unknown


Chuck Norris Database
Jokes
No
No
Unknown


Clash of Clans
Clash of Clans Game Information
apiKey
Yes
Unknown


Clash Royale
Clash Royale Game Information
apiKey
Yes
Unknown


Comic Vine
Comics
No
Yes
Unknown


Deck of Cards
Deck of Cards
No
No
Unknown


Destiny The Game
Bungie Platform API
apiKey
Yes
Unknown


Dota 2
Provides information about Player stats , Match stats, Rankings for Dota 2
No
Yes
Unknown


Dungeons and Dragons
Reference for 5th edition spells, classes, monsters, and more
No
No
No


Eve Online
Third-Party Developer Documentation
OAuth
Yes
Unknown


Final Fantasy XIV
Final Fantasy XIV Game data API
No
Yes
Yes


Fortnite
Fortnite Stats & Cosmetics
apiKey
Yes
Yes


Fortnite
Fortnite Stats
apiKey
Yes
Unknown


Giant Bomb
Video Games
No
Yes
Unknown


Guild Wars 2
Guild Wars 2 Game Information
apiKey
Yes
Unknown


Halo
Halo 5 and Halo Wars 2 Information
apiKey
Yes
Unknown


Hearthstone
Hearthstone Cards Information
X-Mashape-Key
Yes
Unknown


Hypixel
Hypixel player stats
apiKey
Yes
Unknown


IGDB.com
Video Game Database
apiKey
Yes
Unknown


JokeAPI
Programming, Miscellaneous and Dark Jokes
No
Yes
Yes


Jokes
Programming and general jokes
No
Yes
Unknown


Jservice
Jeopardy Question Database
No
No
Unknown


Magic The Gathering
Magic The Gathering Game Information
No
No
Unknown


Marvel
Marvel Comics
apiKey
No
Unknown


mod.io
Cross Platform Mod API
apiKey
Yes
Unknown


Open Trivia
Trivia Questions
No
Yes
Unknown


PandaScore
E-sports games and results
apiKey
Yes
Unknown


PlayerUnknown's Battlegrounds
PUBG Stats
apiKey
Yes
Unknown


Pokéapi
Pokémon Information
No
Yes
Unknown


Pokémon TCG
Pokémon TCG Information
No
Yes
Unknown


Rick and Morty
All the Rick and Morty information, including images
No
Yes
Yes


Riot Games
League of Legends Game Information
apiKey
Yes
Unknown


Scryfall
Magic: The Gathering database
No
Yes
Yes


Steam
Steam Client Interaction
OAuth
Yes
Unknown


SuperHeroes
All SuperHeroes and Villains data from all universes under a single API
apiKey
Yes
Unknown


Tronald Dump
The dumbest things Donald Trump has ever said
No
Yes
Unknown


Vainglory
Vainglory Players, Matches and Telemetry
apiKey
Yes
Yes


Wargaming.net
Wargaming.net info and stats
apiKey
Yes
No


xkcd
Retrieve xkcd comics as JSON
No
Yes
No



⬆ Back to Index
Geocoding



API
Description
Auth
HTTPS
CORS




adresse.data.gouv.fr
Address database of France, geocoding and reverse
No
Yes
Unknown


Battuta
A (country/region/city) in-cascade location API
apiKey
No
Unknown


Bing Maps
Create/customize digital maps based on Bing Maps data
apiKey
Yes
Unknown


bng2latlong
Convert British OSGB36 easting and northing (British National Grid) to WGS84 latitude and longitude
No
Yes
Yes


CitySDK
Open APIs for select European cities
No
Yes
Unknown


Daum Maps
Daum Maps provide multiple APIs for Korean maps
apiKey
No
Unknown


FreeGeoIP
Free geo ip information, no registration required. 15k/hour rate limit
No
Yes
Yes


GeoApi
French geographical data
No
Yes
Unknown


Geocod.io
Address geocoding / reverse geocoding in bulk
apiKey
Yes
Unknown


Geocode.xyz
Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing
No
Yes
Unknown


GeoDataSource
Geocoding of city name by using latitude and longitude coordinates
apiKey
Yes
Unknown


GeoJS
IP geolocation with ChatOps integration
No
Yes
Yes


GeoNames
Place names and other geographical data
No
No
Unknown


geoPlugin
IP geolocation and currency conversion
No
Yes
Yes


Google Earth Engine
A cloud-based platform for planetary-scale environmental data analysis
apiKey
Yes
Unknown


Google Maps
Create/customize digital maps based on Google Maps data
apiKey
Yes
Unknown


HelloSalut
Get hello translation following user language
No
Yes
Unknown


HERE Maps
Create/customize digital maps based on HERE Maps data
apiKey
Yes
Unknown


Indian Cities
Get all Indian cities in a clean JSON Format
No
Yes
Yes


IP 2 Country
Map an IP to a country
No
Yes
Unknown


IP Address Details
Find geolocation with ip address
No
Yes
Unknown


IP Location
Find location with ip address
No
No
Unknown


IP Location
Find IP address location information
No
Yes
Unknown


IP Sidekick
Geolocation API that returns extra information about an IP address
apiKey
Yes
Unknown


IP Vigilante
Free IP Geolocation API
No
Yes
Unknown


IP2Location
IP geolocation web service to get more than 55 parameters
apiKey
Yes
Unknown


IP2Proxy
Detect proxy and VPN using IP address
apiKey
Yes
Unknown


IPGeolocationAPI.com
Locate your visitors by IP with country details
No
Yes
Yes


IPInfoDB
Free Geolocation tools and APIs for country, region, city and time zone lookup by IP address
apiKey
Yes
Unknown


ipstack
Locate and identify website visitors by IP address
apiKey
Yes
Unknown


Kwelo Network
Locate and get detailed information on IP address
No
Yes
Yes


LocationIQ
Provides forward/reverse geocoding and batch geocoding
apiKey
Yes
Yes


Mapbox
Create/customize beautiful digital maps
apiKey
Yes
Unknown


Mexico
Mexico RESTful zip codes API
No
Yes
Unknown


One Map, Singapore
Singapore Land Authority REST API services for Singapore addresses
apiKey
Yes
Unknown


OnWater
Determine if a lat/lon is on water or land
No
Yes
Unknown


OpenCage
Forward and reverse geocoding using open data
apiKey
Yes
Yes


OpenStreetMap
Navigation, geolocation and geographical data
OAuth
No
Unknown


PostcodeData.nl
Provide geolocation data based on postcode for Dutch addresses
No
No
Unknown


Postcodes.io
Postcode lookup & Geolocation for the UK
No
Yes
Yes


REST Countries
Get information about countries via a RESTful API
No
Yes
Unknown


SmartIP.io
IP Geolocation and Threat Intelligence API
apiKey
Yes
Yes


Uebermaps
Discover and share maps with friends
apiKey
Yes
Unknown


US ZipCode
Validate and append data for any US ZipCode
apiKey
Yes
Yes


Utah AGRC
Utah Web API for geocoding Utah addresses
apiKey
Yes
Unknown


ViaCep
Brazil RESTful zip codes API
No
Yes
Unknown


ZipCodeAPI
US zip code distance, radius and location API
apiKey
Yes
Unknown


Zippopotam
Get information about place such as country, city, state, etc
No
No
Unknown



⬆ Back to Index
Government



API
Description
Auth
HTTPS
CORS




BCLaws
Access to the laws of British Columbia
No
No
Unknown


BusinessUSA
Authoritative information on U.S. programs, events, services and more
apiKey
Yes
Unknown


Census.gov
The US Census Bureau provides various APIs and data sets on demographics and businesses
No
Yes
Unknown


City, Lyon Opendata
Lyon(FR) City Open Data
apiKey
Yes
Unknown


City, Nantes Opendata
Nantes(FR) City Open Data
apiKey
Yes
Unknown


City, Prague Opendata
Prague(CZ) City Open Data
No
No
Unknown


Code.gov
The primary platform for Open Source and code sharing for the U.S. Federal Government
apiKey
Yes
Unknown


Colorado Data Engine
Formatted and geolocated Colorado public data
No
Yes
Unknown


Colorado Information Marketplace
Colorado State Government Open Data
No
Yes
Unknown


Data USA
US Public Data
No
Yes
Unknown


Data.gov
US Government Data
apiKey
Yes
Unknown


Data.parliament.uk
Contains live datasets including information about petitions, bills, MP votes, attendance and more
No
No
Unknown


District of Columbia Open Data
Contains D.C. government public datasets, including crime, GIS, financial data, and so on
No
Yes
Unknown


EPA
Web services and data sets from the US Environmental Protection Agency
No
Yes
Unknown


FEC
Information on campaign donations in federal elections
apiKey
Yes
Unknown


Federal Register
The Daily Journal of the United States Government
No
Yes
Unknown


Food Standards Agency
UK food hygiene rating data API
No
No
Unknown


Open Government, Australia
Australian Government Open Data
No
Yes
Unknown


Open Government, Belgium
Belgium Government Open Data
No
Yes
Unknown


Open Government, Canada
Canadian Government Open Data
No
No
Unknown


Open Government, France
French Government Open Data
apiKey
Yes
Unknown


Open Government, India
Indian Government Open Data
apiKey
Yes
Unknown


Open Government, Italy
Italy Government Open Data
No
Yes
Unknown


Open Government, New Zealand
New Zealand Government Open Data
No
Yes
Unknown


Open Government, Romania
Romania Government Open Data
No
No
Unknown


Open Government, Taiwan
Taiwan Government Open Data
No
Yes
Unknown


Open Government, USA
United States Government Open Data
No
Yes
Unknown


Regulations.gov
Federal regulatory materials to increase understanding of the Federal rule making process
apiKey
Yes
Unknown


Represent by Open North
Find Canadian Government Representatives
No
Yes
Unknown


USAspending.gov
US federal spending data
No
Yes
Unknown



⬆ Back to Index
Health



API
Description
Auth
HTTPS
CORS




BetterDoctor
Detailed information about doctors in your area
apiKey
Yes
Unknown


Diabetes
Logging and retrieving diabetes information
No
No
Unknown


Flutrack
Influenza-like symptoms with geotracking
No
No
Unknown


Healthcare.gov
Educational content about the US Health Insurance Marketplace
No
Yes
Unknown


Lexigram
NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology
apiKey
Yes
Unknown


Makeup
Makeup Information
No
No
Unknown


Medicare
Access to the data from the CMS - medicare.gov
No
Yes
Unknown


NPPES
National Plan & Provider Enumeration System, info on healthcare providers registered in US
No
Yes
Unknown


Nutritionix
Worlds largest verified nutrition database
apiKey
Yes
Unknown


openFDA
Public FDA data about drugs, devices and foods
No
Yes
Unknown


USDA Nutrients
National Nutrient Database for Standard Reference
No
Yes
Unknown



⬆ Back to Index
Jobs



API
Description
Auth
HTTPS
CORS




Adzuna
Job board aggregator
apiKey
Yes
Unknown


Authentic Jobs
Job board for designers, hackers and creative pros
apiKey
Yes
Unknown


Careerjet
Job search engine
apiKey
No
Unknown


Github Jobs
Jobs for software developers
No
Yes
Yes


GraphQL Jobs
Jobs with GraphQL
No
Yes
Yes


Indeed
Job board aggregator
apiKey
Yes
Unknown


Jobs2Careers
Job aggregator
apiKey
Yes
Unknown


Jooble
Job search engine
apiKey
Yes
Unknown


Juju
Job search engine
apiKey
No
Unknown


Open Skills
Job titles, skills and related jobs data
No
No
Unknown


Reed
Job board aggregator
apiKey
Yes
Unknown


Search.gov Jobs
Tap into a list of current jobs openings with the United States government
No
Yes
Unknown


The Muse
Job board and company profiles
apiKey
Yes
Unknown


Upwork
Freelance job board and management system
OAuth
Yes
Unknown


USAJOBS
US government job board
apiKey
Yes
Unknown


ZipRecruiter
Job search app and website
apiKey
Yes
Unknown



⬆ Back to Index
Machine Learning



API
Description
Auth
HTTPS
CORS




Clarifai
Computer Vision
OAuth
Yes
Unknown


Cloudmersive
Image captioning, face recognition, NSFW classification
apiKey
Yes
Yes


Deepcode
AI for code review
No
Yes
Unknown


Dialogflow
Natural Language Processing
apiKey
Yes
Unknown


Keen IO
Data Analytics
apiKey
Yes
Unknown


Time Door
A time series analysis API
apiKey
Yes
Yes


Unplugg
Forecasting API for timeseries data
apiKey
Yes
Unknown


Wit.ai
Natural Language Processing
OAuth
Yes
Unknown



⬆ Back to Index
Music



API
Description
Auth
HTTPS
CORS




AI Mastering
Automated Music Mastering
apiKey
Yes
Yes


Bandsintown
Music Events
No
Yes
Unknown


Deezer
Music
OAuth
Yes
Unknown


Discogs
Music
OAuth
Yes
Unknown


Genius
Crowdsourced lyrics and music knowledge
OAuth
Yes
Unknown


Genrenator
Music genre generator
No
Yes
Unknown


iTunes Search
Software products
No
Yes
Unknown


Jamendo
Music
OAuth
Yes
Unknown


KKBOX
Get music libraries, playlists, charts, and perform out of KKBOX's platform
OAuth
Yes
Unknown


LastFm
Music
apiKey
Yes
Unknown


Lyrics.ovh
Simple API to retrieve the lyrics of a song
No
Yes
Unknown


Mixcloud
Music
OAuth
Yes
Yes


MusicBrainz
Music
No
Yes
Unknown


Musikki
Music
apiKey
Yes
Unknown


Musixmatch
Music
apiKey
Yes
Unknown


Openwhyd
Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...)
No
Yes
No


Songkick
Music Events
OAuth
Yes
Unknown


Songsterr
Provides guitar, bass and drums tabs and chords
No
Yes
Unknown


SoundCloud
Allow users to upload and share sounds
OAuth
Yes
Unknown


Spotify
View Spotify music catalog, manage users' libraries, get recommendations and more
OAuth
Yes
Unknown


TasteDive
Similar artist API (also works for movies and TV shows)
apiKey
Yes
Unknown


TheAudioDB
Music
apiKey
Yes
Unknown


Vagalume
Crowdsourced lyrics and music knowledge
apiKey
Yes
Unknown



⬆ Back to Index
News



API
Description
Auth
HTTPS
CORS




Associated Press
Search for news and metadata from Associated Press
apiKey
Yes
Unknown


Chronicling America
Provides access to millions of pages of historic US newspapers from the Library of Congress
No
No
Unknown


Currents
Latest news published in various news sources, blogs and forums
apiKey
Yes
Yes


Feedbin
RSS reader
OAuth
Yes
Unknown


Feedster
Searchable and categorized collections of RSS feeds
apiKey
Yes
Unknown


New York Times
Provides news
apiKey
Yes
Unknown


News
Headlines currently published on a range of news sources and blogs
apiKey
Yes
Unknown


NPR One
Personalized news listening experience from NPR
OAuth
Yes
Unknown


The Guardian
Access all the content the Guardian creates, categorised by tags and section
apiKey
Yes
Unknown


The Old Reader
RSS reader
apiKey
Yes
Unknown



⬆ Back to Index
Open Data



API
Description
Auth
HTTPS
CORS




18F
Unofficial US Federal Government API Development
No
No
Unknown


Abbreviation
Get abbreviations and meanings
X-Mashape-Key
Yes
Unknown


Archive.org
The Internet Archive
No
Yes
Unknown


ARSAT
ARSAT public data
apiKey
Yes
Unknown


Callook.info
United States ham radio callsigns
No
Yes
Unknown


CARTO
Location Information Prediction
apiKey
Yes
Unknown


Celebinfo
Celebrity information
X-Mashape-Key
Yes
Unknown


CivicFeed
News articles and public datasets
apiKey
Yes
Unknown


Datakick
The open product database
apiKey
Yes
Unknown


Enigma Public
Broadest collection of public data
apiKey
Yes
Yes


fonoApi
Mobile Device Description
No
Yes
Unknown


French Address Search
Address search via the French Government
No
Yes
Unknown


LinkPreview
Get JSON formatted summary with title, description and preview image for any requested URL
apiKey
Yes
Yes


Marijuana Strains
Marijuana strains, races, flavors and effects
apiKey
No
Unknown


Microlink.io
Extract structured data from any website
No
Yes
Yes


OpenCorporates
Data on corporate entities and directors in many countries
apiKey
Yes
Unknown


Qmeta
Global Search Engine
apiKey
Yes
Unknown


Quandl
Stock Market Data
No
Yes
Unknown


Recreation Information Database
Recreational areas, federal lands, historic sites, museums, and other attractions/resources(US)
apiKey
Yes
Unknown


Scoop.it
Content Curation Service
apiKey
No
Unknown


Teleport
Quality of Life Data
No
Yes
Unknown


Universities List
University names, countries and domains
No
Yes
Unknown


University of Oslo
Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway)
No
Yes
Unknown


UPC database
More than 1.5 million barcode numbers from all around the world
apiKey
Yes
Unknown


Wikidata
Collaboratively edited knowledge base operated by the Wikimedia Foundation
OAuth
Yes
Unknown


Wikipedia
Mediawiki Encyclopedia
No
Yes
Unknown


Yelp
Find Local Business
OAuth
Yes
Unknown



⬆ Back to Index
Open Source Projects



API
Description
Auth
HTTPS
CORS




Countly
Countly web analytics
No
No
Unknown


Drupal.org
Drupal.org
No
Yes
Unknown


Evil Insult Generator
Evil Insults
No
Yes
Yes


Libraries.io
Open source software libraries
apiKey
Yes
Unknown



⬆ Back to Index
Patent



API
Description
Auth
HTTPS
CORS




EPO
European patent search system api
OAuth
Yes
Unknown


TIPO
Taiwan patent search system api
apiKey
Yes
Unknown


USPTO
USA patent api services
No
Yes
Unknown



⬆ Back to Index
Personality



API
Description
Auth
HTTPS
CORS




Advice Slip
Generate random advice slips
No
Yes
Unknown


chucknorris.io
JSON API for hand curated Chuck Norris jokes
No
Yes
Unknown


FavQs.com
FavQs allows you to collect, discover and share your favorite quotes
apiKey
Yes
Unknown


FOAAS
Fuck Off As A Service
No
No
Unknown


Forismatic
Inspirational Quotes
No
No
Unknown


icanhazdadjoke
The largest selection of dad jokes on the internet
No
Yes
Unknown


kanye.rest
REST API for random Kanye West quotes
No
Yes
Yes


Medium
Community of readers and writers offering unique perspectives on ideas
OAuth
Yes
Unknown


NaMoMemes
Memes on Narendra Modi
No
Yes
Unknown


Programming Quotes
Programming Quotes API for open source projects
No
Yes
Unknown


Quote Garden
REST API for more than 5000 famous quotes
No
Yes
Unknown


Quotes on Design
Inspirational Quotes
No
Yes
Unknown


Traitify
Assess, collect and analyze Personality
No
Yes
Unknown


tronalddump.io
Api & web archive for the things Donald Trump has said
No
Yes
Unknown



⬆ Back to Index
Photography



API
Description
Auth
HTTPS
CORS




Flickr
Flickr Services
OAuth
Yes
Unknown


Getty Images
Build applications using the world's most powerful imagery
OAuth
Yes
Unknown


Gfycat
Jiffier GIFs
OAuth
Yes
Unknown


Giphy
Get all your gifs
apiKey
Yes
Unknown


Gyazo
Upload images
apiKey
Yes
Unknown


Imgur
Images
OAuth
Yes
Unknown


Lorem Picsum
Images from Unsplash
No
Yes
Unknown


Pexels
Free Stock Photos and Videos
apiKey
Yes
Yes


Pixabay
Photography
apiKey
Yes
Unknown


Pixhost
Upload images, photos, galleries
No
Yes
Unknown


PlaceKitten
Resizable kitten placeholder images
No
Yes
Unknown


ScreenShotLayer
URL 2 Image
No
Yes
Unknown


Unsplash
Photography
OAuth
Yes
Unknown


Wallhaven
Wallpapers
apiKey
Yes
Unknown



⬆ Back to Index
Science & Math



API
Description
Auth
HTTPS
CORS




arcsecond.io
Multiple astronomy data sources
No
Yes
Unknown


CORE
Access the world's Open Access research papers
apiKey
Yes
Unknown


GBIF
Global Biodiversity Information Facility
No
Yes
Yes


iDigBio
Access millions of museum specimens from organizations around the world
No
Yes
Unknown


inspirehep.net
High Energy Physics info. system
No
Yes
Unknown


ITIS
Integrated Taxonomic Information System
No
Yes
Unknown


Launch Library
Upcoming Space Launches
No
Yes
Unknown


Minor Planet Center
Asterank.com Information
No
No
Unknown


NASA
NASA data, including imagery
No
Yes
Unknown


NASA APOD (unofficial API)
API for getting APOD (Astronomy Image of the Day) images along with metadata
No
Yes
Yes


Newton
Symbolic and Arithmetic Math Calculator
No
Yes
Unknown


Numbers
Facts about numbers
No
No
Unknown


Open Notify
ISS astronauts, current location, etc
No
No
Unknown


Open Science Framework
Repository and archive for study designs, research materials, data, manuscripts, etc
No
Yes
Unknown


SHARE
A free, open, dataset about research and scholarly activities
No
Yes
Unknown


SpaceX
Company, vehicle, launchpad and launch data
No
Yes
Unknown


Sunrise and Sunset
Sunset and sunrise times for a given latitude and longitude
No
Yes
Unknown


Trefle
Botanical data for plant species
apiKey
Yes
Unknown


USGS Earthquake Hazards Program
Earthquakes data real-time
No
Yes
Unknown


USGS Water Services
Water quality and level info for rivers and lakes
No
Yes
Unknown


World Bank
World Data
No
No
Unknown



⬆ Back to Index
Security



API
Description
Auth
HTTPS
CORS




Censys.io
Search engine for Internet connected host and devices
apiKey
Yes
No


CRXcavator
Chrome extension risk scoring
apiKey
Yes
Unknown


FilterLists
Lists of filters for adblockers and firewalls
No
Yes
Unknown


HaveIBeenPwned
Passwords which have previously been exposed in data breaches
apiKey
Yes
Unknown


National Vulnerability Database
U.S. National Vulnerability Database
No
Yes
Unknown


SecurityTrails
Domain and IP related information such as current and historical WHOIS and DNS records
apiKey
Yes
Unknown


Shodan
Search engine for Internet connected devices
apiKey
Yes
Unknown


UK Police
UK Police data
No
Yes
Unknown



⬆ Back to Index
Shopping



API
Description
Auth
HTTPS
CORS




Best Buy
Products, Buying Options, Categories, Recommendations, Stores and Commerce
apiKey
Yes
Unknown


Bratabase
Database of different types of Bra Sizes
OAuth
Yes
Unknown


eBay
Sell and Buy on eBay
OAuth
Yes
Unknown


Wal-Mart
Item price and availability
apiKey
Yes
Unknown


Wegmans
Wegmans Food Markets
apiKey
Yes
Unknown



⬆ Back to Index
Social



API
Description
Auth
HTTPS
CORS




Buffer
Access to pending and sent updates in Buffer
OAuth
Yes
Unknown


Cisco Spark
Team Collaboration Software
OAuth
Yes
Unknown


Discord
Make bots for Discord, integrate Discord onto an external platform
OAuth
Yes
Unknown


Disqus
Communicate with Disqus data
OAuth
Yes
Unknown


Facebook
Facebook Login, Share on FB, Social Plugins, Analytics and more
OAuth
Yes
Unknown


Foursquare
Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc)
OAuth
Yes
Unknown


Fuck Off as a Service
Asks someone to fuck off
No
Yes
Unknown


Full Contact
Get Social Media profiles and contact Information
OAuth
Yes
Unknown


HackerNews
Social news for CS and entrepreneurship
No
Yes
Unknown


Instagram
Instagram Login, Share on Instagram, Social Plugins and more
OAuth
Yes
Unknown


LinkedIn
The foundation of all digital integrations with LinkedIn
OAuth
Yes
Unknown


Meetup.com
Data about Meetups from Meetup.com
apiKey
Yes
Unknown


Mixer
Game Streaming API
OAuth
Yes
Unknown


MySocialApp
Seamless Social Networking features, API, SDK to any app
apiKey
Yes
Unknown


Open Collective
Get Open Collective data
No
Yes
Unknown


Pinterest
The world's catalog of ideas
OAuth
Yes
Unknown


PWRTelegram bot
Boosted version of the Telegram bot API
OAuth
Yes
Unknown


Reddit
Homepage of the internet
OAuth
Yes
Unknown


SharedCount
Social media like and share data for any URL
apiKey
Yes
Unknown


Slack
Team Instant Messaging
OAuth
Yes
Unknown


Telegram Bot
Simplified HTTP version of the MTProto API for bots
OAuth
Yes
Unknown


Telegram MTProto
Read and write Telegram data
OAuth
Yes
Unknown


Trash Nothing
A freecycling community with thousands of free items posted every day
OAuth
Yes
Yes


Tumblr
Read and write Tumblr Data
OAuth
Yes
Unknown


Twitch
Game Streaming API
OAuth
Yes
Unknown


Twitter
Read and write Twitter data
OAuth
Yes
No


vk
Read and write vk data
OAuth
Yes
Unknown



⬆ Back to Index
Sports & Fitness



API
Description
Auth
HTTPS
CORS




balldontlie
Ballldontlie provides access to stats data from the NBA
No
Yes
Yes


BikeWise
Bikewise is a place to learn about and report bike crashes, hazards and thefts
No
Yes
Unknown


Canadian Football League (CFL)
Official JSON API providing real-time league, team and player statistics about the CFL
apiKey
Yes
No


Cartola FC
The Cartola FC API serves to check the partial points of your team
No
Yes
Unknown


City Bikes
City Bikes around the world
No
No
Unknown


Cricket Live Scores
Live cricket scores
X-Mashape-Key
Yes
Unknown


Ergast F1
F1 data from the beginning of the world championships in 1950
No
Yes
Unknown


Fitbit
Fitbit Information
OAuth
Yes
Unknown


Football (Soccer) Videos
Embed codes for goals and highlights from Premier League, Bundesliga, Serie A and many more
No
Yes
Yes


Football Prediction
Predictions for upcoming football matches, odds, results and stats
X-Mashape-Key
Yes
Unknown


Football-Data.org
Football Data
No
No
Unknown


JCDecaux Bike
JCDecaux's self-service bicycles
apiKey
Yes
Unknown


NBA Stats
Current and historical NBA Statistics
No
Yes
Unknown


NFL Arrests
NFL Arrest Data
No
No
Unknown


NHL Records and Stats
NHL historical data and statistics
No
Yes
Unknown


Pro Motocross
The RESTful AMA Pro Motocross lap times for every racer on the start gate
No
No
Unknown


Strava
Connect with athletes, activities and more
OAuth
Yes
Unknown


SuredBits
Query sports data, including teams, players, games, scores and statistics
No
No
No


TheSportsDB
Crowd-Sourced Sports Data and Artwork
apiKey
Yes
Yes


Wger
Workout manager data as exercises, muscles or equipment
apiKey
Yes
Unknown



⬆ Back to Index
Test Data



API
Description
Auth
HTTPS
CORS




Adorable Avatars
Generate random cartoon avatars
No
Yes
Unknown


Bacon Ipsum
A Meatier Lorem Ipsum Generator
No
Yes
Unknown


Dicebear Avatars
Generate random pixel-art avatars
No
Yes
No


FakeJSON
Service to generate test and fake data
apiKey
Yes
Yes


FHIR
Fast Healthcare Interoperability Resources test data
No
Yes
Unknown


Hipster Ipsum
Generates Hipster Ipsum text
No
No
Unknown


Identicon
Generates abstract avatar image
No
Yes
Yes


JSONPlaceholder
Fake data for testing and prototyping
No
No
Unknown


Lorem Text
Generates Lorem Ipsum text
X-Mashape-Key
Yes
Unknown


LoremPicsum
Generate placeholder pictures
No
No
Unknown


Loripsum
The ""lorem ipsum"" generator that doesn't suck
No
No
Unknown


RandomUser
Generates random user data
No
Yes
Unknown


RoboHash
Generate random robot/alien avatars
No
Yes
Unknown


This Person Does not Exist
Generates real-life faces of people who do not exist
No
Yes
Unknown


UI Names
Generate random fake names
No
Yes
Unknown


Yes No
Generate yes or no randomly
No
Yes
Unknown



⬆ Back to Index
Text Analysis



API
Description
Auth
HTTPS
CORS




Aylien Text Analysis
A collection of information retrieval and natural language APIs
apiKey
Yes
Unknown


Cloudmersive Natural Language Processing
Natural language processing and text analysis
apiKey
Yes
Yes


Detect Language
Detects text language
apiKey
Yes
Unknown


Google Cloud Natural
Natural language understanding technology, including sentiment, entity and syntax analysis
apiKey
Yes
Unknown


Language Identification
Automatic language detection for any texts, supports over 175 languages
X-Mashape-Key
Yes
Unknown


Semantira
Text Analytics with sentiment analysis, categorization & named entity extraction
OAuth
Yes
Unknown


Watson Natural Language Understanding
Natural language processing for advanced text analysis
OAuth
Yes
Unknown



⬆ Back to Index
Tracking



API
Description
Auth
HTTPS
CORS




Postmon
An API to query Brazilian ZIP codes and orders easily, quickly and free
No
No
Unknown


Sweden
Provides information about parcels in transport
apiKey
No
Unknown


UPS
Shipment and Address information
apiKey
Yes
Unknown


WhatPulse
Small application that measures your keyboard/mouse usage
No
Yes
Unknown



⬆ Back to Index
Transportation



API
Description
Auth
HTTPS
CORS




ADS-B Exchange
Access real-time and historical data of any and all airborne aircraft
No
Yes
Unknown


AIS Hub
Real-time data of any marine and inland vessel equipped with AIS tracking system
apiKey
No
Unknown


AIS Web
Aeronautical information in digital media produced by the Department of Airspace Control (DECEA)
apiKey
No
Unknown


Amadeus Travel Innovation Sandbox
Travel Search - Limited usage
apiKey
Yes
Unknown


Bay Area Rapid Transit
Stations and predicted arrivals for BART
apiKey
No
Unknown


BlaBlaCar
Search car sharing trips
apiKey
Yes
Unknown


Community Transit
Transitland API
No
Yes
Unknown


Goibibo
API for travel search
apiKey
Yes
Unknown


GraphHopper
A-to-B routing with turn-by-turn instructions
apiKey
Yes
Unknown


Icelandic APIs
Open APIs that deliver services in or regarding Iceland
No
Yes
Unknown


Indian Railways
Indian Railways Information
apiKey
No
Unknown


Izi
Audio guide for travellers
apiKey
Yes
Unknown


Metro Lisboa
Delays in subway lines
No
No
No


Navitia
The open API for building cool stuff with transport data
apiKey
Yes
Unknown


REFUGE Restrooms
Provides safe restroom access for transgender, intersex and gender nonconforming individuals
No
Yes
Unknown


Schiphol Airport
Schiphol
apiKey
Yes
Unknown


TransitLand
Transit Aggregation
No
Yes
Unknown


Transport for Atlanta, US
Marta
No
No
Unknown


Transport for Auckland, New Zealand
Auckland Transport
No
Yes
Unknown


Transport for Belgium
Belgian transport API
No
Yes
Unknown


Transport for Berlin, Germany
Third-party VBB API
No
Yes
Unknown


Transport for Bordeaux, France
Bordeaux Métropole public transport and more (France)
apiKey
Yes
Unknown


Transport for Boston, US
MBTA API
No
No
Unknown


Transport for Budapest, Hungary
Budapest public transport API
No
Yes
Unknown


Transport for Chicago, US
CTA
No
No
Unknown


Transport for Czech Republic
Czech transport API
No
Yes
Unknown


Transport for Denver, US
RTD
No
No
Unknown


Transport for Finland
Finnish transport API
No
Yes
Unknown


Transport for Germany
Deutsche Bahn (DB) API
apiKey
No
Unknown


Transport for Grenoble, France
Grenoble public transport
No
No
No


Transport for Honolulu, US
Honolulu Transportation Information
apiKey
No
Unknown


Transport for India
India Public Transport API
apiKey
Yes
Unknown


Transport for Lisbon, Portugal
Data about buses routes, parking and traffic
apiKey
Yes
Unknown


Transport for London, England
TfL API
No
Yes
Unknown


Transport for Madrid, Spain
Madrid BUS transport API
apiKey
No
Unknown


Transport for Manchester, England
TfGM transport network data
apiKey
Yes
No


Transport for Minneapolis, US
NexTrip API
OAuth
No
Unknown


Transport for New York City, US
MTA
apiKey
No
Unknown


Transport for Norway
Norwegian transport API
No
No
Unknown


Transport for Ottawa, Canada
OC Transpo next bus arrival API
No
No
Unknown


Transport for Paris, France
Live schedules made simple
No
No
Unknown


Transport for Paris, France
RATP Open Data API
No
No
Unknown


Transport for Philadelphia, US
SEPTA APIs
No
No
Unknown


Transport for Sao Paulo, Brazil
SPTrans
OAuth
No
Unknown


Transport for Sweden
Public Transport consumer
OAuth
Yes
Unknown


Transport for Switzerland
Official Swiss Public Transport Open Data
apiKey
Yes
Unknown


Transport for Switzerland
Swiss public transport API
No
Yes
Unknown


Transport for The Netherlands
NS, only trains
apiKey
No
Unknown


Transport for The Netherlands
OVAPI, country-wide public transport
No
Yes
Unknown


Transport for Toronto, Canada
TTC
No
Yes
Unknown


Transport for United States
NextBus API
No
No
Unknown


Transport for Vancouver, Canada
TransLink
OAuth
Yes
Unknown


Transport for Victoria, AU
PTV API
apiKey
Yes
Unknown


Transport for Washington, US
Washington Metro transport API
OAuth
Yes
Unknown


Uber
Uber ride requests and price estimation
OAuth
Yes
Yes


WhereIsMyTransport
Platform for public transport data in emerging cities
OAuth
Yes
Unknown



⬆ Back to Index
URL Shorteners



API
Description
Auth
HTTPS
CORS




Bitly
URL shortener and link management
OAuth
Yes
Unknown


CleanURI
URL shortener service
No
Yes
Yes


ClickMeter
Monitor, compare and optimize your marketing links
apiKey
Yes
Unknown


Rebrandly
Custom URL shortener for sharing branded links
apiKey
Yes
Unknown


Relink
Free and secure URL shortener
No
Yes
Yes



⬆ Back to Index
Vehicle



API
Description
Auth
HTTPS
CORS




Brazilian Vehicles and Prices
Vehicles information from Fundação Instituto de Pesquisas Econômicas - Fipe
No
Yes
Unknown


Kelley Blue Book
Vehicle info, pricing, configuration, plus much more
apiKey
Yes
No


Mercedes-Benz
Telematics data, remotely access vehicle functions, car configurator, locate service dealers
apiKey
Yes
No


NHTSA
NHTSA Product Information Catalog and Vehicle Listing
No
Yes
Unknown


Smartcar
Lock and unlock vehicles and get data like odometer reading and location. Works on most new cars
OAuth
Yes
Yes



⬆ Back to Index
Video



API
Description
Auth
HTTPS
CORS




An API of Ice And Fire
Game Of Thrones API
No
Yes
Unknown


Breaking Bad
Breaking Bad API
No
Yes
Unknown


Breaking Bad Quotes
Some Breaking Bad quotes
No
Yes
Unknown


Czech Television
TV programme of Czech TV
No
No
Unknown


Dailymotion
Dailymotion Developer API
OAuth
Yes
Unknown


Harry Potter
Harry Potter API
apiKey
Yes
Yes


Open Movie Database
Movie information
apiKey
Yes
Unknown


Ron Swanson Quotes
Television
No
Yes
Unknown


STAPI
Information on all things Star Trek
No
No
No


SWAPI
Star Wars Information
No
Yes
Unknown


The Lord of the Rings
The Lord of the Rings API
apiKey
Yes
Unknown


TMDb
Community-based movie data
apiKey
Yes
Unknown


Trakt
Movie and TV Data
apiKey
Yes
Yes


TVDB
Television data
apiKey
Yes
Unknown


TVMaze
TV Show Data
No
No
Unknown


Utelly
Check where a tv show or movie is available
X-Mashape-Key
Yes
Unknown


Vimeo
Vimeo Developer API
OAuth
Yes
Unknown


YouTube
Add YouTube functionality to your sites and apps
OAuth
Yes
Unknown



⬆ Back to Index
Weather



API
Description
Auth
HTTPS
CORS




7Timer!
Weather, especially for Astroweather
No
No
Unknown


APIXU
Weather
apiKey
Yes
Unknown


Dark Sky
Weather
apiKey
Yes
No


MetaWeather
Weather
No
Yes
No


Meteorologisk Institutt
Weather and climate data
No
Yes
Unknown


NOAA Climate Data
Weather and climate data
apiKey
Yes
Unknown


ODWeather
Weather and weather webcams
No
No
Unknown


OpenUV
Real-time UV Index Forecast
apiKey
Yes
Unknown


OpenWeatherMap
Weather
apiKey
No
Unknown


Storm Glass
Global marine weather from multiple sources
apiKey
Yes
Yes


Weatherbit
Weather
apiKey
Yes
Unknown


Yahoo! Weather
Weather
apiKey
Yes
Unknown



⬆ Back to Index
"
33,Python,"The Algorithms - Python
 
 
 
 
 
 
All algorithms implemented in Python (for education)
These implementations are for learning purposes. They may be less efficient than the implementations in the Python standard library.
Contribution Guidelines
Read our Contribution Guidelines before you contribute.
Community Channel
We're on Gitter! Please join us.
List of Algorithms
See our directory.

"
34,Python,"TensorFlow Models
This repository contains a number of different models implemented in TensorFlow:
The official models are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.
The research models are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.
The samples folder contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.
The tutorials folder is a collection of models described in the TensorFlow tutorials.
Contribution guidelines
If you want to contribute to models, be sure to review the contribution guidelines.
License
Apache License 2.0
"
35,Python,"
youtube-dl - download videos from youtube.com or other video platforms

INSTALLATION
DESCRIPTION
OPTIONS
CONFIGURATION
OUTPUT TEMPLATE
FORMAT SELECTION
VIDEO SELECTION
FAQ
DEVELOPER INSTRUCTIONS
EMBEDDING YOUTUBE-DL
BUGS
COPYRIGHT

INSTALLATION
To install it right away for all UNIX users (Linux, macOS, etc.), type:
sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:
sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl

Windows users can download an .exe file and place it in any location on their PATH except for %SYSTEMROOT%\System32 (e.g. do not put in C:\Windows\System32).
You can also use pip:
sudo -H pip install --upgrade youtube-dl

This command will update youtube-dl if you have already installed it. See the pypi page for more information.
macOS users can install youtube-dl with Homebrew:
brew install youtube-dl

Or with MacPorts:
sudo port install youtube-dl

Alternatively, refer to the developer instructions for how to check out and work with the git repository. For further options, including PGP signatures, see the youtube-dl Download Page.
DESCRIPTION
youtube-dl is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.
youtube-dl [OPTIONS] URL [URL...]

OPTIONS
-h, --help                       Print this help text and exit
--version                        Print program version and exit
-U, --update                     Update this program to latest version. Make
                                 sure that you have sufficient permissions
                                 (run with sudo if needed)
-i, --ignore-errors              Continue on download errors, for example to
                                 skip unavailable videos in a playlist
--abort-on-error                 Abort downloading of further videos (in the
                                 playlist or the command line) if an error
                                 occurs
--dump-user-agent                Display the current browser identification
--list-extractors                List all supported extractors
--extractor-descriptions         Output descriptions of all supported
                                 extractors
--force-generic-extractor        Force extraction to use the generic
                                 extractor
--default-search PREFIX          Use this prefix for unqualified URLs. For
                                 example ""gvsearch2:"" downloads two videos
                                 from google videos for youtube-dl ""large
                                 apple"". Use the value ""auto"" to let
                                 youtube-dl guess (""auto_warning"" to emit a
                                 warning when guessing). ""error"" just throws
                                 an error. The default value ""fixup_error""
                                 repairs broken URLs, but emits an error if
                                 this is not possible instead of searching.
--ignore-config                  Do not read configuration files. When given
                                 in the global configuration file
                                 /etc/youtube-dl.conf: Do not read the user
                                 configuration in ~/.config/youtube-
                                 dl/config (%APPDATA%/youtube-dl/config.txt
                                 on Windows)
--config-location PATH           Location of the configuration file; either
                                 the path to the config or its containing
                                 directory.
--flat-playlist                  Do not extract the videos of a playlist,
                                 only list them.
--mark-watched                   Mark videos watched (YouTube only)
--no-mark-watched                Do not mark videos watched (YouTube only)
--no-color                       Do not emit color codes in output

Network Options:
--proxy URL                      Use the specified HTTP/HTTPS/SOCKS proxy.
                                 To enable SOCKS proxy, specify a proper
                                 scheme. For example
                                 socks5://127.0.0.1:1080/. Pass in an empty
                                 string (--proxy """") for direct connection
--socket-timeout SECONDS         Time to wait before giving up, in seconds
--source-address IP              Client-side IP address to bind to
-4, --force-ipv4                 Make all connections via IPv4
-6, --force-ipv6                 Make all connections via IPv6

Geo Restriction:
--geo-verification-proxy URL     Use this proxy to verify the IP address for
                                 some geo-restricted sites. The default
                                 proxy specified by --proxy (or none, if the
                                 option is not present) is used for the
                                 actual downloading.
--geo-bypass                     Bypass geographic restriction via faking
                                 X-Forwarded-For HTTP header
--no-geo-bypass                  Do not bypass geographic restriction via
                                 faking X-Forwarded-For HTTP header
--geo-bypass-country CODE        Force bypass geographic restriction with
                                 explicitly provided two-letter ISO 3166-2
                                 country code
--geo-bypass-ip-block IP_BLOCK   Force bypass geographic restriction with
                                 explicitly provided IP block in CIDR
                                 notation

Video Selection:
--playlist-start NUMBER          Playlist video to start at (default is 1)
--playlist-end NUMBER            Playlist video to end at (default is last)
--playlist-items ITEM_SPEC       Playlist video items to download. Specify
                                 indices of the videos in the playlist
                                 separated by commas like: ""--playlist-items
                                 1,2,5,8"" if you want to download videos
                                 indexed 1, 2, 5, 8 in the playlist. You can
                                 specify range: ""--playlist-items
                                 1-3,7,10-13"", it will download the videos
                                 at index 1, 2, 3, 7, 10, 11, 12 and 13.
--match-title REGEX              Download only matching titles (regex or
                                 caseless sub-string)
--reject-title REGEX             Skip download for matching titles (regex or
                                 caseless sub-string)
--max-downloads NUMBER           Abort after downloading NUMBER files
--min-filesize SIZE              Do not download any videos smaller than
                                 SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE              Do not download any videos larger than SIZE
                                 (e.g. 50k or 44.6m)
--date DATE                      Download only videos uploaded in this date
--datebefore DATE                Download only videos uploaded on or before
                                 this date (i.e. inclusive)
--dateafter DATE                 Download only videos uploaded on or after
                                 this date (i.e. inclusive)
--min-views COUNT                Do not download any videos with less than
                                 COUNT views
--max-views COUNT                Do not download any videos with more than
                                 COUNT views
--match-filter FILTER            Generic video filter. Specify any key (see
                                 the ""OUTPUT TEMPLATE"" for a list of
                                 available keys) to match if the key is
                                 present, !key to check if the key is not
                                 present, key > NUMBER (like ""comment_count
                                 > 12"", also works with >=, <, <=, !=, =) to
                                 compare against a number, key = 'LITERAL'
                                 (like ""uploader = 'Mike Smith'"", also works
                                 with !=) to match against a string literal
                                 and & to require multiple matches. Values
                                 which are not known are excluded unless you
                                 put a question mark (?) after the operator.
                                 For example, to only match videos that have
                                 been liked more than 100 times and disliked
                                 less than 50 times (or the dislike
                                 functionality is not available at the given
                                 service), but who also have a description,
                                 use --match-filter ""like_count > 100 &
                                 dislike_count <? 50 & description"" .
--no-playlist                    Download only the video, if the URL refers
                                 to a video and a playlist.
--yes-playlist                   Download the playlist, if the URL refers to
                                 a video and a playlist.
--age-limit YEARS                Download only videos suitable for the given
                                 age
--download-archive FILE          Download only videos not listed in the
                                 archive file. Record the IDs of all
                                 downloaded videos in it.
--include-ads                    Download advertisements as well
                                 (experimental)

Download Options:
-r, --limit-rate RATE            Maximum download rate in bytes per second
                                 (e.g. 50K or 4.2M)
-R, --retries RETRIES            Number of retries (default is 10), or
                                 ""infinite"".
--fragment-retries RETRIES       Number of retries for a fragment (default
                                 is 10), or ""infinite"" (DASH, hlsnative and
                                 ISM)
--skip-unavailable-fragments     Skip unavailable fragments (DASH, hlsnative
                                 and ISM)
--abort-on-unavailable-fragment  Abort downloading when some fragment is not
                                 available
--keep-fragments                 Keep downloaded fragments on disk after
                                 downloading is finished; fragments are
                                 erased by default
--buffer-size SIZE               Size of download buffer (e.g. 1024 or 16K)
                                 (default is 1024)
--no-resize-buffer               Do not automatically adjust the buffer
                                 size. By default, the buffer size is
                                 automatically resized from an initial value
                                 of SIZE.
--http-chunk-size SIZE           Size of a chunk for chunk-based HTTP
                                 downloading (e.g. 10485760 or 10M) (default
                                 is disabled). May be useful for bypassing
                                 bandwidth throttling imposed by a webserver
                                 (experimental)
--playlist-reverse               Download playlist videos in reverse order
--playlist-random                Download playlist videos in random order
--xattr-set-filesize             Set file xattribute ytdl.filesize with
                                 expected file size
--hls-prefer-native              Use the native HLS downloader instead of
                                 ffmpeg
--hls-prefer-ffmpeg              Use ffmpeg instead of the native HLS
                                 downloader
--hls-use-mpegts                 Use the mpegts container for HLS videos,
                                 allowing to play the video while
                                 downloading (some players may not be able
                                 to play it)
--external-downloader COMMAND    Use the specified external downloader.
                                 Currently supports
                                 aria2c,avconv,axel,curl,ffmpeg,httpie,wget
--external-downloader-args ARGS  Give these arguments to the external
                                 downloader

Filesystem Options:
-a, --batch-file FILE            File containing URLs to download ('-' for
                                 stdin), one URL per line. Lines starting
                                 with '#', ';' or ']' are considered as
                                 comments and ignored.
--id                             Use only video ID in file name
-o, --output TEMPLATE            Output filename template, see the ""OUTPUT
                                 TEMPLATE"" for all the info
--autonumber-start NUMBER        Specify the start value for %(autonumber)s
                                 (default is 1)
--restrict-filenames             Restrict filenames to only ASCII
                                 characters, and avoid ""&"" and spaces in
                                 filenames
-w, --no-overwrites              Do not overwrite files
-c, --continue                   Force resume of partially downloaded files.
                                 By default, youtube-dl will resume
                                 downloads if possible.
--no-continue                    Do not resume partially downloaded files
                                 (restart from beginning)
--no-part                        Do not use .part files - write directly
                                 into output file
--no-mtime                       Do not use the Last-modified header to set
                                 the file modification time
--write-description              Write video description to a .description
                                 file
--write-info-json                Write video metadata to a .info.json file
--write-annotations              Write video annotations to a
                                 .annotations.xml file
--load-info-json FILE            JSON file containing the video information
                                 (created with the ""--write-info-json""
                                 option)
--cookies FILE                   File to read cookies from and dump cookie
                                 jar in
--cache-dir DIR                  Location in the filesystem where youtube-dl
                                 can store some downloaded information
                                 permanently. By default
                                 $XDG_CACHE_HOME/youtube-dl or
                                 ~/.cache/youtube-dl . At the moment, only
                                 YouTube player files (for videos with
                                 obfuscated signatures) are cached, but that
                                 may change.
--no-cache-dir                   Disable filesystem caching
--rm-cache-dir                   Delete all filesystem cache files

Thumbnail images:
--write-thumbnail                Write thumbnail image to disk
--write-all-thumbnails           Write all thumbnail image formats to disk
--list-thumbnails                Simulate and list all available thumbnail
                                 formats

Verbosity / Simulation Options:
-q, --quiet                      Activate quiet mode
--no-warnings                    Ignore warnings
-s, --simulate                   Do not download the video and do not write
                                 anything to disk
--skip-download                  Do not download the video
-g, --get-url                    Simulate, quiet but print URL
-e, --get-title                  Simulate, quiet but print title
--get-id                         Simulate, quiet but print id
--get-thumbnail                  Simulate, quiet but print thumbnail URL
--get-description                Simulate, quiet but print video description
--get-duration                   Simulate, quiet but print video length
--get-filename                   Simulate, quiet but print output filename
--get-format                     Simulate, quiet but print output format
-j, --dump-json                  Simulate, quiet but print JSON information.
                                 See the ""OUTPUT TEMPLATE"" for a description
                                 of available keys.
-J, --dump-single-json           Simulate, quiet but print JSON information
                                 for each command-line argument. If the URL
                                 refers to a playlist, dump the whole
                                 playlist information in a single line.
--print-json                     Be quiet and print the video information as
                                 JSON (video is still being downloaded).
--newline                        Output progress bar as new lines
--no-progress                    Do not print progress bar
--console-title                  Display progress in console titlebar
-v, --verbose                    Print various debugging information
--dump-pages                     Print downloaded pages encoded using base64
                                 to debug problems (very verbose)
--write-pages                    Write downloaded intermediary pages to
                                 files in the current directory to debug
                                 problems
--print-traffic                  Display sent and read HTTP traffic
-C, --call-home                  Contact the youtube-dl server for debugging
--no-call-home                   Do NOT contact the youtube-dl server for
                                 debugging

Workarounds:
--encoding ENCODING              Force the specified encoding (experimental)
--no-check-certificate           Suppress HTTPS certificate validation
--prefer-insecure                Use an unencrypted connection to retrieve
                                 information about the video. (Currently
                                 supported only for YouTube)
--user-agent UA                  Specify a custom user agent
--referer URL                    Specify a custom referer, use if the video
                                 access is restricted to one domain
--add-header FIELD:VALUE         Specify a custom HTTP header and its value,
                                 separated by a colon ':'. You can use this
                                 option multiple times
--bidi-workaround                Work around terminals that lack
                                 bidirectional text support. Requires bidiv
                                 or fribidi executable in PATH
--sleep-interval SECONDS         Number of seconds to sleep before each
                                 download when used alone or a lower bound
                                 of a range for randomized sleep before each
                                 download (minimum possible number of
                                 seconds to sleep) when used along with
                                 --max-sleep-interval.
--max-sleep-interval SECONDS     Upper bound of a range for randomized sleep
                                 before each download (maximum possible
                                 number of seconds to sleep). Must only be
                                 used along with --min-sleep-interval.

Video Format Options:
-f, --format FORMAT              Video format code, see the ""FORMAT
                                 SELECTION"" for all the info
--all-formats                    Download all available video formats
--prefer-free-formats            Prefer free video formats unless a specific
                                 one is requested
-F, --list-formats               List all available formats of requested
                                 videos
--youtube-skip-dash-manifest     Do not download the DASH manifests and
                                 related data on YouTube videos
--merge-output-format FORMAT     If a merge is required (e.g.
                                 bestvideo+bestaudio), output to given
                                 container format. One of mkv, mp4, ogg,
                                 webm, flv. Ignored if no merge is required

Subtitle Options:
--write-sub                      Write subtitle file
--write-auto-sub                 Write automatically generated subtitle file
                                 (YouTube only)
--all-subs                       Download all the available subtitles of the
                                 video
--list-subs                      List all available subtitles for the video
--sub-format FORMAT              Subtitle format, accepts formats
                                 preference, for example: ""srt"" or
                                 ""ass/srt/best""
--sub-lang LANGS                 Languages of the subtitles to download
                                 (optional) separated by commas, use --list-
                                 subs for available language tags

Authentication Options:
-u, --username USERNAME          Login with this account ID
-p, --password PASSWORD          Account password. If this option is left
                                 out, youtube-dl will ask interactively.
-2, --twofactor TWOFACTOR        Two-factor authentication code
-n, --netrc                      Use .netrc authentication data
--video-password PASSWORD        Video password (vimeo, smotri, youku)

Adobe Pass Options:
--ap-mso MSO                     Adobe Pass multiple-system operator (TV
                                 provider) identifier, use --ap-list-mso for
                                 a list of available MSOs
--ap-username USERNAME           Multiple-system operator account login
--ap-password PASSWORD           Multiple-system operator account password.
                                 If this option is left out, youtube-dl will
                                 ask interactively.
--ap-list-mso                    List all supported multiple-system
                                 operators

Post-processing Options:
-x, --extract-audio              Convert video files to audio-only files
                                 (requires ffmpeg or avconv and ffprobe or
                                 avprobe)
--audio-format FORMAT            Specify audio format: ""best"", ""aac"",
                                 ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or
                                 ""wav""; ""best"" by default; No effect without
                                 -x
--audio-quality QUALITY          Specify ffmpeg/avconv audio quality, insert
                                 a value between 0 (better) and 9 (worse)
                                 for VBR or a specific bitrate like 128K
                                 (default 5)
--recode-video FORMAT            Encode the video to another format if
                                 necessary (currently supported:
                                 mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS        Give these arguments to the postprocessor
-k, --keep-video                 Keep the video file on disk after the post-
                                 processing; the video is erased by default
--no-post-overwrites             Do not overwrite post-processed files; the
                                 post-processed files are overwritten by
                                 default
--embed-subs                     Embed subtitles in the video (only for mp4,
                                 webm and mkv videos)
--embed-thumbnail                Embed thumbnail in the audio as cover art
--add-metadata                   Write metadata to the video file
--metadata-from-title FORMAT     Parse additional metadata like song title /
                                 artist from the video title. The format
                                 syntax is the same as --output. Regular
                                 expression with named capture groups may
                                 also be used. The parsed parameters replace
                                 existing values. Example: --metadata-from-
                                 title ""%(artist)s - %(title)s"" matches a
                                 title like ""Coldplay - Paradise"". Example
                                 (regex): --metadata-from-title
                                 ""(?P<artist>.+?) - (?P<title>.+)""
--xattrs                         Write metadata to the video file's xattrs
                                 (using dublin core and xdg standards)
--fixup POLICY                   Automatically correct known faults of the
                                 file. One of never (do nothing), warn (only
                                 emit a warning), detect_or_warn (the
                                 default; fix file if we can, warn
                                 otherwise)
--prefer-avconv                  Prefer avconv over ffmpeg for running the
                                 postprocessors
--prefer-ffmpeg                  Prefer ffmpeg over avconv for running the
                                 postprocessors (default)
--ffmpeg-location PATH           Location of the ffmpeg/avconv binary;
                                 either the path to the binary or its
                                 containing directory.
--exec CMD                       Execute a command on the file after
                                 downloading, similar to find's -exec
                                 syntax. Example: --exec 'adb push {}
                                 /sdcard/Music/ && rm {}'
--convert-subs FORMAT            Convert the subtitles to other format
                                 (currently supported: srt|ass|vtt|lrc)

CONFIGURATION
You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at /etc/youtube-dl.conf and the user wide configuration file at ~/.config/youtube-dl/config. On Windows, the user wide configuration file locations are %APPDATA%\youtube-dl\config.txt or C:\Users\<user name>\youtube-dl.conf. Note that by default configuration file may not exist so you may need to create it yourself.
For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under Movies directory in your home directory:
# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s

Note that options in configuration file are just the same options aka switches used in regular command line calls thus there must be no whitespace after - or --, e.g. -o or --proxy but not - o or -- proxy.
You can use --ignore-config if you want to disable the configuration file for a particular youtube-dl run.
You can also use --config-location if you want to use custom configuration file for a particular youtube-dl run.
Authentication with .netrc file
You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with --username and --password) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a .netrc file on a per extractor basis. For that you will need to create a .netrc file in your $HOME and restrict permissions to read/write by only you:
touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc

After that you can add credentials for an extractor in the following format, where extractor is the name of the extractor in lowercase:
machine <extractor> login <login> password <password>

For example:
machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password

To activate authentication with the .netrc file you should pass --netrc to youtube-dl or place it in the configuration file.
On Windows you may also need to setup the %HOME% environment variable manually. For example:
set HOME=%USERPROFILE%

OUTPUT TEMPLATE
The -o option allows users to indicate a template for the output file names.
tl;dr: navigate me to examples.
The basic usage is not to set any template arguments when downloading a single file, like in youtube-dl -o funny_video.flv ""https://some/video"". However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to python string formatting operations. For example, %(NAME)s or %(NAME)05d. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:

id (string): Video identifier
title (string): Video title
url (string): Video URL
ext (string): Video filename extension
alt_title (string): A secondary title of the video
display_id (string): An alternative identifier for the video
uploader (string): Full name of the video uploader
license (string): License name the video is licensed under
creator (string): The creator of the video
release_date (string): The date (YYYYMMDD) when the video was released
timestamp (numeric): UNIX timestamp of the moment the video became available
upload_date (string): Video upload date (YYYYMMDD)
uploader_id (string): Nickname or id of the video uploader
channel (string): Full name of the channel the video is uploaded on
channel_id (string): Id of the channel
location (string): Physical location where the video was filmed
duration (numeric): Length of the video in seconds
view_count (numeric): How many users have watched the video on the platform
like_count (numeric): Number of positive ratings of the video
dislike_count (numeric): Number of negative ratings of the video
repost_count (numeric): Number of reposts of the video
average_rating (numeric): Average rating give by users, the scale used depends on the webpage
comment_count (numeric): Number of comments on the video
age_limit (numeric): Age restriction for the video (years)
is_live (boolean): Whether this video is a live stream or a fixed-length video
start_time (numeric): Time in seconds where the reproduction should start, as specified in the URL
end_time (numeric): Time in seconds where the reproduction should end, as specified in the URL
format (string): A human-readable description of the format
format_id (string): Format code specified by --format
format_note (string): Additional info about the format
width (numeric): Width of the video
height (numeric): Height of the video
resolution (string): Textual description of width and height
tbr (numeric): Average bitrate of audio and video in KBit/s
abr (numeric): Average audio bitrate in KBit/s
acodec (string): Name of the audio codec in use
asr (numeric): Audio sampling rate in Hertz
vbr (numeric): Average video bitrate in KBit/s
fps (numeric): Frame rate
vcodec (string): Name of the video codec in use
container (string): Name of the container format
filesize (numeric): The number of bytes, if known in advance
filesize_approx (numeric): An estimate for the number of bytes
protocol (string): The protocol that will be used for the actual download
extractor (string): Name of the extractor
extractor_key (string): Key name of the extractor
epoch (numeric): Unix epoch when creating the file
autonumber (numeric): Five-digit number that will be increased with each download, starting at zero
playlist (string): Name or id of the playlist that contains the video
playlist_index (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist
playlist_id (string): Playlist identifier
playlist_title (string): Playlist title
playlist_uploader (string): Full name of the playlist uploader
playlist_uploader_id (string): Nickname or id of the playlist uploader

Available for the video that belongs to some logical chapter or section:

chapter (string): Name or title of the chapter the video belongs to
chapter_number (numeric): Number of the chapter the video belongs to
chapter_id (string): Id of the chapter the video belongs to

Available for the video that is an episode of some series or programme:

series (string): Title of the series or programme the video episode belongs to
season (string): Title of the season the video episode belongs to
season_number (numeric): Number of the season the video episode belongs to
season_id (string): Id of the season the video episode belongs to
episode (string): Title of the video episode
episode_number (numeric): Number of the video episode within a season
episode_id (string): Id of the video episode

Available for the media that is a track or a part of a music album:

track (string): Title of the track
track_number (numeric): Number of the track within an album or a disc
track_id (string): Id of the track
artist (string): Artist(s) of the track
genre (string): Genre(s) of the track
album (string): Title of the album the track belongs to
album_type (string): Type of the album
album_artist (string): List of all artists appeared on the album
disc_number (numeric): Number of the disc or other physical medium the track belongs to
release_year (numeric): Year (YYYY) when the album was released

Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with NA.
For example for -o %(title)s-%(id)s.%(ext)s and an mp4 video with title youtube-dl test video and id BaW_jenozKcj, this will result in a youtube-dl test video-BaW_jenozKcj.mp4 file created in the current directory.
For numeric sequences you can use numeric related formatting, for example, %(view_count)05d will result in a string with view count padded with zeros up to 5 characters, like in 00042.
Output templates can also contain arbitrary hierarchical path, e.g. -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.
To use percent literals in an output template use %%. To output to stdout use -o -.
The current default template is %(title)s-%(id)s.%(ext)s.
In some cases, you don't want special characters such as 中, spaces, or &, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the --restrict-filenames flag to get a shorter title:
Output template and Windows batch files
If you are using an output template inside a Windows batch file then you must escape plain percent characters (%) by doubling, so that -o ""%(title)s-%(id)s.%(ext)s"" should become -o ""%%(title)s-%%(id)s.%%(ext)s"". However you should not touch %'s that are not plain characters, e.g. environment variables for expansion should stay intact: -o ""C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"".
Output template examples
Note that on Windows you may need to use double quotes instead of single.
$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
youtube-dl test video ''_ä↭𝕐.mp4    # All kinds of weird characters

$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          # A simple file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ youtube-dl -o ""C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s"" https://videomore.ru/kino_v_detalayah/5_sezon/367617

# Stream the video being downloaded to stdout
$ youtube-dl -o - BaW_jenozKc
FORMAT SELECTION
By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you don't need to pass any special options, youtube-dl will guess it for you by default.
But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called format selection based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.
The general syntax for format selection is --format FORMAT or shorter -f FORMAT where FORMAT is a selector expression, i.e. an expression that describes format or formats you would like to download.
tl;dr: navigate me to examples.
The simplest case is requesting a specific format, for example with -f 22 you can download the format with format code equal to 22. You can get the list of available format codes for particular video using --list-formats or -F. Note that these format codes are extractor specific.
You can also use a file extension (currently 3gp, aac, flv, m4a, mp3, mp4, ogg, wav, webm are supported) to download the best quality format of a particular file extension served as a single file, e.g. -f webm will download the best quality format with the webm extension served as a single file.
You can also use special names to select particular edge case formats:

best: Select the best quality format represented by a single file with video and audio.
worst: Select the worst quality format represented by a single file with video and audio.
bestvideo: Select the best quality video-only format (e.g. DASH video). May not be available.
worstvideo: Select the worst quality video-only format. May not be available.
bestaudio: Select the best quality audio only-format. May not be available.
worstaudio: Select the worst quality audio only-format. May not be available.

For example, to download the worst quality video-only format you can use -f worstvideo.
If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example -f 22/17/18 will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.
If you want to download several formats of the same video use a comma as a separator, e.g. -f 22,17,18 will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: -f 136/137/mp4/bestvideo,140/m4a/bestaudio.
You can also filter the video formats by putting a condition in brackets, as in -f ""best[height=720]"" (or -f ""[filesize>10M]"").
The following numeric meta fields can be used with comparisons <, <=, >, >=, = (equals), != (not equals):

filesize: The number of bytes, if known in advance
width: Width of the video, if known
height: Height of the video, if known
tbr: Average bitrate of audio and video in KBit/s
abr: Average audio bitrate in KBit/s
vbr: Average video bitrate in KBit/s
asr: Audio sampling rate in Hertz
fps: Frame rate

Also filtering work for comparisons = (equals), ^= (starts with), $= (ends with), *= (contains) and following string meta fields:

ext: File extension
acodec: Name of the audio codec in use
vcodec: Name of the video codec in use
container: Name of the container format
protocol: The protocol that will be used for the actual download, lower-case (http, https, rtsp, rtmp, rtmpe, mms, f4m, ism, http_dash_segments, m3u8, or m3u8_native)
format_id: A short description of the format

Any string comparison may be prefixed with negation ! in order to produce an opposite comparison, e.g. !*= (does not contain).
Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.
Formats for which the value is not known are excluded unless you put a question mark (?) after the operator. You can combine format filters, so -f ""[height <=? 720][tbr>500]"" selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.
You can merge the video and audio of two formats into a single file using -f <video-format>+<audio-format> (requires ffmpeg or avconv installed), for example -f bestvideo+bestaudio will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.
Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use -f '(mp4,webm)[height<480]'.
Since the end of April 2015 and version 2015.04.26, youtube-dl uses -f bestvideo+bestaudio/best as the default format selection (see #5447, #5456). If ffmpeg or avconv are installed this results in downloading bestvideo and bestaudio separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to best and results in downloading the best available quality served as a single file. best is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add -f bestvideo[height<=?1080]+bestaudio/best to your configuration file. Note that if you use youtube-dl to stream to stdout (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as -o -, youtube-dl still uses -f best format selection in order to start content delivery immediately to your player and not to wait until bestvideo and bestaudio are downloaded and muxed.
If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with -f best. You may want to add it to the configuration file in order not to type it every time you run youtube-dl.
Format selection examples
Note that on Windows you may need to use double quotes instead of single.
# Download best mp4 format available or any other best if no mp4 available
$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'

# Download best format available but no better than 480p
$ youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'

# Download best video only format but no bigger than 50 MB
$ youtube-dl -f 'best[filesize<50M]'

# Download best format available via direct link over HTTP/HTTPS protocol
$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'

# Download the best video format and the best audio format without merging them
$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'
Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.
VIDEO SELECTION
Videos can be filtered by their upload date using the options --date, --datebefore or --dateafter. They accept dates in two formats:

Absolute dates: Dates in the format YYYYMMDD.
Relative dates: Dates in the format (now|today)[+-][0-9](day|week|month|year)(s)?

Examples:
# Download only the videos uploaded in the last 6 months
$ youtube-dl --dateafter now-6months

# Download only the videos uploaded on January 1, 1970
$ youtube-dl --date 19700101

$ # Download only the videos uploaded in the 200x decade
$ youtube-dl --dateafter 20000101 --datebefore 20091231
FAQ
How do I update youtube-dl?
If you've followed our manual installation instructions, you can simply run youtube-dl -U (or, on Linux, sudo youtube-dl -U).
If you have used pip, a simple sudo pip install -U youtube-dl is sufficient to update.
If you have installed youtube-dl using a package manager like apt-get or yum, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to https://yt-dl.org to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.
As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like
sudo apt-get remove -y youtube-dl

Afterwards, simply follow our manual installation instructions:
sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
hash -r

Again, from then on you'll be able to update with sudo youtube-dl -U.
youtube-dl is extremely slow to start on Windows
Add a file exclusion for youtube-dl.exe in Windows Defender settings.
I'm getting an error Unable to extract OpenGraph title on YouTube playlists
YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.
If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to report bugs to the Ubuntu packaging people - all they have to do is update the package to a somewhat recent version. See above for a way to update.
I'm getting an error when trying to use output template: error: using output template conflicts with using title, video ID or auto number
Make sure you are not using -o with any of these options -t, --title, --id, -A or --auto-number set in command line or in a configuration file. Remove the latter if any.
Do I always have to pass -citw?
By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, please file an issue where you explain that). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of -citw that is regularly useful is -i.
Can you please put the -b option back?
Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the -b option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the -f option and youtube-dl will try to download it.
I get HTTP error 402 when trying to download a video. What's this?
Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're considering to provide a way to let you solve the CAPTCHA, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.
Do I need any other programs?
youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need avconv or ffmpeg. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.
Videos or video formats streamed via RTMP protocol can only be downloaded when rtmpdump is installed. Downloading MMS and RTSP videos requires either mplayer or mpv to be installed.
I have downloaded a video but how can I play it?
Once the video is fully downloaded, use any video player, such as mpv, vlc or mplayer.
I extracted a video URL with -g, but it does not play on another machine / in my web browser.
It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the --cookies option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use --dump-user-agent to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with --dump-json.
It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.
Please bear in mind that some URL protocols are not supported by browsers out of the box, including RTMP. If you are using -g, your own downloader must support these as well.
If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use -o - to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.
ERROR: no fmt_url_map or conn information found in video info
YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See above for how to update youtube-dl.
ERROR: unable to download video
YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See above for how to update youtube-dl.
Video URL contains an ampersand and I'm getting some strange output [1] 2839 or 'v' is not recognized as an internal or external command
That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).
For example if your URL is https://www.youtube.com/watch?t=4&v=BaW_jenozKc you should end up with following command:
youtube-dl 'https://www.youtube.com/watch?t=4&v=BaW_jenozKc'
or
youtube-dl https://www.youtube.com/watch?t=4\&v=BaW_jenozKc
For Windows you have to use the double quotes:
youtube-dl ""https://www.youtube.com/watch?t=4&v=BaW_jenozKc""
ExtractorError: Could not find JS function u'OF'
In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See above for how to update youtube-dl.
HTTP Error 429: Too Many Requests or 402: Payment Required
These two error codes indicate that the service is blocking your IP address because of overuse. Contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the --proxy or --source-address options to select another IP address.
SyntaxError: Non-ASCII character
The error
File ""youtube-dl"", line 2
SyntaxError: Non-ASCII character '\x93' ...

means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.
What is this binary file? Where has the code gone?
Since June 2012 (#342) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to youtube-dl.zip first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the __main__.py file. To recompile the executable, run make youtube-dl.
The exe throws an error due to missing MSVCR100.dll
To run the exe you need to install first the Microsoft Visual C++ 2010 Redistributable Package (x86).
On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?
If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.
To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, C:\bin, or C:\Users\<User name>\bin), put all the executables directly in there, and then set your PATH environment variable to include that directory.
From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing youtube-dl or ffmpeg, no matter what directory you're in.
How do I put downloads into a specific folder?
Use the -o to specify an output template, for example -o ""/home/user/videos/%(title)s-%(id)s.%(ext)s"". If you want this for all of your downloads, put the option into your configuration file.
How do I download a video starting with a -?
Either prepend https://www.youtube.com/watch?v= or separate the ID from the options with --:
youtube-dl -- -wNyEUrxzFU
youtube-dl ""https://www.youtube.com/watch?v=-wNyEUrxzFU""

How do I pass cookies to youtube-dl?
Use the --cookies option, for example --cookies /path/to/cookies/file.txt.
In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, cookies.txt (for Chrome) or cookies.txt (for Firefox).
Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either # HTTP Cookie File or # Netscape HTTP Cookie File. Make sure you have correct newline format in the cookies file and convert newlines if necessary to correspond with your OS, namely CRLF (\r\n) for Windows and LF (\n) for Unix and Unix-like systems (Linux, macOS, etc.). HTTP Error 400: Bad Request when using --cookies is a good sign of invalid newline format.
Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around CAPTCHA some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).
How do I stream directly to media player?
You will first need to tell youtube-dl to stream media to stdout with -o -, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to vlc can be achieved with:
youtube-dl -o - ""https://www.youtube.com/watch?v=BaW_jenozKcj"" | vlc -

How do I download only new videos from a playlist?
Use download-archive feature. With this feature you should initially download the complete playlist with --download-archive /path/to/download/archive/file.txt that will record identifiers of all the videos in a special file. Each subsequent run with the same --download-archive will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.
For example, at first,
youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""

will download the complete PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re playlist and create a file archive.txt. Each subsequent run will only download new videos if any:
youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""

Should I add --hls-prefer-native into my config?
When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.
When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.
In particular, the generic extractor (used when your website is not in the list of supported sites by youtube-dl cannot mandate one specific downloader.
If you put either --hls-prefer-native or --hls-prefer-ffmpeg into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to file an issue or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.
Can you add support for this anime video site, or site which shows current movies for free?
As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.
A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should not be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A ""fair use"" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.
Support requests for services that do purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.
How can I speed up work on my issue?
(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:
First of all, please do report the issue at our issue tracker. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.
Please read the bug reporting instructions below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.
If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).
Feel free to bump the issue from time to time by writing a small comment (""Issue is still present in youtube-dl version ...from France, but fixed from Belgium""), but please not more than once a month. Please do not declare your issue as important or urgent.
How can I detect whether a given URL is supported by youtube-dl?
For one, have a look at the list of supported sites. Note that it can sometimes happen that the site changes its URL scheme (say, from https://example.com/video/1234567 to https://example.com/v/1234567 ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.
It is not possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches all URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.
If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an UnsupportedError exception if you run it from a Python program.
Why do I need to go through that much red tape when filing bugs?
Before we had the issue template, despite our extensive bug reporting instructions, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said ""please install ffmpeg"", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.
youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of youtube-dl -v YOUR_URL_HERE is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.
DEVELOPER INSTRUCTIONS
Most users do not need to build youtube-dl and can download the builds or get them from their distribution.
To run youtube-dl as a developer, you don't need to build anything either. Simply execute
python -m youtube_dl

To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:
python -m unittest discover
python test/test_download.py
nosetests

See item 6 of new extractor tutorial for how to run extractor specific test cases.
If you want to create a build of youtube-dl yourself, you'll need

python
make (only GNU make is supported)
pandoc
zip
nosetests

Adding support for a new site
If you want to add support for a new site, first of all make sure this site is not dedicated to copyright infringement. youtube-dl does not support such sites thus pull requests adding support for them will be rejected.
After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called yourextractor):


Fork this repository


Check out the source code with:
 git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git



Start a new git branch with
 cd youtube-dl
 git checkout -b yourextractor



Start with this simple template and save it to youtube_dl/extractor/yourextractor.py:
# coding: utf-8
from __future__ import unicode_literals

from .common import InfoExtractor


class YourExtractorIE(InfoExtractor):
    _VALID_URL = r'https?://(?:www\.)?yourextractor\.com/watch/(?P<id>[0-9]+)'
    _TEST = {
        'url': 'https://yourextractor.com/watch/42',
        'md5': 'TODO: md5 sum of the first 10241 bytes of the video file (use --test)',
        'info_dict': {
            'id': '42',
            'ext': 'mp4',
            'title': 'Video title goes here',
            'thumbnail': r're:^https?://.*\.jpg$',
            # TODO more properties, either as:
            # * A value
            # * MD5 checksum; start the string with md5:
            # * A regular expression; start the string with re:
            # * Any Python type (for example int or float)
        }
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage = self._download_webpage(url, video_id)

        # TODO more code goes here, for example ...
        title = self._html_search_regex(r'<h1>(.+?)</h1>', webpage, 'title')

        return {
            'id': video_id,
            'title': title,
            'description': self._og_search_description(webpage),
            'uploader': self._search_regex(r'<div[^>]+id=""uploader""[^>]*>([^<]+)<', webpage, 'uploader', fatal=False),
            # TODO more properties (see youtube_dl/extractor/common.py)
        }


Add an import in youtube_dl/extractor/extractors.py.


Run python test/test_download.py TestDownload.test_YourExtractor. This should fail at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename _TEST to _TESTS and make it into a list of dictionaries. The tests will then be named TestDownload.test_YourExtractor, TestDownload.test_YourExtractor_1, TestDownload.test_YourExtractor_2, etc. Note that tests with only_matching key in test's dict are not counted in.


Have a look at youtube_dl/extractor/common.py for possible helper methods and a detailed description of what your extractor should and may return. Add tests and code for as many as you want.


Make sure your code follows youtube-dl coding conventions and check the code with flake8:
 $ flake8 youtube_dl/extractor/yourextractor.py



Make sure your code works under all Python versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.


When the tests pass, add the new files and commit them and push the result, like this:
$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor



Finally, create a pull request. We'll then review and merge it.


In any case, thank you very much for your contributions!
youtube-dl coding conventions
This section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.
Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.
Mandatory and optional metafields
For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an information dictionary or simply info dict. Only the following meta fields in the info dict are considered mandatory for a successful extraction process by youtube-dl:

id (media identifier)
title (media title)
url (media download URL) or formats

In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats id and title as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.
Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields.
Example
Say you have some source dictionary meta that you've fetched as JSON with HTTP request and it has a key summary:
meta = self._download_json(url, video_id)
Assume at this point meta's layout is:
{
    ...
    ""summary"": ""some fancy summary text"",
    ...
}
Assume you want to extract summary and put it into the resulting info dict as description. Since description is an optional meta field you should be ready that this key may be missing from the meta dict, so that you should extract it like:
description = meta.get('summary')  # correct
and not like:
description = meta['summary']  # incorrect
The latter will break extraction process with KeyError if summary disappears from meta at some later time but with the former approach extraction will just go ahead with description set to None which is perfectly fine (remember None is equivalent to the absence of data).
Similarly, you should pass fatal=False when extracting optional data from a webpage with _search_regex, _html_search_regex or similar methods, for instance:
description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', fatal=False)
With fatal set to False if _search_regex fails to extract description it will emit a warning and continue extraction.
You can also pass default=<some fallback value>, for example:
description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', default=None)
On failure this code will silently continue the extraction with description set to None. That is useful for metafields that may or may not be present.
Provide fallbacks
When extracting metadata try to do so from multiple sources. For example if title is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.
Example
Say meta from the previous example has a title and you are about to extract it. Since title is a mandatory meta field you should end up with something like:
title = meta['title']
If title disappears from meta in future due to some changes on the hoster's side the extraction would fail since title is mandatory. That's expected.
Assume that you have some another source you can extract title from, for example og:title HTML meta of a webpage. In this case you can provide a fallback scenario:
title = meta.get('title') or self._og_search_title(webpage)
This code will try to extract from meta first and if it fails it will try extracting og:title from a webpage.
Regular expressions
Don't capture groups you don't use
Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.
Example
Don't capture id attribute name here since you can't use it for anything anyway.
Correct:
r'(?:id|ID)=(?P<id>\d+)'
Incorrect:
r'(id|ID)=(?P<id>\d+)'
Make regular expressions relaxed and flexible
When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.
Example
Say you need to extract title from the following HTML code:
<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">some fancy title</span>
The code for that task should look similar to:
title = self._search_regex(
    r'<span[^>]+class=""title""[^>]*>([^<]+)', webpage, 'title')
Or even better:
title = self._search_regex(
    r'<span[^>]+class=([""\'])title\1[^>]*>(?P<title>[^<]+)',
    webpage, 'title', group='title')
Note how you tolerate potential changes in the style attribute's value or switch from using double quotes to single for class attribute:
The code definitely should not look like:
title = self._search_regex(
    r'<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">(.*?)</span>',
    webpage, 'title', group='title')
Long lines policy
There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.
For example, you should never split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:
Correct:
'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Incorrect:
'https://www.youtube.com/watch?v=FqZTN594JQw&list='
'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Inline values
Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.
Example
Correct:
title = self._html_search_regex(r'<title>([^<]+)</title>', webpage, 'title')
Incorrect:
TITLE_RE = r'<title>([^<]+)</title>'
# ...some lines of code...
title = self._html_search_regex(TITLE_RE, webpage, 'title')
Collapse fallbacks
Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.
Example
Good:
description = self._html_search_meta(
    ['og:description', 'description', 'twitter:description'],
    webpage, 'description', default=None)
Unwieldy:
description = (
    self._og_search_description(webpage, default=None)
    or self._html_search_meta('description', webpage, default=None)
    or self._html_search_meta('twitter:description', webpage, default=None))
Methods supporting list of patterns are: _search_regex, _html_search_regex, _og_search_property, _html_search_meta.
Trailing parentheses
Always move trailing parentheses after the last argument.
Example
Correct:
    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list)
Incorrect:
    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list,
)
Use convenience conversion and parsing functions
Wrap all extracted numeric data into safe functions from youtube_dl/utils.py: int_or_none, float_or_none. Use them for string to number conversions as well.
Use url_or_none for safe URL processing.
Use try_get for safe metadata extraction from parsed JSON.
Use unified_strdate for uniform upload_date or any YYYYMMDD meta field extraction, unified_timestamp for uniform timestamp extraction, parse_filesize for filesize extraction, parse_count for count meta fields extraction, parse_resolution, parse_duration for duration extraction, parse_age_limit for age_limit extraction.
Explore youtube_dl/utils.py for more useful convenience functions.
More examples
Safely extract optional description from parsed JSON
description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)
Safely extract more optional metadata
video = try_get(response, lambda x: x['result']['video'][0], dict) or {}
description = video.get('summary')
duration = float_or_none(video.get('durationMs'), scale=1000)
view_count = int_or_none(video.get('views'))
EMBEDDING YOUTUBE-DL
youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to create a report.
From a Python program, you can embed youtube-dl in a more powerful fashion, like this:
from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
Most likely, you'll want to use various options. For a list of options available, have a look at youtube_dl/YoutubeDL.py. For a start, if you want to intercept youtube-dl's output, set a logger object.
Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:
from __future__ import unicode_literals
import youtube_dl


class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now converting ...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
BUGS
Bugs and suggestions should be reported at: https://github.com/ytdl-org/youtube-dl/issues. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel #youtube-dl on freenode (webchat).
Please include the full output of youtube-dl when run with -v, i.e. add -v flag to your command line, copy the whole output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:
$ youtube-dl -v <your command line>
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...

Do not post screenshots of verbose logs; only plain text is acceptable.
The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.
Please re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):
Is the description of the issue itself sufficient?
We often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.
So please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious

What the problem is
How it could be fixed
How your proposed solution would look like

If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.
For bug reports, this means that your report should contain the complete output of youtube-dl when called with the -v flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.
If your server has multiple IPs or you suspect censorship, adding --call-home may be a good idea to get more diagnostics. If the error is ERROR: Unable to extract ... and you cannot reproduce it from multiple countries, add --dump-pages (warning: this will yield a rather large output, redirect it to the file log.txt by adding >log.txt 2>&1 to your command-line) or upload the .dump files you get when you add --write-pages somewhere.
Site support requests must contain an example URL. An example URL is a URL you might want to download, like https://www.youtube.com/watch?v=BaW_jenozKc. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. https://www.youtube.com/) is not an example URL.
Are you using the latest version?
Before reporting any issue, type youtube-dl -U. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.
Is the issue already documented?
Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the GitHub Issues of this repository. If there is an issue, feel free to write something along the lines of ""This affects me as well, with version 2015.01.01. Here is some more information on the issue: ..."". While some issues may be old, a new post into them often spurs rapid activity.
Why are existing options not enough?
Before requesting a new feature, please have a quick peek at the list of supported options. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do not solve your problem.
Is there enough context in your bug report?
People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).
We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.
Does the issue involve one problem, and one problem only?
Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.
In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.
Is anyone going to need the feature?
Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.
Is your question about youtube-dl?
It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.
COPYRIGHT
youtube-dl is released into the public domain by the copyright holders.
This README file was originally written by Daniel Bolton and is likewise released into the public domain.
"
36,Python,"The Fuck     
The Fuck is a magnificent app, inspired by a @liamosaur
tweet,
that corrects errors in previous console commands.
Is The Fuck too slow? Try the experimental instant mode!

More examples:
➜ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

➜ fuck
sudo apt-get install vim [enter/↑/↓/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
➜ git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin master


➜ fuck
git push --set-upstream origin master [enter/↑/↓/ctrl+c]
Counting objects: 9, done.
...
➜ puthon
No command 'puthon' found, did you mean:
 Command 'python' from package 'python-minimal' (main)
 Command 'python' from package 'python3' (main)
zsh: command not found: puthon

➜ fuck
python [enter/↑/↓/ctrl+c]
Python 3.4.2 (default, Oct  8 2014, 13:08:17)
...
➜ git brnch
git: 'brnch' is not a git command. See 'git --help'.

Did you mean this?
    branch

➜ fuck
git branch [enter/↑/↓/ctrl+c]
* master
➜ lein rpl
'rpl' is not a task. See 'lein help'.

Did you mean this?
         repl

➜ fuck
lein repl [enter/↑/↓/ctrl+c]
nREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848
REPL-y 0.3.1
...
If you're not afraid of blindly running corrected commands, the
require_confirmation settings option can be disabled:
➜ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

➜ fuck
sudo apt-get install vim
[sudo] password for nvbn:
Reading package lists... Done
...
Requirements

python (3.4+)
pip
python-dev

Installation
On OS X, you can install The Fuck via Homebrew (or via Linuxbrew on Linux):
brew install thefuck
On Ubuntu / Mint, install The Fuck with the following commands:
sudo apt update
sudo apt install python3-dev python3-pip python3-setuptools
sudo pip3 install thefuck
On FreeBSD, install The Fuck with the following commands:
pkg install thefuck
On ChromeOS, install The Fuck using chromebrew with the following command:
crew install thefuck
On other systems, install The Fuck  by using pip:
pip install thefuck
Alternatively, you may use an OS package manager (OS X, Ubuntu, Arch).
#
It is recommended that you place this command in your .bash_profile,
.bashrc, .zshrc or other startup script:
eval $(thefuck --alias)
# You can use whatever you want as an alias, like for Mondays:
eval $(thefuck --alias FUCK)
Or in your shell config (Bash, Zsh, Fish, Powershell, tcsh).
Changes are only available in a new shell session. To make changes immediately
available, run source ~/.bashrc (or your shell config file like .zshrc).
To run fixed commands without confirmation, use the --yeah option (or just -y for short, or --hard if you're especially frustrated):
fuck --yeah
To fix commands recursively until succeeding, use the -r option:
fuck -r
Updating
pip3 install thefuck --upgrade
Note: Alias functionality was changed in v1.34 of The Fuck
How it works
The Fuck attempts to match the previous command with a rule. If a match is
found, a new command is created using the matched rule and executed. The
following rules are enabled by default:

adb_unknown_command – fixes misspelled commands like adb logcta;
ag_literal – adds -Q to ag when suggested;
aws_cli – fixes misspelled commands like aws dynamdb scan;
az_cli – fixes misspelled commands like az providers;
cargo – runs cargo build instead of cargo;
cargo_no_command – fixes wrongs commands like cargo buid;
cat_dir – replaces cat with ls when you try to cat a directory;
cd_correction – spellchecks and correct failed cd commands;
cd_mkdir – creates directories before cd'ing into them;
cd_parent – changes cd.. to cd ..;
chmod_x – add execution bit;
choco_install – append common suffixes for chocolatey packages;
composer_not_command – fixes composer command name;
cp_omitting_directory – adds -a when you cp directory;
cpp11 – adds missing -std=c++11 to g++ or clang++;
dirty_untar – fixes tar x command that untarred in the current directory;
dirty_unzip – fixes unzip command that unzipped in the current directory;
django_south_ghost – adds --delete-ghost-migrations to failed because ghosts django south migration;
django_south_merge – adds --merge to inconsistent django south migration;
docker_login – executes a docker login and repeats the previous command;
docker_not_command – fixes wrong docker commands like docker tags;
docker_image_being_used_by_container ‐ removes the container that is using the image before removing the image;
dry – fixes repetitions like git git push;
fab_command_not_found – fix misspelled fabric commands;
fix_alt_space – replaces Alt+Space with Space character;
fix_file – opens a file with an error in your $EDITOR;
gem_unknown_command – fixes wrong gem commands;
git_add – fixes ""pathspec 'foo' did not match any file(s) known to git."";
git_add_force – adds --force to git add <pathspec>... when paths are .gitignore'd;
git_bisect_usage – fixes git bisect strt, git bisect goood, git bisect rset, etc. when bisecting;
git_branch_delete – changes git branch -d to git branch -D;
git_branch_delete_checked_out – changes git branch -d to git checkout master && git branch -D when trying to delete a checked out branch;
git_branch_exists – offers git branch -d foo, git branch -D foo or git checkout foo when creating a branch that already exists;
git_branch_list – catches git branch list in place of git branch and removes created branch;
git_checkout – fixes branch name or creates new branch;
git_commit_amend – offers git commit --amend after previous commit;
git_commit_reset – offers git reset HEAD~ after previous commit;
git_diff_no_index – adds --no-index to previous git diff on untracked files;
git_diff_staged – adds --staged to previous git diff with unexpected output;
git_fix_stash – fixes git stash commands (misspelled subcommand and missing save);
git_flag_after_filename – fixes fatal: bad flag '...' after filename
git_help_aliased – fixes git help <alias> commands replacing  with the aliased command;
git_merge – adds remote to branch names;
git_merge_unrelated – adds --allow-unrelated-histories when required
git_not_command – fixes wrong git commands like git brnch;
git_pull – sets upstream before executing previous git pull;
git_pull_clone – clones instead of pulling when the repo does not exist;
git_pull_uncommitted_changes – stashes changes before pulling and pops them afterwards;
git_push – adds --set-upstream origin $branch to previous failed git push;
git_push_different_branch_names – fixes pushes when local brach name does not match remote branch name;
git_push_pull – runs git pull when push was rejected;
git_push_without_commits – Creates an initial commit if you forget and only git add ., when setting up a new project;
git_rebase_no_changes – runs git rebase --skip instead of git rebase --continue when there are no changes;
git_remote_delete – replaces git remote delete remote_name with git remote remove remote_name;
git_rm_local_modifications –  adds -f or --cached when you try to rm a locally modified file;
git_rm_recursive – adds -r when you try to rm a directory;
git_rm_staged –  adds -f or --cached when you try to rm a file with staged changes
git_rebase_merge_dir – offers git rebase (--continue | --abort | --skip) or removing the .git/rebase-merge dir when a rebase is in progress;
git_remote_seturl_add – runs git remote add when git remote set_url on nonexistent remote;
git_stash – stashes your local modifications before rebasing or switching branch;
git_stash_pop – adds your local modifications before popping stash, then resets;
git_tag_force – adds --force to git tag <tagname> when the tag already exists;
git_two_dashes – adds a missing dash to commands like git commit -amend or git rebase -continue;
go_run – appends .go extension when compiling/running Go programs;
go_unknown_command – fixes wrong go commands, for example go bulid;
gradle_no_task – fixes not found or ambiguous gradle task;
gradle_wrapper – replaces gradle with ./gradlew;
grep_arguments_order – fixes grep arguments order for situations like grep -lir . test;
grep_recursive – adds -r when you try to grep directory;
grunt_task_not_found – fixes misspelled grunt commands;
gulp_not_task – fixes misspelled gulp tasks;
has_exists_script – prepends ./ when script/binary exists;
heroku_multiple_apps – add --app <app> to heroku commands like heroku pg;
heroku_not_command – fixes wrong heroku commands like heroku log;
history – tries to replace command with most similar command from history;
hostscli – tries to fix hostscli usage;
ifconfig_device_not_found – fixes wrong device names like wlan0 to wlp2s0;
java – removes .java extension when running Java programs;
javac – appends missing .java when compiling Java files;
lein_not_task – fixes wrong lein tasks like lein rpl;
long_form_help – changes -h to --help when the short form version is not supported
ln_no_hard_link – catches hard link creation on directories, suggest symbolic link;
ln_s_order – fixes ln -s arguments order;
ls_all – adds -A to ls when output is empty;
ls_lah – adds -lah to ls;
man – changes manual section;
man_no_space – fixes man commands without spaces, for example mandiff;
mercurial – fixes wrong hg commands;
missing_space_before_subcommand – fixes command with missing space like npminstall;
mkdir_p – adds -p when you try to create a directory without parent;
mvn_no_command – adds clean package to mvn;
mvn_unknown_lifecycle_phase – fixes misspelled life cycle phases with mvn;
npm_missing_script – fixes npm custom script name in npm run-script <script>;
npm_run_script – adds missing run-script for custom npm scripts;
npm_wrong_command – fixes wrong npm commands like npm urgrade;
no_command – fixes wrong console commands, for example vom/vim;
no_such_file – creates missing directories with mv and cp commands;
open – either prepends http:// to address passed to open or create a new file or directory and passes it to open;
pip_install – fixes permission issues with pip install commands by adding --user or prepending sudo if necessary;
pip_unknown_command – fixes wrong pip commands, for example pip instatl/pip install;
php_s – replaces -s by -S when trying to run a local php server;
port_already_in_use – kills process that bound port;
prove_recursively – adds -r when called with directory;
pyenv_no_such_command – fixes wrong pyenv commands like pyenv isntall or pyenv list;
python_command – prepends python when you try to run non-executable/without ./ python script;
python_execute – appends missing .py when executing Python files;
quotation_marks – fixes uneven usage of ' and "" when containing args';
path_from_history – replaces not found path with similar absolute path from history;
react_native_command_unrecognized – fixes unrecognized react-native commands;
remove_shell_prompt_literal – remove leading shell prompt symbol $, common when copying commands from documentations;
remove_trailing_cedilla – remove trailing cedillas ç, a common typo for european keyboard layouts;
rm_dir – adds -rf when you try to remove a directory;
scm_correction – corrects wrong scm like hg log to git log;
sed_unterminated_s – adds missing '/' to sed's s commands;
sl_ls – changes sl to ls;
ssh_known_hosts – removes host from known_hosts on warning;
sudo – prepends sudo to previous command if it failed because of permissions;
sudo_command_from_user_path – runs commands from users $PATH with sudo;
switch_lang – switches command from your local layout to en;
systemctl – correctly orders parameters of confusing systemctl;
terraform_init.py – run terraform init before plan or apply;
test.py – runs py.test instead of test.py;
touch – creates missing directories before ""touching"";
tsuru_login – runs tsuru login if not authenticated or session expired;
tsuru_not_command – fixes wrong tsuru commands like tsuru shell;
tmux – fixes tmux commands;
unknown_command – fixes hadoop hdfs-style ""unknown command"", for example adds missing '-' to the command on hdfs dfs ls;
unsudo – removes sudo from previous command if a process refuses to run on super user privilege.
vagrant_up – starts up the vagrant instance;
whois – fixes whois command;
workon_doesnt_exists – fixes virtualenvwrapper env name os suggests to create new.
yarn_alias – fixes aliased yarn commands like yarn ls;
yarn_command_not_found – fixes misspelled yarn commands;
yarn_command_replaced – fixes replaced yarn commands;
yarn_help – makes it easier to open yarn documentation;

The following rules are enabled by default on specific platforms only:

apt_get – installs app from apt if it not installed (requires python-commandnotfound / python3-commandnotfound);
apt_get_search – changes trying to search using apt-get with searching using apt-cache;
apt_invalid_operation – fixes invalid apt and apt-get calls, like apt-get isntall vim;
apt_list_upgradable – helps you run apt list --upgradable after apt update;
apt_upgrade – helps you run apt upgrade after apt list --upgradable;
brew_cask_dependency – installs cask dependencies;
brew_install – fixes formula name for brew install;
brew_reinstall – turns brew install <formula> into brew reinstall <formula>;
brew_link – adds --overwrite --dry-run if linking fails;
brew_uninstall – adds --force to brew uninstall if multiple versions were installed;
brew_unknown_command – fixes wrong brew commands, for example brew docto/brew doctor;
brew_update_formula – turns brew update <formula> into brew upgrade <formula>;
dnf_no_such_command – fixes mistyped DNF commands;
nixos_cmd_not_found – installs apps on NixOS;
pacman – installs app with pacman if it is not installed (uses yay or yaourt if available);
pacman_not_found – fixes package name with pacman, yay or yaourt.
yum_invalid_operation – fixes invalid yum calls, like yum isntall vim;

The following commands are bundled with The Fuck, but are not enabled by
default:

git_push_force – adds --force-with-lease to a git push (may conflict with git_push_pull);
rm_root – adds --no-preserve-root to rm -rf / command.

Creating your own rules
To add your own rule, create a file named your-rule-name.py
in ~/.config/thefuck/rules. The rule file must contain two functions:
match(command: Command) -> bool
get_new_command(command: Command) -> str | list[str]
Additionally, rules can contain optional functions:
side_effect(old_command: Command, fixed_command: str) -> None
Rules can also contain the optional variables enabled_by_default, requires_output and priority.
Command has three attributes: script, output and script_parts.
Your rule should not change Command.
Rules api changed in 3.0: To access a rule's settings, import it with
from thefuck.conf import settings
settings is a special object assembled from ~/.config/thefuck/settings.py,
and values from env (see more below).
A simple example rule for running a script with sudo:
def match(command):
    return ('permission denied' in command.output.lower()
            or 'EACCES' in command.output)


def get_new_command(command):
    return 'sudo {}'.format(command.script)

# Optional:
enabled_by_default = True

def side_effect(command, fixed_command):
    subprocess.call('chmod 777 .', shell=True)

priority = 1000  # Lower first, default is 1000

requires_output = True
More examples of rules,
utility functions for rules,
app/os-specific helpers.
Settings
Several The Fuck parameters can be changed in the file $XDG_CONFIG_HOME/thefuck/settings.py
($XDG_CONFIG_HOME defaults to ~/.config):

rules – list of enabled rules, by default thefuck.const.DEFAULT_RULES;
exclude_rules – list of disabled rules, by default [];
require_confirmation – requires confirmation before running new command, by default True;
wait_command – max amount of time in seconds for getting previous command output;
no_colors – disable colored output;
priority – dict with rules priorities, rule with lower priority will be matched first;
debug – enables debug output, by default False;
history_limit – numeric value of how many history commands will be scanned, like 2000;
alter_history – push fixed command to history, by default True;
wait_slow_command – max amount of time in seconds for getting previous command output if it in slow_commands list;
slow_commands – list of slow commands;
num_close_matches – maximum number of close matches to suggest, by default 3.

An example of settings.py:
rules = ['sudo', 'no_command']
exclude_rules = ['git_push']
require_confirmation = True
wait_command = 10
no_colors = False
priority = {'sudo': 100, 'no_command': 9999}
debug = False
history_limit = 9999
wait_slow_command = 20
slow_commands = ['react-native', 'gradle']
num_close_matches = 5
Or via environment variables:

THEFUCK_RULES – list of enabled rules, like DEFAULT_RULES:rm_root or sudo:no_command;
THEFUCK_EXCLUDE_RULES – list of disabled rules, like git_pull:git_push;
THEFUCK_REQUIRE_CONFIRMATION – require confirmation before running new command, true/false;
THEFUCK_WAIT_COMMAND – max amount of time in seconds for getting previous command output;
THEFUCK_NO_COLORS – disable colored output, true/false;
THEFUCK_PRIORITY – priority of the rules, like no_command=9999:apt_get=100,
rule with lower priority will be matched first;
THEFUCK_DEBUG – enables debug output, true/false;
THEFUCK_HISTORY_LIMIT – how many history commands will be scanned, like 2000;
THEFUCK_ALTER_HISTORY – push fixed command to history true/false;
THEFUCK_WAIT_SLOW_COMMAND – max amount of time in seconds for getting previous command output if it in slow_commands list;
THEFUCK_SLOW_COMMANDS – list of slow commands, like lein:gradle;
THEFUCK_NUM_CLOSE_MATCHES – maximum number of close matches to suggest, like 5.

For example:
export THEFUCK_RULES='sudo:no_command'
export THEFUCK_EXCLUDE_RULES='git_pull:git_push'
export THEFUCK_REQUIRE_CONFIRMATION='true'
export THEFUCK_WAIT_COMMAND=10
export THEFUCK_NO_COLORS='false'
export THEFUCK_PRIORITY='no_command=9999:apt_get=100'
export THEFUCK_HISTORY_LIMIT='2000'
export THEFUCK_NUM_CLOSE_MATCHES='5'
Third-party packages with rules
If you'd like to make a specific set of non-public rules, but would still like
to share them with others, create a package named thefuck_contrib_* with
the following structure:
thefuck_contrib_foo
  thefuck_contrib_foo
    rules
      __init__.py
      *third-party rules*
    __init__.py
    *third-party-utils*
  setup.py

The Fuck will find rules located in the rules module.
Experimental instant mode
The default behavior of The Fuck requires time to re-run previous commands.
When in instant mode, The Fuck saves time by logging output with script,
then reading the log.

Currently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly.
To enable instant mode, add --enable-experimental-instant-mode
to the alias initialization in .bashrc, .bash_profile or .zshrc.
For example:
eval $(thefuck --alias --enable-experimental-instant-mode)
Developing
See CONTRIBUTING.md
License MIT
Project License can be found here.
"
37,Python,"Flask
Flask is a lightweight WSGI web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around Werkzeug
and Jinja and has become one of the most popular Python web
application frameworks.
Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.

Installing
Install and update using pip:
pip install -U Flask


A Simple Example
from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello, World!""
$ env FLASK_APP=hello.py flask run
 * Serving Flask app ""hello""
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)


Contributing
For guidance on setting up a development environment and how to make a
contribution to Flask, see the contributing guidelines.

Donate
The Pallets organization develops and supports Flask and the libraries
it uses. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, please
donate today.

Links

Website: https://palletsprojects.com/p/flask/
Documentation: https://flask.palletsprojects.com/
Releases: https://pypi.org/project/Flask/
Code: https://github.com/pallets/flask
Issue tracker: https://github.com/pallets/flask/issues
Test status: https://dev.azure.com/pallets/flask/_build
Official chat: https://discord.gg/t6rrQZH

"
38,Python,"Keras: Deep Learning for humans



You have just found Keras.
Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.
Use Keras if you need a deep learning library that:

Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).
Supports both convolutional networks and recurrent networks, as well as combinations of the two.
Runs seamlessly on CPU and GPU.

Read the documentation at Keras.io.
Keras is compatible with: Python 2.7-3.6.

Multi-backend Keras and tf.keras:
At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).
Keras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).
The current release is Keras 2.3.0, which makes significant API changes and add support for TensorFlow 2.0. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by tf.keras.
Bugs present in multi-backend Keras will only be fixed until April 2020 (as part of minor releases).
For more information about the future of Keras, see the Keras meeting notes.

Guiding principles


User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.


Modularity. A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions and regularization schemes are all standalone modules that you can combine to create new models.


Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.


Work with Python. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.



Getting started: 30 seconds to Keras
The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.
Here is the Sequential model:
from keras.models import Sequential

model = Sequential()
Stacking layers is as easy as .add():
from keras.layers import Dense

model.add(Dense(units=64, activation='relu', input_dim=100))
model.add(Dense(units=10, activation='softmax'))
Once your model looks good, configure its learning process with .compile():
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))
You can now iterate on your training data in batches:
# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.
model.fit(x_train, y_train, epochs=5, batch_size=32)
Alternatively, you can feed batches to your model manually:
model.train_on_batch(x_batch, y_batch)
Evaluate your performance in one line:
loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
Or generate predictions on new data:
classes = model.predict(x_test, batch_size=128)
Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?
For a more in-depth tutorial about Keras, you can check out:

Getting started with the Sequential model
Getting started with the functional API

In the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.

Installation
Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.

TensorFlow installation instructions.
Theano installation instructions.
CNTK installation instructions.

You may also consider installing the following optional dependencies:

cuDNN (recommended if you plan on running Keras on GPU).
HDF5 and h5py (required if you plan on saving Keras models to disk).
graphviz and pydot (used by visualization utilities to plot model graphs).

Then, you can install Keras itself. There are two ways to install Keras:

Install Keras from PyPI (recommended):

Note: These installation steps assume that you are on a Linux or Mac environment.
If you are on Windows, you will need to remove sudo to run the commands below.
sudo pip install keras
If you are using a virtualenv, you may want to avoid using sudo:
pip install keras

Alternatively: install Keras from the GitHub source:

First, clone Keras using git:
git clone https://github.com/keras-team/keras.git
Then, cd to the Keras folder and run the install command:
cd keras
sudo python setup.py install

Configuring your Keras backend
By default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.

Support
You can ask questions and join the development discussion:

On the Keras Google group.
On the Keras Slack channel. Use this link to request an invitation to the channel.

You can also post bug reports and feature requests (only) in GitHub issues. Make sure to read our guidelines first.

Why this name, Keras?
Keras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).
Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).

""Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them."" Homer, Odyssey 19. 562 ff (Shewring translation).


"
39,Python,"Django
Django is a high-level Python Web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.
All documentation is in the ""docs"" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

First, read docs/intro/install.txt for instructions on installing Django.
Next, work through the tutorials in order (docs/intro/tutorial01.txt,
docs/intro/tutorial02.txt, etc.).
If you want to set up an actual deployment server, read
docs/howto/deployment/index.txt for instructions.
You'll probably want to read through the topical guides (in docs/topics)
next; from there you can jump to the HOWTOs (in docs/howto) for specific
problems, and check out the reference (docs/ref) for gory details.
See docs/README for instructions on building an HTML version of the docs.

Docs are updated rigorously. If you find any problems in the docs, or think
they should be clarified in any way, please take 30 seconds to fill out a
ticket here: https://code.djangoproject.com/newticket
To get more help:

Join the #django channel on irc.freenode.net. Lots of helpful people hang
out there. See https://en.wikipedia.org/wiki/Wikipedia:IRC/Tutorial if you're
new to IRC.
Join the django-users mailing list, or read the archives, at
https://groups.google.com/group/django-users.

To contribute to Django:

Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for
information about getting involved.

To run Django's test suite:

Follow the instructions in the ""Unit tests"" section of
docs/internals/contributing/writing-code/unit-tests.txt, published online at
https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests

"
40,Python,"HTTPie: a CLI, cURL-like tool for humans
HTTPie (pronounced aitch-tee-tee-pie) is a command line HTTP client.
Its goal is to make CLI interaction with web services as human-friendly
as possible. It provides a simple http command that allows for sending
arbitrary HTTP requests using a simple and natural syntax, and displays
colorized output. HTTPie can be used for testing, debugging, and
generally interacting with HTTP servers.
 
   


Contents

1   Main features
2   Installation
2.1   macOS
2.2   Linux
2.3   Windows, etc.
2.4   Python version
2.5   Unstable version


3   Usage
3.1   Examples


4   HTTP method
5   Request URL
5.1   Querystring parameters
5.2   URL shortcuts for localhost
5.3   Other default schemes


6   Request items
6.1   Escaping rules


7   JSON
7.1   Default behaviour
7.2   Explicit JSON
7.3   Non-string JSON fields


8   Forms
8.1   Regular forms
8.2   File upload forms


9   HTTP headers
9.1   Default request headers
9.2   Empty headers and header un-setting
9.3   Limiting response headers


10   Cookies
11   Authentication
11.1   Basic auth
11.2   Digest auth
11.3   Password prompt
11.4   .netrc
11.5   Auth plugins


12   HTTP redirects
12.1   Follow Location
12.2   Showing intermediary redirect responses
12.3   Limiting maximum redirects followed


13   Proxies
13.1   Environment variables
13.2   SOCKS


14   HTTPS
14.1   Server SSL certificate verification
14.2   Custom CA bundle
14.3   Client side SSL certificate
14.4   SSL version


15   Output options
15.1   What parts of the HTTP exchange should be printed
15.2   Viewing intermediary requests/responses
15.3   Conditional body download


16   Redirected Input
16.1   Request data from a filename


17   Terminal output
17.1   Colors and formatting
17.2   Binary data


18   Redirected output
19   Download mode
19.1   Downloaded filename
19.2   Piping while downloading
19.3   Resuming downloads
19.4   Other notes


20   Streamed responses
20.1   Disabling buffering
20.2   Examples use cases


21   Sessions
21.1   Named sessions
21.2   Anonymous sessions
21.3   Readonly session


22   Config
22.1   Config file directory
22.2   Configurable options
22.2.1   default_options


22.3   Un-setting previously specified options


23   Scripting
23.1   Best practices


24   Meta
24.1   Interface design
24.2   User support
24.3   Related projects
24.3.1   Dependencies
24.3.2   HTTPie friends
24.3.3   Alternatives


24.4   Contributing
24.5   Change log
24.6   Artwork
24.7   Licence
24.8   Authors





1   Main features

Expressive and intuitive syntax
Formatted and colorized terminal output
Built-in JSON support
Forms and file uploads
HTTPS, proxies, and authentication
Arbitrary request data
Custom headers
Persistent sessions
Wget-like downloads
Linux, macOS and Windows support
Plugins
Documentation
Test coverage



2   Installation

2.1   macOS
On macOS, HTTPie can be installed via Homebrew
(recommended):
$ brew install httpie
A MacPorts port is also available:
$ port install httpie

2.2   Linux
Most Linux distributions provide a package that can be installed using the
system package manager, for example:
# Debian, Ubuntu, etc.
$ apt-get install httpie
# Fedora
$ dnf install httpie
# CentOS, RHEL, ...
$ yum install httpie
# Arch Linux
$ pacman -S httpie

2.3   Windows, etc.
A universal installation method (that works on Windows, Mac OS X, Linux, …,
and always provides the latest version) is to use pip:
# Make sure we have an up-to-date version of pip and setuptools:
$ pip install --upgrade pip setuptools

$ pip install --upgrade httpie
(If pip installation fails for some reason, you can try
easy_install httpie as a fallback.)

2.4   Python version
Starting with version 2.0.0 (currently under development) Python 3.6+ is required.

2.5   Unstable version
You can also install the latest unreleased development version directly from
the master branch on GitHub.  It is a work-in-progress of a future stable
release so the experience might be not as smooth.


On macOS you can install it with Homebrew:
$ brew install httpie --HEAD
Otherwise with pip:
$ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz
Verify that now we have the
current development version identifier
with the -dev suffix, for example:
$ http --version
1.0.0-dev

3   Usage
Hello World:
$ http httpie.org
Synopsis:
$ http [flags] [METHOD] URL [ITEM [ITEM]]
See also http --help.

3.1   Examples
Custom HTTP method, HTTP headers and JSON data:
$ http PUT example.org X-API-Token:123 name=John
Submitting forms:
$ http -f POST example.org hello=World
See the request that is being sent using one of the output options:
$ http -v example.org
Use Github API to post a comment on an
issue
with authentication:
$ http -a USERNAME POST https://api.github.com/repos/jakubroztocil/httpie/issues/83/comments body='HTTPie is awesome! :heart:'
Upload a file using redirected input:
$ http example.org < file.json
Download a file and save it via redirected output:
$ http example.org/file > file
Download a file wget style:
$ http --download example.org/file
Use named sessions to make certain aspects or the communication persistent
between requests to the same host:
$ http --session=logged-in -a username:password httpbin.org/get API-Key:123

$ http --session=logged-in httpbin.org/headers
Set a custom Host header to work around missing DNS records:
$ http localhost:8000 Host:example.com

4   HTTP method
The name of the HTTP method comes right before the URL argument:
$ http DELETE example.org/todos/7
Which looks similar to the actual Request-Line that is sent:
DELETE /todos/7 HTTP/1.1
When the METHOD argument is omitted from the command, HTTPie defaults to
either GET (with no request data) or POST (with request data).

5   Request URL
The only information HTTPie needs to perform a request is a URL.
The default scheme is, somewhat unsurprisingly, http://,
and can be omitted from the argument – http example.org works just fine.

5.1   Querystring parameters
If you find yourself manually constructing URLs with querystring parameters
on the terminal, you may appreciate the param==value syntax for appending
URL parameters.
With that, you don't have to worry about escaping the &
separators for your shell. Additionally, any special characters in the
parameter name or value get automatically URL-escaped
(as opposed to parameters specified in the full URL, which HTTPie doesn’t
modify).
$ http https://api.github.com/search/repositories q==httpie per_page==1
GET /search/repositories?q=httpie&per_page=1 HTTP/1.1

5.2   URL shortcuts for localhost
Additionally, curl-like shorthand for localhost is supported.
This means that, for example :3000 would expand to http://localhost:3000
If the port is omitted, then port 80 is assumed.
$ http :/foo
GET /foo HTTP/1.1
Host: localhost
$ http :3000/bar
GET /bar HTTP/1.1
Host: localhost:3000
$ http :
GET / HTTP/1.1
Host: localhost

5.3   Other default schemes
When HTTPie is invoked as https then the default scheme is https://
($ https example.org will make a request to https://example.org).
You can also use the --default-scheme <URL_SCHEME> option to create
shortcuts for other protocols than HTTP (possibly supported via plugins).
Example for the httpie-unixsocket plugin:
# Before
$ http http+unix://%2Fvar%2Frun%2Fdocker.sock/info
# Create an alias
$ alias http-unix='http --default-scheme=""http+unix""'
# Now the scheme can be omitted
$ http-unix %2Fvar%2Frun%2Fdocker.sock/info

6   Request items
There are a few different request item types that provide a
convenient mechanism for specifying HTTP headers, simple JSON and
form data, files, and URL parameters.
They are key/value pairs specified after the URL. All have in
common that they become part of the actual request that is sent and that
their type is distinguished only by the separator used:
:, =, :=, ==, @, =@, and :=@. The ones with an
@ expect a file path as value.


Item Type
Description



HTTP Headers
Name:Value
Arbitrary HTTP header, e.g. X-API-Token:123.

URL parameters
name==value
Appends the given name/value pair as a query
string parameter to the URL.
The == separator is used.

Data Fields
field=value,
field=@file.txt
Request data fields to be serialized as a JSON
object (default), or to be form-encoded
(--form, -f).

Raw JSON fields
field:=json,
field:=@file.json
Useful when sending JSON and one or
more fields need to be a Boolean, Number,
nested Object, or an Array,  e.g.,
meals:='[""ham"",""spam""]' or pies:=[1,2,3]
(note the quotes).

Form File Fields
field@/dir/file
Only available with --form, -f.
For example screenshot@~/Pictures/img.png.
The presence of a file field results
in a multipart/form-data request.



Note that data fields aren't the only way to specify request data:
Redirected input is a mechanism for passing arbitrary request data.

6.1   Escaping rules
You can use \ to escape characters that shouldn't be used as separators
(or parts thereof). For instance, foo\==bar will become a data key/value
pair (foo= and bar) instead of a URL parameter.
Often it is necessary to quote the values, e.g. foo='bar baz'.
If any of the field names or headers starts with a minus
(e.g., -fieldname), you need to place all such items after the special
token -- to prevent confusion with --arguments:
$ http httpbin.org/post  --  -name-starting-with-dash=foo -Unusual-Header:bar
POST /post HTTP/1.1
-Unusual-Header: bar
Content-Type: application/json

{
    ""-name-starting-with-dash"": ""foo""
}

7   JSON
JSON is the lingua franca of modern web services and it is also the
implicit content type HTTPie uses by default.
Simple example:
$ http PUT example.org name=John email=john@example.org
PUT / HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: example.org

{
    ""name"": ""John"",
    ""email"": ""john@example.org""
}

7.1   Default behaviour
If your command includes some data request items, they are serialized as a JSON
object by default. HTTPie also automatically sets the following headers,
both of which can be overwritten:


Content-Type
application/json

Accept
application/json, */*




7.2   Explicit JSON
You can use --json, -j to explicitly set Accept
to application/json regardless of whether you are sending data
(it's a shortcut for setting the header via the usual header notation:
http url Accept:'application/json, */*'). Additionally,
HTTPie will try to detect JSON responses even when the
Content-Type is incorrectly text/plain or unknown.

7.3   Non-string JSON fields
Non-string fields use the := separator, which allows you to embed raw JSON
into the resulting object. Text and raw JSON files can also be embedded into
fields using =@ and :=@:
$ http PUT api.example.com/person/1 \
    name=John \
    age:=29 married:=false hobbies:='[""http"", ""pies""]' \  # Raw JSON
    description=@about-john.txt \   # Embed text file
    bookmarks:=@bookmarks.json      # Embed JSON file
PUT /person/1 HTTP/1.1
Accept: application/json, */*
Content-Type: application/json
Host: api.example.com

{
    ""age"": 29,
    ""hobbies"": [
        ""http"",
        ""pies""
    ],
    ""description"": ""John is a nice guy who likes pies."",
    ""married"": false,
    ""name"": ""John"",
    ""bookmarks"": {
        ""HTTPie"": ""https://httpie.org"",
    }
}
Please note that with this syntax the command gets unwieldy when sending
complex data. In that case it's always better to use redirected input:
$ http POST api.example.com/person/1 < person.json

8   Forms
Submitting forms is very similar to sending JSON requests. Often the only
difference is in adding the --form, -f option, which ensures that
data fields are serialized as, and Content-Type is set to,
application/x-www-form-urlencoded; charset=utf-8. It is possible to make
form data the implicit content type instead of JSON
via the config file.

8.1   Regular forms
$ http --form POST api.example.org/person/1 name='John Smith'
POST /person/1 HTTP/1.1
Content-Type: application/x-www-form-urlencoded; charset=utf-8

name=John+Smith

8.2   File upload forms
If one or more file fields is present, the serialization and content type is
multipart/form-data:
$ http -f POST example.com/jobs name='John Smith' cv@~/Documents/cv.pdf
The request above is the same as if the following HTML form were
submitted:
<form enctype=""multipart/form-data"" method=""post"" action=""http://example.com/jobs"">
    <input type=""text"" name=""name"" />
    <input type=""file"" name=""cv"" />
</form>
Note that @ is used to simulate a file upload form field, whereas
=@ just embeds the file content as a regular text field value.

9   HTTP headers
To set custom headers you can use the Header:Value notation:
$ http example.org  User-Agent:Bacon/1.0  'Cookie:valued-visitor=yes;foo=bar'  \
    X-Foo:Bar  Referer:https://httpie.org/
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Cookie: valued-visitor=yes;foo=bar
Host: example.org
Referer: https://httpie.org/
User-Agent: Bacon/1.0
X-Foo: Bar

9.1   Default request headers
There are a couple of default headers that HTTPie sets:
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
User-Agent: HTTPie/<version>
Host: <taken-from-URL>
Any of these except Host can be overwritten and some of them unset.

9.2   Empty headers and header un-setting
To unset a previously specified header
(such a one of the default headers), use Header::
$ http httpbin.org/headers Accept: User-Agent:
To send a header with an empty value, use Header;:
$ http httpbin.org/headers 'Header;'

9.3   Limiting response headers
The --max-headers=n options allows you to control the number of headers
HTTPie reads before giving up (the default 0, i.e., there’s no limit).
$ http --max-headers=100 httpbin.org/get

10   Cookies
HTTP clients send cookies to the server as regular HTTP headers. That means,
HTTPie does not offer any special syntax for specifying cookies — the usual
Header:Value notation is used:
Send a single cookie:
$ http example.org Cookie:sessionid=foo
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo
Host: example.org
User-Agent: HTTPie/0.9.9
Send multiple cookies
(note the header is quoted to prevent the shell from interpreting the ;):
$ http example.org 'Cookie:sessionid=foo;another-cookie=bar'
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo;another-cookie=bar
Host: example.org
User-Agent: HTTPie/0.9.9
If you often deal with cookies in your requests, then chances are you'd appreciate
the sessions feature.

11   Authentication
The currently supported authentication schemes are Basic and Digest
(see auth plugins for more). There are two flags that control authentication:


--auth, -a
Pass a username:password pair as
the argument. Or, if you only specify a username
(-a username), you'll be prompted for
the password before the request is sent.
To send an empty password, pass username:.
The username:password@hostname URL syntax is
supported as well (but credentials passed via -a
have higher priority).

--auth-type, -A
Specify the auth mechanism. Possible values are
basic and digest. The default value is
basic so it can often be omitted.




11.1   Basic auth
$ http -a username:password example.org

11.2   Digest auth
$ http -A digest -a username:password example.org

11.3   Password prompt
$ http -a username example.org

11.4   .netrc
Authentication information from your ~/.netrc
file is by default honored as well.
For example:
$ cat ~/.netrc
machine httpbin.org
login httpie
password test
$ http httpbin.org/basic-auth/httpie/test
HTTP/1.1 200 OK
[...]
This can be disabled with the --ignore-netrc option:
$ http --ignore-netrc httpbin.org/basic-auth/httpie/test
HTTP/1.1 401 UNAUTHORIZED
[...]

11.5   Auth plugins
Additional authentication mechanism can be installed as plugins.
They can be found on the Python Package Index.
Here's a few picks:

httpie-api-auth: ApiAuth
httpie-aws-auth: AWS / Amazon S3
httpie-edgegrid: EdgeGrid
httpie-hmac-auth: HMAC
httpie-jwt-auth: JWTAuth (JSON Web Tokens)
httpie-negotiate: SPNEGO (GSS Negotiate)
httpie-ntlm: NTLM (NT LAN Manager)
httpie-oauth: OAuth
requests-hawk: Hawk


12   HTTP redirects
By default, HTTP redirects are not followed and only the first
response is shown:
$ http httpbin.org/redirect/3

12.1   Follow Location
To instruct HTTPie to follow the Location header of 30x responses
and show the final response instead, use the --follow, -F option:
$ http --follow httpbin.org/redirect/3

12.2   Showing intermediary redirect responses
If you additionally wish to see the intermediary requests/responses,
then use the --all option as well:
$ http --follow --all httpbin.org/redirect/3

12.3   Limiting maximum redirects followed
To change the default limit of maximum 30 redirects, use the
--max-redirects=<limit> option:
$ http --follow --all --max-redirects=5 httpbin.org/redirect/3

13   Proxies
You can specify proxies to be used through the --proxy argument for each
protocol (which is included in the value in case of redirects across protocols):
$ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org
With Basic authentication:
$ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org

13.1   Environment variables
You can also configure proxies by environment variables ALL_PROXY,
HTTP_PROXY and HTTPS_PROXY, and the underlying Requests library will
pick them up as well. If you want to disable proxies configured through
the environment variables for certain hosts, you can specify them in NO_PROXY.
In your ~/.bash_profile:
export HTTP_PROXY=http://10.10.1.10:3128
export HTTPS_PROXY=https://10.10.1.10:1080
export NO_PROXY=localhost,example.com

13.2   SOCKS
Homebrew-installed HTTPie comes with SOCKS proxy support out of the box.
To enable SOCKS proxy support for non-Homebrew  installations, you'll
might need to install requests[socks] manually using pip:
$ pip install -U requests[socks]
Usage is the same as for other types of proxies:
$ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org

14   HTTPS

14.1   Server SSL certificate verification
To skip the host's SSL certificate verification, you can pass --verify=no
(default is yes):
$ http --verify=no https://example.org

14.2   Custom CA bundle
You can also use --verify=<CA_BUNDLE_PATH> to set a custom CA bundle path:
$ http --verify=/ssl/custom_ca_bundle https://example.org

14.3   Client side SSL certificate
To use a client side certificate for the SSL communication, you can pass
the path of the cert file with --cert:
$ http --cert=client.pem https://example.org
If the private key is not contained in the cert file you may pass the
path of the key file with --cert-key:
$ http --cert=client.crt --cert-key=client.key https://example.org

14.4   SSL version
Use the --ssl=<PROTOCOL> to specify the desired protocol version to use.
This will default to SSL v2.3 which will negotiate the highest protocol that both
the server and your installation of OpenSSL support. The available protocols
are ssl2.3, ssl3, tls1, tls1.1, tls1.2, tls1.3. (The actually
available set of protocols may vary depending on your OpenSSL installation.)
# Specify the vulnerable SSL v3 protocol to talk to an outdated server:
$ http --ssl=ssl3 https://vulnerable.example.org

15   Output options
By default, HTTPie only outputs the final response and the whole response
message is printed (headers as well as the body). You can control what should
be printed via several options:


--headers, -h
Only the response headers are printed.

--body, -b
Only the response body is printed.

--verbose, -v
Print the whole HTTP exchange (request and response).
This option also enables --all (see below).

--print, -p
Selects parts of the HTTP exchange.



--verbose can often be useful for debugging the request and generating
documentation examples:
$ http --verbose PUT httpbin.org/put hello=world
PUT /put HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: httpbin.org
User-Agent: HTTPie/0.2.7dev

{
    ""hello"": ""world""
}


HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 477
Content-Type: application/json
Date: Sun, 05 Aug 2012 00:25:23 GMT
Server: gunicorn/0.13.4

{
    […]
}

15.1   What parts of the HTTP exchange should be printed
All the other output options are under the hood just shortcuts for
the more powerful --print, -p. It accepts a string of characters each
of which represents a specific part of the HTTP exchange:


Character
Stands for



H
request headers

B
request body

h
response headers

b
response body



Print request and response headers:
$ http --print=Hh PUT httpbin.org/put hello=world

15.2   Viewing intermediary requests/responses
To see all the HTTP communication, i.e. the final request/response as
well as any possible  intermediary requests/responses, use the --all
option. The intermediary HTTP communication include followed redirects
(with --follow), the first unauthorized request when HTTP digest
authentication is used (--auth=digest), etc.
# Include all responses that lead to the final one:
$ http --all --follow httpbin.org/redirect/3
The intermediary requests/response are by default formatted according to
--print, -p (and its shortcuts described above). If you'd like to change
that, use the --history-print, -P option. It takes the same
arguments as --print, -p but applies to the intermediary requests only.
# Print the intermediary requests/responses differently than the final one:
$ http -A digest -a foo:bar --all -p Hh -P H httpbin.org/digest-auth/auth/foo/bar

15.3   Conditional body download
As an optimization, the response body is downloaded from the server
only if it's part of the output. This is similar to performing a HEAD
request, except that it applies to any HTTP method you use.
Let's say that there is an API that returns the whole resource when it is
updated, but you are only interested in the response headers to see the
status code after an update:
$ http --headers PATCH example.org/Really-Huge-Resource name='New Name'
Since we are only printing the HTTP headers here, the connection to the server
is closed as soon as all the response headers have been received.
Therefore, bandwidth and time isn't wasted downloading the body
which you don't care about. The response headers are downloaded always,
even if they are not part of the output

16   Redirected Input
The universal method for passing request data is through redirected stdin
(standard input)—piping. Such data is buffered and then with no further
processing used as the request body. There are multiple useful ways to use
piping:
Redirect from a file:
$ http PUT example.com/person/1 X-API-Token:123 < person.json
Or the output of another program:
$ grep '401 Unauthorized' /var/log/httpd/error_log | http POST example.org/intruders
You can use echo for simple data:
$ echo '{""name"": ""John""}' | http PATCH example.com/person/1 X-API-Token:123
You can also use a Bash here string:
$ http example.com/ <<<'{""name"": ""John""}'
You can even pipe web services together using HTTPie:
$ http GET https://api.github.com/repos/jakubroztocil/httpie | http POST httpbin.org/post
You can use cat to enter multiline data on the terminal:
$ cat | http POST example.com
<paste>
^D
$ cat | http POST example.com/todos Content-Type:text/plain
- buy milk
- call parents
^D
On OS X, you can send the contents of the clipboard with pbpaste:
$ pbpaste | http PUT example.com
Passing data through stdin cannot be combined with data fields specified
on the command line:
$ echo 'data' | http POST example.org more=data   # This is invalid
To prevent HTTPie from reading stdin data you can use the
--ignore-stdin option.

16.1   Request data from a filename
An alternative to redirected stdin is specifying a filename (as
@/path/to/file) whose content is used as if it came from stdin.
It has the advantage that the Content-Type
header is automatically set to the appropriate value based on the
filename extension. For example, the following request sends the
verbatim contents of that XML file with Content-Type: application/xml:
$ http PUT httpbin.org/put @/data/file.xml

17   Terminal output
HTTPie does several things by default in order to make its terminal output
easy to read.

17.1   Colors and formatting
Syntax highlighting is applied to HTTP headers and bodies (where it makes
sense). You can choose your preferred color scheme via the --style option
if you don't like the default one (see $ http --help for the possible
values).
Also, the following formatting is applied:

HTTP headers are sorted by name.
JSON data is indented, sorted by keys, and unicode escapes are converted
to the characters they represent.

One of these options can be used to control output processing:


--pretty=all
Apply both colors and formatting.
Default for terminal output.

--pretty=colors
Apply colors.

--pretty=format
Apply formatting.

--pretty=none
Disables output processing.
Default for redirected output.




17.2   Binary data
Binary data is suppressed for terminal output, which makes it safe to perform
requests to URLs that send back binary data. Binary data is suppressed also in
redirected, but prettified output. The connection is closed as soon as we know
that the response body is binary,
$ http example.org/Movie.mov
You will nearly instantly see something like this:
HTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Encoding: gzip
Content-Type: video/quicktime
Transfer-Encoding: chunked

+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

18   Redirected output
HTTPie uses a different set of defaults for redirected output than for
terminal output. The differences being:

Formatting and colors aren't applied (unless --pretty is specified).
Only the response body is printed (unless one of the output options is set).
Also, binary data isn't suppressed.

The reason is to make piping HTTPie's output to another programs and
downloading files work with no extra flags. Most of the time, only the raw
response body is of an interest when the output is redirected.
Download a file:
$ http example.org/Movie.mov > Movie.mov
Download an image of Octocat, resize it using ImageMagick, upload it elsewhere:
$ http octodex.github.com/images/original.jpg | convert - -resize 25% -  | http example.org/Octocats
Force colorizing and formatting, and show both the request and the response in
less pager:
$ http --pretty=all --verbose example.org | less -R
The -R flag tells less to interpret color escape sequences included
HTTPie`s output.
You can create a shortcut for invoking HTTPie with colorized and paged output
by adding the following to your ~/.bash_profile:
function httpless {
    # `httpless example.org'
    http --pretty=all --print=hb ""$@"" | less -R;
}

19   Download mode
HTTPie features a download mode in which it acts similarly to wget.
When enabled using the --download, -d flag, response headers are printed to
the terminal (stderr), and a progress bar is shown while the response body
is being saved to a file.
$ http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz
HTTP/1.1 200 OK
Content-Disposition: attachment; filename=httpie-master.tar.gz
Content-Length: 257336
Content-Type: application/x-gzip

Downloading 251.30 kB to ""httpie-master.tar.gz""
Done. 251.30 kB in 2.73862s (91.76 kB/s)

19.1   Downloaded filename
There are three mutually exclusive ways through which HTTPie determines
the output filename (with decreasing priority):

You can explicitly provide it via --output, -o.
The file gets overwritten if it already exists
(or appended to with --continue, -c).
The server may specify the filename in the optional Content-Disposition
response header. Any leading dots are stripped from a server-provided filename.
The last resort HTTPie uses is to generate the filename from a combination
of the request URL and the response Content-Type.
The initial URL is always used as the basis for
the generated filename — even if there has been one or more redirects.

To prevent data loss by overwriting, HTTPie adds a unique numerical suffix to the
filename when necessary (unless specified with --output, -o).

19.2   Piping while downloading
You can also redirect the response body to another program while the response
headers and progress are still shown in the terminal:
$ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz |  tar zxf -

19.3   Resuming downloads
If --output, -o is specified, you can resume a partial download using the
--continue, -c option. This only works with servers that support
Range requests and 206 Partial Content responses. If the server doesn't
support that, the whole file will simply be downloaded:
$ http -dco file.zip example.org/file

19.4   Other notes

The --download option only changes how the response body is treated.
You can still set custom headers, use sessions, --verbose, -v, etc.
--download always implies --follow (redirects are followed).
HTTPie exits with status code 1 (error) if the body hasn't been fully
downloaded.
Accept-Encoding cannot be set with --download.


20   Streamed responses
Responses are downloaded and printed in chunks which allows for streaming
and large file downloads without using too much memory. However, when
colors and formatting is applied, the whole response is buffered and only
then processed at once.

20.1   Disabling buffering
You can use the --stream, -S flag to make two things happen:

The output is flushed in much smaller chunks without any buffering,
which makes HTTPie behave kind of like tail -f for URLs.
Streaming becomes enabled even when the output is prettified: It will be
applied to each line of the response and flushed immediately. This makes
it possible to have a nice output for long-lived requests, such as one
to the Twitter streaming API.


20.2   Examples use cases
Prettified streamed response:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track='Justin Bieber'
Streamed output by small chunks alá tail -f:
# Send each new tweet (JSON object) mentioning ""Apple"" to another
# server as soon as it arrives from the Twitter streaming API:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track=Apple \
| while read tweet; do echo ""$tweet"" | http POST example.org/tweets ; done

21   Sessions
By default, every request HTTPie makes is completely independent of any
previous ones to the same host.
However, HTTPie also supports persistent
sessions via the --session=SESSION_NAME_OR_PATH option. In a session,
custom HTTP headers (except for the ones starting with Content- or If-),
authentication, and cookies
(manually specified or sent by the server) persist between requests
to the same host.
# Create a new session
$ http --session=/tmp/session.json example.org API-Token:123

# Re-use an existing session — API-Token will be set:
$ http --session=/tmp/session.json example.org
All session data, including credentials, cookie data,
and custom headers are stored in plain text.
That means session files can also be created and edited manually in a text
editor—they are regular JSON. It also means that they can be read by anyone
who has access to the session file.

21.1   Named sessions
You can create one or more named session per host. For example, this is how
you can create a new session named user1 for example.org:
$ http --session=user1 -a user1:password example.org X-Foo:Bar
From now on, you can refer to the session by its name. When you choose to
use the session again, any previously specified authentication or HTTP headers
will automatically be set:
$ http --session=user1 example.org
To create or reuse a different session, simple specify a different name:
$ http --session=user2 -a user2:password example.org X-Bar:Foo
Named sessions’s data is stored in JSON files in the the sessions
subdirectory of the config directory:
~/.httpie/sessions/<host>/<name>.json
(%APPDATA%\httpie\sessions\<host>\<name>.json on Windows).

21.2   Anonymous sessions
Instead of a name, you can also directly specify a path to a session file. This
allows for sessions to be re-used across multiple hosts:
$ http --session=/tmp/session.json example.org
$ http --session=/tmp/session.json admin.example.org
$ http --session=~/.httpie/sessions/another.example.org/test.json example.org
$ http --session-read-only=/tmp/session.json example.org

21.3   Readonly session
To use an existing session file without updating it from the request/response
exchange once it is created, specify the session name via
--session-read-only=SESSION_NAME_OR_PATH instead.

22   Config
HTTPie uses a simple config.json file. The file doesn’t exist by default
but you can create it manually.

22.1   Config file directory
The default location of the configuration file is ~/.httpie/config.json
(or %APPDATA%\httpie\config.json on Windows).
The config directory can be changed by setting the $HTTPIE_CONFIG_DIR
environment variable:
$ export HTTPIE_CONFIG_DIR=/tmp/httpie
$ http example.org
To view the exact location run http --debug.

22.2   Configurable options
Currently HTTPie offers a single configurable option:

22.2.1   default_options
An Array (by default empty) of default options that should be applied to
every invocation of HTTPie.
For instance, you can use this config option to change your default color theme:
$ cat ~/.httpie/config.json
{
    ""default_options"": [
      ""--style=fruity""
    ]
}
Even though it is technically possible to include there any of HTTPie’s
options, it is not recommended to modify the default behaviour in a way
that would break your compatibility with the wider world as that can
generate a lot of confusion.

22.3   Un-setting previously specified options
Default options from the config file, or specified any other way,
can be unset for a particular invocation via --no-OPTION arguments passed
on the command line (e.g., --no-style or --no-session).

23   Scripting
When using HTTPie from shell scripts, it can be handy to set the
--check-status flag. It instructs HTTPie to exit with an error if the
HTTP status is one of 3xx, 4xx, or 5xx. The exit status will
be 3 (unless --follow is set), 4, or 5,
respectively.
#!/bin/bash

if http --check-status --ignore-stdin --timeout=2.5 HEAD example.org/health &> /dev/null; then
    echo 'OK!'
else
    case $? in
        2) echo 'Request timed out!' ;;
        3) echo 'Unexpected HTTP 3xx Redirection!' ;;
        4) echo 'HTTP 4xx Client Error!' ;;
        5) echo 'HTTP 5xx Server Error!' ;;
        6) echo 'Exceeded --max-redirects=<n> redirects!' ;;
        *) echo 'Other Error!' ;;
    esac
fi

23.1   Best practices
The default behaviour of automatically reading stdin is typically not
desirable during non-interactive invocations. You most likely want to
use the --ignore-stdin option to disable it.
It is a common gotcha that without this option HTTPie seemingly hangs.
What happens is that when HTTPie is invoked for example from a cron job,
stdin is not connected to a terminal.
Therefore, rules for redirected input apply, i.e., HTTPie starts to read it
expecting that the request body will be passed through.
And since there's no data nor EOF, it will be stuck. So unless you're
piping some data to HTTPie, this flag should be used in scripts.
Also, it might be good to set a connection --timeout limit to prevent
your program from hanging if the server never responds.

24   Meta

24.1   Interface design
The syntax of the command arguments closely corresponds to the actual HTTP
requests sent over the wire. It has the advantage  that it's easy to remember
and read. It is often possible to translate an HTTP request to an HTTPie
argument list just by inlining the request elements. For example, compare this
HTTP request:
POST /collection HTTP/1.1
X-API-Key: 123
User-Agent: Bacon/1.0
Content-Type: application/x-www-form-urlencoded

name=value&name2=value2
with the HTTPie command that sends it:
$ http -f POST example.org/collection \
  X-API-Key:123 \
  User-Agent:Bacon/1.0 \
  name=value \
  name2=value2
Notice that both the order of elements and the syntax is very similar,
and that only a small portion of the command is used to control HTTPie and
doesn't directly correspond to any part of the request (here it's only -f
asking HTTPie to send a form request).
The two modes, --pretty=all (default for terminal) and --pretty=none
(default for redirected output), allow for both user-friendly interactive use
and usage from scripts, where HTTPie serves as a generic HTTP client.
As HTTPie is still under heavy development, the existing command line
syntax and some of the --OPTIONS may change slightly before
HTTPie reaches its final version 1.0. All changes are recorded in the
change log.

24.2   User support
Please use the following support channels:

GitHub issues
for bug reports and feature requests.
Our Gitter chat room
to ask questions, discuss features, and for general discussion.
StackOverflow
to ask questions (please make sure to use the
httpie tag).
Tweet directly to @clihttp.
You can also tweet directly to @jakubroztocil.


24.3   Related projects

24.3.1   Dependencies
Under the hood, HTTPie uses these two amazing libraries:

Requests
— Python HTTP library for humans
Pygments
— Python syntax highlighter


24.3.2   HTTPie friends
HTTPie plays exceptionally well with the following tools:

jq
— CLI JSON processor that
works great in conjunction with HTTPie
http-prompt
—  interactive shell for HTTPie featuring autocomplete
and command syntax highlighting


24.3.3   Alternatives

httpcat — a lower-level sister utility
of HTTPie for constructing raw HTTP requests on the command line.
curl — a ""Swiss knife"" command line tool and
an exceptional library for transferring data with URLs.


24.4   Contributing
See CONTRIBUTING.rst.

24.5   Change log
See CHANGELOG.

24.6   Artwork

Logo by Cláudia Delgado.
Animation by Allen Smith of GitHub.


24.7   Licence
BSD-3-Clause: LICENSE.

24.8   Authors
Jakub Roztocil  (@jakubroztocil) created HTTPie and these fine people
have contributed.
"
41,Python,"Awesome Machine Learning 
A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by awesome-php.
If you want to contribute to this list (please do), send me a pull request or contact me @josephmisiti.
Also, a listed repository should be deprecated if:

Repository's owner explicitly say that ""this library is not maintained"".
Not committed for long time (2~3 years).

Further resources:


For a list of free machine learning books available for download, go here.


For a list of (mostly) free machine learning courses available online, go here.


For a list of blogs and newsletters on data science and machine learning, go here.


For a list of free-to-attend meetups and local events, go here.


Table of Contents
Frameworks and Libraries

Awesome Machine Learning 

Table of Contents

Frameworks and Libraries
Tools


APL

General-Purpose Machine Learning


C

General-Purpose Machine Learning
Computer Vision


C++

Computer Vision
General-Purpose Machine Learning
Natural Language Processing
Speech Recognition
Sequence Analysis
Gesture Detection


Common Lisp

General-Purpose Machine Learning


Clojure

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Crystal

General-Purpose Machine Learning


Elixir

General-Purpose Machine Learning
Natural Language Processing


Erlang

General-Purpose Machine Learning


Go

Natural Language Processing
General-Purpose Machine Learning
Spatial analysis and geometry
Data Analysis / Data Visualization
Computer vision


Haskell

General-Purpose Machine Learning


Java

Natural Language Processing
General-Purpose Machine Learning
Speech Recognition
Data Analysis / Data Visualization
Deep Learning


Javascript

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning
Misc
Demos and Scripts


Julia

General-Purpose Machine Learning
Natural Language Processing
Data Analysis / Data Visualization
Misc Stuff / Presentations


Lua

General-Purpose Machine Learning
Demos and Scripts


Matlab

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


.NET

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Objective C

General-Purpose Machine Learning


OCaml

General-Purpose Machine Learning


Perl

Data Analysis / Data Visualization
General-Purpose Machine Learning


Perl 6

Data Analysis / Data Visualization
General-Purpose Machine Learning


PHP

Natural Language Processing
General-Purpose Machine Learning


Python

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc Scripts / iPython Notebooks / Codebases
Neural Networks
Kaggle Competition Source Code
Reinforcement Learning


Ruby

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc


Rust

General-Purpose Machine Learning


R

General-Purpose Machine Learning
Data Analysis / Data Visualization


SAS

General-Purpose Machine Learning
Data Analysis / Data Visualization
Natural Language Processing
Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning


Tools

Neural Networks
Misc


Credits
写个脚本把它们爬下来 - Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning



Tools

Neural Networks
Misc

Credits

APL

General-Purpose Machine Learning

naive-apl - Naive Bayesian Classifier implementation in APL. [Deprecated]


C

General-Purpose Machine Learning

Darknet - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.
Recommender - A C library for product recommendations/suggestions using collaborative filtering (CF).
Hybrid Recommender System - A hybrid recommender system based upon scikit-learn algorithms. [Deprecated]
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.


Computer Vision

CCV - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.
VLFeat - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox.


C++

Computer Vision

DLib - DLib has C++ and Python interfaces for face detection and training general object detectors.
EBLearn - Eblearn is an object-oriented C++ library that implements various machine learning models [Deprecated]
OpenCV - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.
VIGRA - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.


General-Purpose Machine Learning

BanditLib - A simple Multi-armed Bandit library. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.
CNTK - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.
CUDA - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]
DeepDetect - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.
Distributed Machine learning Tool Kit (DMTK) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.
DLib - A suite of ML tools designed to be easy to imbed in other applications.
DSSTNE - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.
DyNet - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.
Fido - A highly-modular C++ machine learning library for embedded electronics and robotics.
igraph - General purpose graph library.
Intel(R) DAAL - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.
LightGBM - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
libfm - A generic approach that allows to mimic most factorization models by feature engineering.
MLDB - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.
mlpack - A scalable C++ machine learning library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
proNet-core - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.
PyCUDA - Python interface to CUDA
ROOT - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.
shark - A fast, modular, feature-rich open-source C++ machine learning library.
Shogun - The Shogun Machine Learning Toolbox.
sofia-ml - Suite of fast incremental algorithms.
Stan - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.
Timbl - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.
Vowpal Wabbit (VW) - A fast out-of-core learning system.
Warp-CTC - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.
XGBoost - A parallelized optimized general purpose gradient boosting library.
ThunderGBM - A fast library for GBDTs and Random Forests on GPUs.
ThunderSVM - A fast SVM library on GPUs and CPUs.
LKYDeepNN - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
Featuretools - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering ""primitives"".
skynet - A library for learning neural network, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.
Feast - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.
Polyaxon - A platform for reproducible and scalable machine learning and deep learning.


Natural Language Processing

BLLIP Parser - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).
colibri-core - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
CRF++ - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. [Deprecated]
CRFsuite - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. [Deprecated]
frog - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.
libfolia - C++ library for the FoLiA format
MeTA - MeTA : ModErn Text Analysis is a C++ Data Sciences Toolkit that facilitates mining big text data.
MIT Information Extraction Toolkit - C, C++, and Python tools for named entity recognition and relation extraction
ucto - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.


Speech Recognition

Kaldi - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.


Sequence Analysis

ToPS - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. [Deprecated]


Gesture Detection

grt - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.


Common Lisp

General-Purpose Machine Learning

mgl - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.
mgl-gpr - Evolutionary algorithms. [Deprecated]
cl-libsvm - Wrapper for the libsvm support vector machine library. [Deprecated]
cl-online-learning - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).
cl-random-forest - Implementation of Random Forest in Common Lisp.


Clojure

Natural Language Processing

Clojure-openNLP - Natural Language Processing in Clojure (opennlp).
Infections-clj - Rails-like inflection library for Clojure and ClojureScript.


General-Purpose Machine Learning

Touchstone - Clojure A/B testing library. [Deprecated]
Clojush - The Push programming language and the PushGP genetic programming system implemented in Clojure.
Infer - Inference and machine learning in Clojure. [Deprecated]
Clj-ML - A machine learning library for Clojure built on top of Weka and friends. [Deprecated]
DL4CLJ - Clojure wrapper for Deeplearning4j.
Encog - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). [Deprecated]
Fungp - A genetic programming library for Clojure. [Deprecated]
Statistiker - Basic Machine Learning algorithms in Clojure. [Deprecated]
clortex - General Machine Learning library using Numenta’s Cortical Learning Algorithm. [Deprecated]
comportex - Functionally composable Machine Learning library using Numenta’s Cortical Learning Algorithm. [Deprecated]
cortex - Neural networks, regression and feature learning in Clojure.
lambda-ml - Simple, concise implementations of machine learning techniques and utilities in Clojure.


Data Analysis / Data Visualization

Incanter - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.
PigPen - Map-Reduce for Clojure.
Envision - Clojure Data Visualisation library, based on Statistiker and D3.


Crystal

General-Purpose Machine Learning

machine - Simple machine learning algorithm.
crystal-fann - FANN (Fast Artificial Neural Network) binding.


Elixir

General-Purpose Machine Learning

Simple Bayes - A Simple Bayes / Naive Bayes implementation in Elixir.
emel - A simple and functional machine learning library written in Elixir.
Tensorflex - Tensorflow bindings for the Elixir programming language.


Natural Language Processing

Stemmer - An English (Porter2) stemming implementation in Elixir.


Erlang

General-Purpose Machine Learning

Disco - Map Reduce in Erlang. [Deprecated]
Yanni - ANN neural networks using Erlangs leightweight processes.


Go

Natural Language Processing

snowball - Snowball Stemmer for Go.
word-embedding - Word Embeddings: the full implementation of word2vec, GloVe in Go.
sentences - Golang implementation of Punkt sentence tokenizer.
go-ngram - In-memory n-gram index with compression. [Deprecated]
paicehusk - Golang implementation of the Paice/Husk Stemming Algorithm. [Deprecated]
go-porterstemmer - A native Go clean room implementation of the Porter Stemming algorithm. [Deprecated]


General-Purpose Machine Learning

birdland - A recommendation library in Go.
eaopt - An evolutionary optimization library.
leaves - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.
gobrain - Neural Networks written in Go.
go-mxnet-predictor - Go binding for MXNet c_predict_api to do inference with pre-trained model.
go-ml-transpiler - An open source Go transpiler for machine learning models.
golearn - Machine learning for Go.
goml - Machine learning library written in pure Go.
gorgonia - Deep learning in Go.
gorse - An offline recommender system backend based on collaborative filtering written in Go.
therfoo - An embedded deep learning library for Go.
neat - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). [Deprecated]
go-pr - Pattern recognition package in Go lang. [Deprecated]
go-ml - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. [Deprecated]
GoNN - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. [Deprecated]
bayesian - Naive Bayesian Classification for Golang. [Deprecated]
go-galib - Genetic Algorithms library written in Go / Golang. [Deprecated]
Cloudforest - Ensembles of decision trees in Go/Golang. [Deprecated]
go-dnn - Deep Neural Networks for Golang (powered by MXNet)


Spatial analysis and geometry

go-geom - Go library to handle geometries.
gogeo - Spherical geometry in Go.


Data Analysis / Data Visualization

gota - Dataframes.
gonum/mat - A linear algebra package for Go.
gonum/optimize - Implementations of optimization algorithms.
gonum/plot - A plotting library.
gonum/stat - A statistics library.
SVGo - The Go Language library for SVG generation.
glot - Glot is a plotting library for Golang built on top of gnuplot.
globe - Globe wireframe visualization.
gonum/graph - General-purpose graph library.
go-graph - Graph library for Go/Golang language. [Deprecated]
RF - Random forests implementation in Go. [Deprecated]


Computer vision

GoCV - Package for computer vision using OpenCV 4 and beyond.


Haskell

General-Purpose Machine Learning

haskell-ml - Haskell implementations of various ML algorithms. [Deprecated]
HLearn - a suite of libraries for interpreting machine learning models according to their algebraic structure. [Deprecated]
hnn - Haskell Neural Network library.
hopfield-networks - Hopfield Networks for unsupervised learning in Haskell. [Deprecated]
DNNGraph - A DSL for deep neural networks. [Deprecated]
LambdaNet - Configurable Neural Networks in Haskell. [Deprecated]


Java

Natural Language Processing

Cortical.io - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.
IRIS - Cortical.io's FREE NLP, Retina API Analysis Tool (written in JavaFX!) - See the Tutorial Video.
CoreNLP - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.
Stanford Parser - A natural language parser is a program that works out the grammatical structure of sentences.
Stanford POS Tagger - A Part-Of-Speech Tagger (POS Tagger).
Stanford Name Entity Recognizer - Stanford NER is a Java implementation of a Named Entity Recognizer.
Stanford Word Segmenter - Tokenization of raw text is a standard pre-processing step for many NLP tasks.
Tregex, Tsurgeon and Semgrex - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for ""tree regular expressions"").
Stanford Phrasal: A Phrase-Based Translation System
Stanford English Tokenizer - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.
Stanford Tokens Regex - A tokenizer divides text into a sequence of tokens, which roughly correspond to ""words"".
Stanford Temporal Tagger - SUTime is a library for recognizing and normalizing time expressions.
Stanford SPIED - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.
Stanford Topic Modeling Toolbox - Topic modeling tools to social scientists and others who wish to perform analysis on datasets.
Twitter Text Java - A Java implementation of Twitter's text processing library.
MALLET - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
OpenNLP - a machine learning based toolkit for the processing of natural language text.
LingPipe - A tool kit for processing text using computational linguistics.
ClearTK - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. [Deprecated]
Apache cTAKES - Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.
NLP4J - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. [Deprecated]
CogcompNLP - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example illinois-core-utilities which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, illinois-edison a library for feature extraction from illinois-core-utilities data structures and many other packages.


General-Purpose Machine Learning

aerosolve - A machine learning library by Airbnb designed from the ground up to be human friendly.
AMIDST Toolbox - A Java Toolbox for Scalable Probabilistic Machine Learning.
Datumbox - Machine Learning framework for rapid development of Machine Learning and Statistical applications.
ELKI - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
H2O - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.
htm.java - General Machine Learning library using Numenta’s Cortical Learning Algorithm.
liblinear-java - Java version of liblinear.
Mahout - Distributed machine learning.
Meka - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Neuroph - Neuroph is lightweight Java neural network framework
ORYX - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.
Samoa SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.
RankLib - RankLib is a library of learning to rank algorithms. [Deprecated]
rapaio - statistics, data mining and machine learning toolbox in Java.
RapidMiner - RapidMiner integration into Java code.
Stanford Classifier - A classifier is a machine learning tool that will take data items and place them into one of k classes.
SmileMiner - Statistical Machine Intelligence & Learning Engine.
SystemML - flexible, scalable machine learning (ML) language.
Weka - Weka is a collection of machine learning algorithms for data mining tasks.
LBJava - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.


Speech Recognition

CMU Sphinx - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.


Data Analysis / Data Visualization

Flink - Open source platform for distributed stream and batch data processing.
Hadoop - Hadoop/HDFS.
Onyx - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.
Spark - Spark is a fast and general engine for large-scale data processing.
Storm - Storm is a distributed realtime computation system.
Impala - Real-time Query for Hadoop.
DataMelt - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.
Dr. Michael Thomas Flanagan's Java Scientific Library [Deprecated]


Deep Learning

Deeplearning4j - Scalable deep learning for industry with parallel GPUs.
Keras Beginner Tutorial - Friendly guide on using Keras to implement a simple Neural Network in Python


Javascript

Natural Language Processing

Twitter-text - A JavaScript implementation of Twitter's text processing library.
natural - General natural language facilities for node.
Knwl.js - A Natural Language Processor in JS.
Retext - Extensible system for analyzing and manipulating natural language.
NLP Compromise - Natural Language processing in the browser.
nlp.js - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more


Data Analysis / Data Visualization

D3.js
High Charts
NVD3.js
dc.js
chartjs
dimple
amCharts
D3xter - Straight forward plotting built on D3. [Deprecated]
statkit - Statistics kit for JavaScript. [Deprecated]
datakit - A lightweight framework for data analysis in JavaScript
science.js - Scientific and statistical computing in JavaScript. [Deprecated]
Z3d - Easily make interactive 3d plots built on Three.js [Deprecated]
Sigma.js - JavaScript library dedicated to graph drawing.
C3.js - customizable library based on D3.js for easy chart drawing.
Datamaps - Customizable SVG map/geo visualizations using D3.js. [Deprecated]
ZingChart - library written on Vanilla JS for big data visualization.
cheminfo - Platform for data visualization and analysis, using the visualizer project.
Learn JS Data
AnyChart
FusionCharts
Nivo - built on top of the awesome d3 and Reactjs libraries


General-Purpose Machine Learning

Auto ML - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!
Convnet.js - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] [Deprecated]
Clusterfck - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. [Deprecated]
Clustering.js - Clustering algorithms implemented in Javascript for Node.js and the browser. [Deprecated]
Decision Trees - NodeJS Implementation of Decision Tree using ID3 Algorithm. [Deprecated]
DN2A - Digital Neural Networks Architecture. [Deprecated]
figue - K-means, fuzzy c-means and agglomerative clustering.
Gaussian Mixture Model - Unsupervised machine learning with multivariate Gaussian mixture model.
Node-fann - FANN (Fast Artificial Neural Network Library) bindings for Node.js [Deprecated]
Keras.js - Run Keras models in the browser, with GPU support provided by WebGL 2.
Kmeans.js - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. [Deprecated]
LDA.js - LDA topic modeling for Node.js
Learning.js - Javascript implementation of logistic regression/c4.5 decision tree [Deprecated]
machinelearn.js - Machine Learning library for the web, Node.js and developers
mil-tokyo - List of several machine learning libraries.
Node-SVM - Support Vector Machine for Node.js
Brain - Neural networks in JavaScript [Deprecated]
Brain.js - Neural networks in JavaScript - continued community fork of Brain.
Bayesian-Bandit - Bayesian bandit implementation for Node and the browser. [Deprecated]
Synaptic - Architecture-free neural network library for Node.js and the browser.
kNear - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.
NeuralN - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. [Deprecated]
kalman - Kalman filter for Javascript. [Deprecated]
shaman - Node.js library with support for both simple and multiple linear regression. [Deprecated]
ml.js - Machine learning and numerical analysis tools for Node.js and the Browser!
ml5 - Friendly machine learning for the web!
Pavlov.js - Reinforcement learning using Markov Decision Processes.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TensorFlow.js - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.
JSMLT - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see visualml.io).
xgboost-node - Run XGBoost model and make predictions in Node.js.
Netron - Visualizer for machine learning models.
WebDNN - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.


Misc

stdlib - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.
sylvester - Vector and Matrix math for JavaScript. [Deprecated]
simple-statistics - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.
regression-js - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.
Lyric - Linear Regression library. [Deprecated]
GreatCircle - Library for calculating great circle distance.
MLPleaseHelp - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at https://jgreenemi.github.io/MLPleaseHelp/, provided via Github Pages.


Demos and Scripts

The Bot - Example of how the neural network learns to predict the angle between two points created with Synaptic.
Half Beer - Beer glass classifier created with Synaptic.
NSFWJS - Indecent content checker with TensorFlow.js
Rock Paper Scissors - Rock Paper Scissors trained in the browser with TensorFlow.js


Julia

General-Purpose Machine Learning

MachineLearning - Julia Machine Learning library. [Deprecated]
MLBase - A set of functions to support the development of machine learning algorithms.
PGM - A Julia framework for probabilistic graphical models.
DA - Julia package for Regularized Discriminant Analysis.
Regression - Algorithms for regression analysis (e.g. linear regression and logistic regression). [Deprecated]
Local Regression - Local regression, so smooooth!
Naive Bayes - Simple Naive Bayes implementation in Julia. [Deprecated]
Mixed Models - A Julia package for fitting (statistical) mixed-effects models.
Simple MCMC - basic mcmc sampler implemented in Julia. [Deprecated]
Distances - Julia module for Distance evaluation.
Decision Tree - Decision Tree Classifier and Regressor.
Neural - A neural network in Julia.
MCMC - MCMC tools for Julia. [Deprecated]
Mamba - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.
GLM - Generalized linear models in Julia.
Gaussian Processes - Julia package for Gaussian processes.
Online Learning [Deprecated]
GLMNet - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.
Clustering - Basic functions for clustering data: k-means, dp-means, etc.
SVM - SVM's for Julia. [Deprecated]
Kernel Density - Kernel density estimators for julia.
MultivariateStats - Methods for dimensionality reduction.
NMF - A Julia package for non-negative matrix factorization.
ANN - Julia artificial neural networks. [Deprecated]
Mocha - Deep Learning framework for Julia inspired by Caffe. [Deprecated]
XGBoost - eXtreme Gradient Boosting Package in Julia.
ManifoldLearning - A Julia package for manifold learning and nonlinear dimensionality reduction.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Merlin - Flexible Deep Learning Framework in Julia.
ROCAnalysis - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.
GaussianMixtures - Large scale Gaussian Mixture Models.
ScikitLearn - Julia implementation of the scikit-learn API.
Knet - Koç University Deep Learning Framework.
Flux - Relax! Flux is the ML library that doesn't make you tensor


Natural Language Processing

Topic Models - TopicModels for Julia. [Deprecated]
Text Analysis - Julia package for text analysis.
Word Tokenizers - Tokenizers for Natural Language Processing in Julia
Corpus Loaders - A julia package providing a variety of loaders for various NLP corpora.
Embeddings - Functions and data dependencies for loading various word embeddings
Languages - Julia package for working with various human languages
WordNet - A Julia package for Princeton's WordNet


Data Analysis / Data Visualization

Graph Layout - Graph layout algorithms in pure Julia.
LightGraphs - Graph modeling and analysis.
Data Frames Meta - Metaprogramming tools for DataFrames.
Julia Data - library for working with tabular data in Julia. [Deprecated]
Data Read - Read files from Stata, SAS, and SPSS.
Hypothesis Tests - Hypothesis tests for Julia.
Gadfly - Crafty statistical graphics for Julia.
Stats - Statistical tests for Julia.
RDataSets - Julia package for loading many of the data sets available in R.
DataFrames - library for working with tabular data in Julia.
Distributions - A Julia package for probability distributions and associated functions.
Data Arrays - Data structures that allow missing values. [Deprecated]
Time Series - Time series toolkit for Julia.
Sampling - Basic sampling algorithms for Julia.


Misc Stuff / Presentations

DSP - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).
JuliaCon Presentations - Presentations for JuliaCon.
SignalProcessing - Signal Processing tools for Julia.
Images - An image library for Julia.
DataDeps - Reproducible data setup for reproducible science.


Lua

General-Purpose Machine Learning

Torch7

cephes - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. [Deprecated]
autograd - Autograd automatically differentiates native Torch code. Inspired by the original Python version.
graph - Graph package for Torch. [Deprecated]
randomkit - Numpy's randomkit, wrapped for Torch. [Deprecated]
signal - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.
nn - Neural Network package for Torch.
torchnet - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.
nngraph - This package provides graphical computation for nn library in Torch7.
nnx - A completely unstable and experimental package that extends Torch's builtin nn library.
rnn - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.
dpnn - Many useful features that aren't part of the main nn package.
dp - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. [Deprecated]
optim - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.
unsup - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). [Deprecated]
manifold - A package to manipulate manifolds.
svm - Torch-SVM library. [Deprecated]
lbfgs - FFI Wrapper for liblbfgs. [Deprecated]
vowpalwabbit - An old vowpalwabbit interface to torch. [Deprecated]
OpenGM - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. [Deprecated]
spaghetti - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu [Deprecated]
LuaSHKit - A lua wrapper around the Locality sensitive hashing library SHKit [Deprecated]
kernel smoothing - KNN, kernel-weighted average, local linear regression smoothers. [Deprecated]
cutorch - Torch CUDA Implementation.
cunn - Torch CUDA Neural Network Implementation.
imgraph - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. [Deprecated]
videograph - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. [Deprecated]
saliency - code and tools around integral images. A library for finding interest points based on fast integral histograms. [Deprecated]
stitch - allows us to use hugin to stitch images and apply same stitching to a video sequence. [Deprecated]
sfm - A bundle adjustment/structure from motion package. [Deprecated]
fex - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. [Deprecated]
OverFeat - A state-of-the-art generic dense feature extractor. [Deprecated]
wav2letter - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.


Numeric Lua
Lunatic Python
SciLua
Lua - Numerical Algorithms [Deprecated]
Lunum [Deprecated]


Demos and Scripts

Core torch7 demos repository.

linear-regression, logistic-regression
face detector (training and detection as separate demos)
mst-based-segmenter
train-a-digit-classifier
train-autoencoder
optical flow demo
train-on-housenumbers
train-on-cifar
tracking with deep nets
kinect demo
filter-bank visualization
saliency-networks


Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)
Music Tagging - Music Tagging scripts for torch7.
torch-datasets - Scripts to load several popular datasets including:

BSR 500
CIFAR-10
COIL
Street View House Numbers
MNIST
NORB


Atari2600 - Scripts to generate a dataset with static frames from the Arcade Learning Environment.


Matlab

Computer Vision

Contourlets - MATLAB source code that implements the contourlet transform and its utility functions.
Shearlets - MATLAB code for shearlet transform.
Curvelets - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.
Bandlets - MATLAB code for bandlet transform.
mexopencv - Collection and a development kit of MATLAB mex functions for OpenCV library.


Natural Language Processing

NLP - An NLP library for Matlab.


General-Purpose Machine Learning

Training a deep autoencoder or a classifier
on MNIST digits - Training a deep autoencoder or a classifier
on MNIST digits[DEEP LEARNING].
Convolutional-Recursive Deep Learning for 3D Object Classification - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].
Spider - The spider is intended to be a complete object orientated environment for machine learning in Matlab.
LibSVM - A Library for Support Vector Machines.
ThunderSVM - An Open-Source SVM Library on GPUs and CPUs
LibLinear - A Library for Large Linear Classification.
Machine Learning Module - Class on machine w/ PDF, lectures, code
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
Pattern Recognition Toolbox - A complete object-oriented environment for machine learning in Matlab.
Pattern Recognition and Machine Learning - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Machine Learning in MatLab/Octave - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.


Data Analysis / Data Visualization

matlab_bgl - MatlabBGL is a Matlab package for working with graphs.
gaimc - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.


.NET

Computer Vision

OpenCVDotNet - A wrapper for the OpenCV project to be used with .NET applications.
Emgu CV - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.
AForge.NET - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.
Accord.NET - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.


Natural Language Processing

Stanford.NLP for .NET - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.


General-Purpose Machine Learning

Accord-Framework -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.
Accord.MachineLearning - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.
DiffSharp - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
GeneticSharp - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.
Infer.NET - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.
ML.NET - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.
Neural Network Designer - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.
Synapses - Neural network library in F#.
Vulpes - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.


Data Analysis / Data Visualization

numl - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.
Math.NET Numerics - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.
Sho - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.


Objective C

General-Purpose Machine Learning

YCML - A Machine Learning framework for Objective-C and Swift (OS X / iOS).
MLPNeuralNet - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. [Deprecated]
MAChineLearning - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.
BPN-NeuralNetwork - It implemented 3 layers neural network ( Input Layer, Hidden Layer and Output Layer ) and it named Back Propagation Neural Network (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. [Deprecated]
Multi-Perceptron-NeuralNetwork - it implemented multi-perceptrons neural network (ニューラルネットワーク) based on Back Propagation Neural Network (BPN) and designed unlimited-hidden-layers.
KRHebbian-Algorithm - It is a non-supervisor and self-learning algorithm (adjust the weights) in neural network of Machine Learning. [Deprecated]
KRKmeans-Algorithm - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression. [Deprecated]
KRFuzzyCMeans-Algorithm - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. [Deprecated]


OCaml

General-Purpose Machine Learning

Oml - A general statistics and machine learning library.
GPR - Efficient Gaussian Process Regression in OCaml.
Libra-Tk - Algorithms for learning and inference with discrete probabilistic models.
TensorFlow - OCaml bindings for TensorFlow.


Perl

Data Analysis / Data Visualization

Perl Data Language, a pluggable architecture for data and image processing, which can
be used for machine learning.


General-Purpose Machine Learning

MXnet for Deep Learning, in Perl,
also released in CPAN.
Perl Data Language,
using AWS machine learning platform from Perl.
Algorithm::SVMLight,
implementation of Support Vector Machines with SVMLight under it. [Deprecated]
Several machine learning and artificial intelligence models are
included in the AI
namespace. For instance, you can
find Naïve Bayes.


Perl 6

Support Vector Machines
Naïve Bayes

Data Analysis / Data Visualization

Perl Data Language,
a pluggable architecture for data and image processing, which can
be
used for machine learning.

General-Purpose Machine Learning

PHP

Natural Language Processing

jieba-php - Chinese Words Segmentation Utilities.


General-Purpose Machine Learning

PHP-ML - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Rubix ML - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.
19 Questions - A machine learning / bayesian inference assigning attributes to objects.


Python

Computer Vision

Scikit-Image - A collection of algorithms for image processing in Python.
SimpleCV - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.
Vigranumpy - Python bindings for the VIGRA C++ computer vision library.
OpenFace - Free and open source face recognition with deep neural networks.
PCV - Open source Python module for computer vision. [Deprecated]
face_recognition - Face recognition library that recognize and manipulate faces from Python or from the command line.
dockerface - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.
Detectron - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.
albumentations - А fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.
pytessarct - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and ""read"" the text embedded in images.Python-tesseract is a wrapper for Google's Tesseract-OCR Engine>.
imutils - A library containg Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.
PyTorchCV - A PyTorch-Based Framework for Deep Learning in Computer Vision.
neural-style-pt - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).


Natural Language Processing

pkuseg-python - A better version of Jieba, developed by Peking University.
NLTK - A leading platform for building Python programs to work with human language data.
Pattern - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.
Quepy - A python framework to transform natural language questions to queries in a database query language.
TextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.
YAlign - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. [Deprecated]
jieba - Chinese Words Segmentation Utilities.
SnowNLP - A library for processing Chinese text.
spammy - A library for email Spam filtering built on top of nltk
loso - Another Chinese segmentation library. [Deprecated]
genius - A Chinese segment base on Conditional Random Field.
KoNLPy - A Python package for Korean natural language processing.
nut - Natural language Understanding Toolkit. [Deprecated]
Rosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit)
BLLIP Parser - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). [Deprecated]
PyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments.
python-ucto - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).
python-frog - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)
python-zpar - Python bindings for ZPar, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.
colibri-core - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
spaCy - Industrial strength NLP with Python and Cython.
PyStanfordDependencies - Python interface for converting Penn Treebank trees to Stanford Dependencies.
Distance - Levenshtein and Hamming distance computation. [Deprecated]
Fuzzy Wuzzy - Fuzzy String Matching in Python.
jellyfish - a python library for doing approximate and phonetic matching of strings.
editdistance - fast implementation of edit distance.
textacy - higher-level NLP built on Spacy.
stanford-corenlp-python - Python wrapper for Stanford CoreNLP [Deprecated]
CLTK - The Classical Language Toolkit.
rasa_nlu - turn natural language into structured data.
yase - Transcode sentence (or other sequence) to list of word vector .
Polyglot - Multilingual text (NLP) processing toolkit.
DrQA - Reading Wikipedia to answer open-domain questions.
Dedupe - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.
Snips NLU - Natural Language Understanding library for intent classification and entity extraction
NeuroNER - Named-entity recognition using neural networks providing state-of-the-art-results
DeepPavlov - conversational AI library with many pretrained Russian NLP models.
BigARTM - topic modelling platform.


General-Purpose Machine Learning

PyOD -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.
steppy -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design.
steppy-toolkit -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.
CNTK - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found here.
auto_ml - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.
machine learning - automated build consisting of a web-interface, and set of programmatic-interface API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.
XGBoost - Python bindings for eXtreme Gradient Boosting (Tree) Library.
Apache SINGA - An Apache Incubating project for developing an open source machine learning library.
Bayesian Methods for Hackers - Book/iPython notebooks on Probabilistic Programming in Python.
Featureforge A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
scikit-learn - A Python module for machine learning built on top of SciPy.
metric-learn - A Python module for metric learning.
SimpleAI Python implementation of many of the artificial intelligence algorithms described on the book ""Artificial Intelligence, a Modern Approach"". It focuses on providing an easy to use, well documented and tested library.
astroML - Machine Learning and Data Mining for Astronomy.
graphlab-create - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.
BigML - A library that contacts external servers.
pattern - Web mining module for Python.
NuPIC - Numenta Platform for Intelligent Computing.
Pylearn2 - A Machine Learning library based on Theano. [Deprecated]
keras - High-level neural networks frontend for TensorFlow, CNTK and Theano.
Lasagne - Lightweight library to build and train neural networks in Theano.
hebel - GPU-Accelerated Deep Learning Library in Python. [Deprecated]
Chainer - Flexible neural network framework.
prophet - Fast and automated time series forecasting framework by Facebook.
gensim - Topic Modelling for Humans.
topik - Topic modelling toolkit. [Deprecated]
PyBrain - Another Python Machine Learning Library.
Brainstorm - Fast, flexible and fun neural networks. This is the successor of PyBrain.
Surprise - A scikit for building and analyzing recommender systems.
Crab - A flexible, fast recommender engine. [Deprecated]
python-recsys - A Python library for implementing a Recommender System.
thinking bayes - Book on Bayesian Analysis.
Image-to-Image Translation with Conditional Adversarial Networks - Implementation of image to image (pix2pix) translation from the paper by isola et al.[DEEP LEARNING]
Restricted Boltzmann Machines -Restricted Boltzmann Machines in Python. [DEEP LEARNING]
Bolt - Bolt Online Learning Toolbox. [Deprecated]
CoverTree - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree [Deprecated]
nilearn - Machine learning for NeuroImaging in Python.
neuropredict - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing the much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.
imbalanced-learn - Python module to perform under sampling and over sampling with various techniques.
Shogun - The Shogun Machine Learning Toolbox.
Pyevolve - Genetic algorithm framework. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
breze - Theano based library for deep and recurrent neural networks.
Cortex - Open source platform for deploying machine learning models in production.
pyhsmm - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.
mrjob - A library to let Python program run on Hadoop.
SKLL - A wrapper around scikit-learn that makes it simpler to conduct experiments.
neurolab
Spearmint - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. [Deprecated]
Pebl - Python Environment for Bayesian Learning. [Deprecated]
Theano - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.
TensorFlow - Open source software library for numerical computation using data flow graphs.
pomegranate - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.
python-timbl - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.
deap - Evolutionary algorithm framework.
pydeep - Deep Learning In Python. [Deprecated]
mlxtend - A library consisting of useful tools for data science and machine learning tasks.
neon - Nervana's high-performance Python-based Deep Learning framework [DEEP LEARNING].
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.
Neural Networks and Deep Learning - Code samples for my book ""Neural Networks and Deep Learning"" [DEEP LEARNING].
Annoy - Approximate nearest neighbours implementation.
TPOT - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.
pgmpy A python library for working with Probabilistic Graphical Models.
DIGITS - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.
Orange - Open source data visualization and data analysis for novices and experts.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
milk - Machine learning toolkit focused on supervised classification. [Deprecated]
TFLearn - Deep learning library featuring a higher-level API for TensorFlow.
REP - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. [Deprecated]
rgf_python - Python bindings for Regularized Greedy Forest (Tree) Library.
skbayes - Python package for Bayesian Machine Learning with scikit-learn API.
fuku-ml - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.
Xcessiv - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.
PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration
ML-From-Scratch - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.
Edward - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.
xRBM - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.
stacked_generalization - Implementation of machine learning stacking technic as handy library in Python.
modAL - A modular active learning framework for Python, built on top of scikit-learn.
Cogitare: A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.
Parris - Parris, the automated infrastructure setup tool for machine learning algorithms.
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.
Turi Create - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
mlens - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.
Netron - Visualizer for machine learning models.
Thampi - Machine Learning Prediction System on AWS Lambda
MindsDB - Open Source framework to streamline use of neural networks.
Microsoft Recommenders: Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.
StellarGraph: Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.
BentoML: Toolkit for package and deploy machine learning models for serving in production
MiraiML: An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.
numpy-ML: Reference implementations of ML models written in numpy
creme: A framework for online machine learning.
Neuraxle: A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.
Cornac - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.
JAX - JAX is Autograd and XLA, brought together for high-performance machine learning research.
fast.ai - A library simplifies training fast and accurate neural nets using modern best practices and already supports  vision, text, tabular, and collab (collaborative filtering) models ""out of the box""
Catalyst - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.
Fastai - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.


Data Analysis / Data Visualization

SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
NumPy - A fundamental package for scientific computing with Python.
Numba - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.
Mars - A tensor-based framework for large-scale data computation which often regarded as a parallel and distributed version of NumPy.
NetworkX - A high-productivity software for complex networks.
igraph - binding to igraph library - General purpose graph library.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Open Mining - Business Intelligence (BI) in Python (Pandas web interface) [Deprecated]
PyMC - Markov Chain Monte Carlo sampling toolkit.
zipline - A Pythonic algorithmic trading library.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.
SymPy - A Python library for symbolic mathematics.
statsmodels - Statistical modeling and econometrics in Python.
astropy - A community Python library for Astronomy.
matplotlib - A Python 2D plotting library.
bokeh - Interactive Web Plotting for Python.
plotly - Collaborative web plotting for Python and matplotlib.
altair - A Python to Vega translator.
d3py - A plotting library for Python, based on D3.js.
PyDexter - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.
ggplot - Same API as ggplot2 for R. [Deprecated]
ggfortify - Unified interface to ggplot2 popular R packages.
Kartograph.py - Rendering beautiful SVG maps in Python.
pygal - A Python SVG Charts Creator.
PyQtGraph - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.
pycascading [Deprecated]
Petrel - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.
Blaze - NumPy and Pandas interface to Big Data.
emcee - The Python ensemble sampling toolkit for affine-invariant MCMC.
windML - A Python Framework for Wind Energy Analysis and Prediction.
vispy - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.
cerebro2 A web-based visualization and debugging platform for NuPIC. [Deprecated]
NuPIC Studio An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! [Deprecated]
SparklingPandas Pandas on PySpark (POPS).
Seaborn - A python visualization library based on matplotlib.
bqplot - An API for plotting in Jupyter (IPython).
pastalog - Simple, realtime visualization of neural network training performance.
Superset - A data exploration platform designed to be visual, intuitive, and interactive.
Dora - Tools for exploratory data analysis in Python.
Ruffus - Computation Pipeline library for python.
SOMPY - Self Organizing Map written in Python (Uses neural networks for data analysis).
somoclu Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.
HDBScan - implementation of the hdbscan algorithm in Python - used for clustering
visualize_ML - A python package for data exploration and data analysis. [Deprecated]
scikit-plot - A visualization library for quick and easy generation of common plots in data analysis and machine learning.
Bowtie - A dashboard library for interactive visualizations using flask socketio and react.
lime - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.
PyCM - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters
Dash - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask
Lambdo - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.
TensorWatch - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.
dowel - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to logger.log().


Misc Scripts / iPython Notebooks / Codebases

Map/Reduce implementations of common ML algorithms: Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.
BioPy - Biologically-Inspired and Machine Learning Algorithms in Python. [Deprecated]
SVM Explorer - Interactive SVM Explorer, using Dash and scikit-learn
pattern_classification
thinking stats 2
hyperopt
numpic
2012-paper-diginorm
A gallery of interesting IPython notebooks
ipython-notebooks
data-science-ipython-notebooks - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.
decision-weights
Sarah Palin LDA - Topic Modeling the Sarah Palin emails.
Diffusion Segmentation - A collection of image segmentation algorithms based on diffusion methods.
Scipy Tutorials - SciPy tutorials. This is outdated, check out scipy-lecture-notes.
Crab - A recommendation engine library for Python.
BayesPy - Bayesian Inference Tools in Python.
scikit-learn tutorials - Series of notebooks for learning scikit-learn.
sentiment-analyzer - Tweets Sentiment Analyzer
sentiment_classifier - Sentiment classifier using word sense disambiguation.
group-lasso - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.
jProcessing - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.
mne-python-notebooks - IPython notebooks for EEG/MEG data processing using mne-python.
Neon Course - IPython notebooks for a complete course around understanding Nervana's Neon.
pandas cookbook - Recipes for using Python's pandas library.
climin - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.
Allen Downey’s Data Science Course - Code for Data Science at Olin College, Spring 2014.
Allen Downey’s Think Bayes Code - Code repository for Think Bayes.
Allen Downey’s Think Complexity Code - Code for Allen Downey's book Think Complexity.
Allen Downey’s Think OS Code - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.
Python Programming for the Humanities - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.
GreatCircle - Library for calculating great circle distance.
Optunity examples - Examples demonstrating how to use Optunity in synergy with machine learning libraries.
Dive into Machine Learning  with Python Jupyter notebook and scikit-learn - ""I learned Python by hacking first, and getting serious later. I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.""
TDB - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.
Suiron - Machine Learning for RC Cars.
Introduction to machine learning with scikit-learn - IPython notebooks from Data School's video tutorials on scikit-learn.
Practical XGBoost in Python - comprehensive online course about using XGBoost in Python.
Introduction to Machine Learning with Python - Notebooks and code for the book ""Introduction to Machine Learning with Python""
Pydata book - Materials and IPython notebooks for ""Python for Data Analysis"" by Wes McKinney, published by O'Reilly Media
Homemade Machine Learning - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained
Prodmodel - Build tool for data science pipelines.
the-elements-of-statistical-learning - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.


Neural Networks

nn_builder - nn_builder is a python package that lets you build neural networks in 1 line
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm.
=======
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. [Deprecated]
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm. [Deprecated]
Data Driven Code - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.
Machine Learning, Data Science and Deep Learning with Python - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.


Kaggle Competition Source Code

open-solution-home-credit -> source code and experiments results for Home Credit Default Risk.
open-solution-googleai-object-detection -> source code and experiments results for Google AI Open Images - Object Detection Track.
open-solution-salt-identification -> source code and experiments results for TGS Salt Identification Challenge.
open-solution-ship-detection -> source code and experiments results for Airbus Ship Detection Challenge.
open-solution-data-science-bowl-2018 -> source code and experiments results for 2018 Data Science Bowl.
open-solution-value-prediction -> source code and experiments results for Santander Value Prediction Challenge.
open-solution-toxic-comments -> source code for Toxic Comment Classification Challenge.
wiki challenge - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.
kaggle insults - Kaggle Submission for ""Detecting Insults in Social Commentary"".
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
kaggle-cifar - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.
kaggle-blackbox - Deep learning made easy.
kaggle-accelerometer - Code for Accelerometer Biometric Competition at Kaggle.
kaggle-advertised-salaries - Predicting job salaries from ads - a Kaggle competition.
kaggle amazon - Amazon access control challenge.
kaggle-bestbuy_big - Code for the Best Buy competition at Kaggle.
kaggle-bestbuy_small
Kaggle Dogs vs. Cats - Code for Kaggle Dogs vs. Cats competition.
Kaggle Galaxy Challenge - Winning solution for the Galaxy Challenge on Kaggle.
Kaggle Gender - A Kaggle competition: discriminate gender based on handwriting.
Kaggle Merck - Merck challenge at Kaggle.
Kaggle Stackoverflow - Predicting closed questions on Stack Overflow.
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
wine-quality - Predicting wine quality.


Reinforcement Learning

DeepMind Lab - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.
Gym - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
Serpent.AI - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.
ViZDoom - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.
Roboschool - Open-source software for robot simulation, integrated with OpenAI Gym.
Retro - Retro Games in Gym
SLM Lab - Modular Deep Reinforcement Learning framework in PyTorch.
Coach - Reinforcement Learning Coach by Intel® AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms
garage - A toolkit for reproducible reinforcement learning research
metaworld - An open source robotics benchmark for meta- and multi-task reinforcement learning


Ruby

Natural Language Processing

Awesome NLP with Ruby - Curated link list for practical natural language processing in Ruby.
Treat - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I’ve encountered so far for Ruby.
Stemmer - Expose libstemmer_c to Ruby. [Deprecated]
Raspel - raspell is an interface binding for ruby. [Deprecated]
UEA Stemmer - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.
Twitter-text-rb - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.


General-Purpose Machine Learning

Awesome Machine Learning with Ruby - Curated list of ML related resources for Ruby.
Ruby Machine Learning - Some Machine Learning algorithms, implemented in Ruby. [Deprecated]
Machine Learning Ruby [Deprecated]
jRuby Mahout - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. [Deprecated]
CardMagic-Classifier - A general classifier module to allow Bayesian and other types of classifications.
rb-libsvm - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.
Scoruby - Creates Random Forest classifiers from PMML files.


Data Analysis / Data Visualization

rsruby - Ruby - R bridge.
data-visualization-ruby - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. [Deprecated]
ruby-plot - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. [Deprecated]
plot-rb - A plotting library in Ruby built on top of Vega and D3. [Deprecated]
scruffy - A beautiful graphing toolkit for Ruby.
SciRuby
Glean - A data management tool for humans. [Deprecated]
Bioruby
Arel [Deprecated]


Misc

Big Data For Chimps
Listof - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. Demo/Search for a list


Rust

General-Purpose Machine Learning

deeplearn-rs - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.
rustlearn - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.
rusty-machine - a pure-rust machine learning library.
leaf - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [Deprecated]
RustNN - RustNN is a feedforward neural network library. [Deprecated]
RusticSOM - A Rust library for Self Organising Maps (SOM).


R

General-Purpose Machine Learning

ahaz - ahaz: Regularization for semiparametric additive hazards regression. [Deprecated]
arules - arules: Mining Association Rules and Frequent Itemsets
biglasso - biglasso: Extending Lasso Model Fitting to Big Data in R.
bmrm - bmrm: Bundle Methods for Regularized Risk Minimization Package.
Boruta - Boruta: A wrapper algorithm for all-relevant feature selection.
bst - bst: Gradient Boosting.
C50 - C50: C5.0 Decision Trees and Rule-Based Models.
caret - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.
caretEnsemble - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. [Deprecated]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.
Clever Algorithms For Machine Learning
CORElearn - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.
CoxBoost - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks [Deprecated]
Cubist - Cubist: Rule- and Instance-Based Regression Modeling.
e1071 - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien
earth - earth: Multivariate Adaptive Regression Spline Models
elasticnet - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.
ElemStatLearn - ElemStatLearn: Data sets, functions and examples from the book: ""The Elements of Statistical Learning, Data Mining, Inference, and Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.
evtree - evtree: Evolutionary Learning of Globally Optimal Trees.
forecast - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.
forecastHybrid - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the ""forecast"" package.
fpc - fpc: Flexible procedures for clustering.
frbs - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. [Deprecated]
GAMBoost - GAMBoost: Generalized linear and additive models by likelihood based boosting. [Deprecated]
gamboostLSS - gamboostLSS: Boosting Methods for GAMLSS.
gbm - gbm: Generalized Boosted Regression Models.
glmnet - glmnet: Lasso and elastic-net regularized generalized linear models.
glmpath - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.
GMMBoost - GMMBoost: Likelihood-based Boosting for Generalized mixed models. [Deprecated]
grplasso - grplasso: Fitting user specified models with Group Lasso penalty.
grpreg - grpreg: Regularization paths for regression models with grouped covariates.
h2o - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.
hda - hda: Heteroscedastic Discriminant Analysis. [Deprecated]
Introduction to Statistical Learning
ipred - ipred: Improved Predictors.
kernlab - kernlab: Kernel-based Machine Learning Lab.
klaR - klaR: Classification and visualization.
L0Learn - L0Learn: Fast algorithms for best subset selection.
lars - lars: Least Angle Regression, Lasso and Forward Stagewise. [Deprecated]
lasso2 - lasso2: L1 constrained estimation aka ‘lasso’.
LiblineaR - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.
LogicReg - LogicReg: Logic Regression.
Machine Learning For Hackers
maptree - maptree: Mapping, pruning, and graphing tree models. [Deprecated]
mboost - mboost: Model-Based Boosting.
medley - medley: Blending regression models, using a greedy stepwise approach.
mlr - mlr: Machine Learning in R.
ncvreg - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.
nnet - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. [Deprecated]
pamr - pamr: Pam: prediction analysis for microarrays. [Deprecated]
party - party: A Laboratory for Recursive Partytioning.
partykit - partykit: A Toolkit for Recursive Partytioning.
penalized - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.
penalizedLDA - penalizedLDA: Penalized classification using Fisher's linear discriminant. [Deprecated]
penalizedSVM - penalizedSVM: Feature Selection SVM using penalty functions.
quantregForest - quantregForest: Quantile Regression Forests.
randomForest - randomForest: Breiman and Cutler's random forests for classification and regression.
randomForestSRC - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).
rattle - rattle: Graphical user interface for data mining in R.
rda - rda: Shrunken Centroids Regularized Discriminant Analysis.
rdetools - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. [Deprecated]
REEMtree - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. [Deprecated]
relaxo - relaxo: Relaxed Lasso. [Deprecated]
rgenoud - rgenoud: R version of GENetic Optimization Using Derivatives
Rmalschains - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.
rminer - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. [Deprecated]
ROCR - ROCR: Visualizing the performance of scoring classifiers. [Deprecated]
RoughSets - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. [Deprecated]
rpart - rpart: Recursive Partitioning and Regression Trees.
RPMM - RPMM: Recursively Partitioned Mixture Model.
RSNNS - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).
RWeka - RWeka: R/Weka interface.
RXshrink - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.
sda - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. [Deprecated]
spectralGraphTopology - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.
SuperLearner - Multi-algorithm ensemble learning packages.
svmpath - svmpath: svmpath: the SVM Path algorithm. [Deprecated]
tgp - tgp: Bayesian treed Gaussian process models. [Deprecated]
tree - tree: Classification and regression trees.
varSelRF - varSelRF: Variable selection using random forests.
XGBoost.R - R binding for eXtreme Gradient Boosting (Tree) Library.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.
igraph - binding to igraph library - General purpose graph library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TDSP-Utilities - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).


Data Analysis / Data Visualization

ggplot2 - A data visualization package based on the grammar of graphics.
tmap for visualizing geospatial data with static maps and leaflet for interactive maps
tm and quanteda are the main packages for managing,  analyzing, and visualizing textual data.
shiny is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with htmlwidgets bringing javascript libraries to R. These include, plotly, dygraphs, highcharter, and several others.


SAS

General-Purpose Machine Learning

Visual Data Mining and Machine Learning - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.
Enterprise Miner - Data mining and machine learning that creates deployable models using a GUI or code.
Factory Miner - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.


Data Analysis / Data Visualization

SAS/STAT - For conducting advanced statistical analysis.
University Edition - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.


Natural Language Processing

Contextual Analysis - Add structure to unstructured text using a GUI.
Sentiment Analysis - Extract sentiment from text using a GUI.
Text Miner - Text mining using a GUI or code.


Demos and Scripts

ML_Tables - Concise cheat sheets containing machine learning best practices.
enlighten-apply - Example code and materials that illustrate applications of SAS machine learning techniques.
enlighten-integration - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.
enlighten-deep - Example code and materials that illustrate using neural networks with several hidden layers in SAS.
dm-flow - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.


Scala

Natural Language Processing

ScalaNLP - ScalaNLP is a suite of machine learning and numerical computing libraries.
Breeze - Breeze is a numerical processing library for Scala.
Chalk - Chalk is a natural language processing library. [Deprecated]
FACTORIE - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.
Montague - Montague is a semantic parsing library for Scala with an easy-to-use DSL.
Spark NLP - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.


Data Analysis / Data Visualization

MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Scalding - A Scala API for Cascading.
Summing Bird - Streaming MapReduce with Scalding and Storm.
Algebird - Abstract Algebra for Scala.
xerial - Data management utilities for Scala. [Deprecated]
PredictionIO - PredictionIO, a machine learning server for software developers and data engineers.
BIDMat - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.
Flink - Open source platform for distributed stream and batch data processing.
Spark Notebook - Interactive and Reactive Data Science using Scala and Spark.


General-Purpose Machine Learning

DeepLearning.scala - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.
Conjecture - Scalable Machine Learning in Scalding.
brushfire - Distributed decision tree ensemble learning in Scala.
ganitha - Scalding powered machine learning. [Deprecated]
adam - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.
bioscala - Bioinformatics for the Scala programming language
BIDMach - CPU and GPU-accelerated Machine Learning Library.
Figaro - a Scala library for constructing probabilistic models.
H2O Sparkling Water - H2O and Spark interoperability.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
DynaML - Scala Library/REPL for Machine Learning Research.
Saul - Flexible Declarative Learning-Based Programming.
SwiftLearner - Simply written algorithms to help study ML or write your own implementations.
Smile - Statistical Machine Intelligence and Learning Engine.
doddle-model - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.
TensorFlow Scala -   Strongly-typed Scala API for TensorFlow.


Scheme

Neural Networks

layer - Neural network inference from the command line, implemented in CHICKEN Scheme.


Swift

General-Purpose Machine Learning

Bender - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.
Swift AI - Highly optimized artificial intelligence and machine learning library written in Swift.
Swift for Tensorflow - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.
BrainCore - The iOS and OS X neural network framework.
swix - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. [Deprecated]
AIToolbox - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.
MLKit - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.
Swift Brain - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...
Perfect TensorFlow - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Awesome CoreML - A curated list of pretrained CoreML models.
Awesome Core ML Models - A curated list of machine learning models in CoreML format.


TensorFlow

General-Purpose Machine Learning

Awesome TensorFlow - A list of all things related to TensorFlow.
Golden TensorFlow - A page of content on TensorFlow, including academic papers and links to related topics.


Tools

Neural Networks

layer - Neural network inference from the command line


Misc

ML Workspace - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).
Notebooks - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.
DVC - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.
Kedro - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.
guild.ai - Tool to log, analyze, compare and ""optimize"" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.
Sacred - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemestry/Biology. The community has built multiple add-ons leveraging the proposed standard.
MLFlow - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework anf language agnostic, take a look at all the built-in integrations.
More tools to improve the ML lifecycle: Catalyst, PachydermIO. The following are Github-alike and targetting teams Weights & Biases, Neptune.Ml, Comet.ml, Valohai.ai.


Credits

Some of the python libraries were cut-and-pasted from vinta
References for Go were mostly cut-and-pasted from gopherdata

"
42,Python,"
    
    
    
    
     Python 3.7.4 (default, Sep  7 2019, 18:27:02)
     >>> import requests
     >>> r = requests.get('https://api.github.com/repos/psf/requests')
     >>> r.json()[""description""]
     'A simple, yet elegant HTTP library. Handcrafted, with ♥, for the Python community.'
    

    
This software has been designed for you, with much joy,
by Kenneth Reitz & is protected by The Python Software Foundation.
   


  
Requests is an elegant and simple HTTP library for Python, built with ♥.
 
>>> import requests
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{""type"":""User""...'
>>> r.json()
{u'disk_usage': 368627, u'private_gists': 484, ...}

 
Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your PUT & POST data — but nowadays, just use the json method!
Requests is the most downloaded Python package today, pulling in around 14M downloads / week— according to GitHub, Requests is currently depended upon by 367_296 repositories. You may certainly put your trust in this code.
 



 
Supported Features & Best–Practices
Requests is ready for the demands of building robust and reliable HTTP–speak applications, for the needs of today.
         + International Domains and URLs       + Keep-Alive & Connection Pooling
         + Sessions with Cookie Persistence     + Browser-style SSL Verification
         + Basic & Digest Authentication        + Familiar `dict`–like Cookies
         + Automatic Decompression of Content   + Automatic Content Decoding
         + Automatic Connection Pooling         + Unicode Response Bodies*
         + Multi-part File Uploads              + SOCKS Proxy Support
         + Connection Timeouts                  + Streaming Downloads
         + Automatic honoring of `.netrc`       + Chunked HTTP Requests

                            &, of course, rock–solid stability!


✨ 🍰 ✨            

 
Requests Module Installation
The recommended way to intall the requests module is to simply use pipenv (or pip, of
course):
$ pipenv install requests
Adding requests to Pipfile's [packages]…
✔ Installation Succeeded
…
Requests officially supports Python 2.7 & 3.4–3.8.

P.S. — Documentation is Available at //requests.readthedocs.io.




 



 



"
43,Python,"
 
 
  
 
 


Ansible
Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible website.

Design Principles

Have a dead simple setup process and a minimal learning curve.
Manage machines very quickly and in parallel.
Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.
Describe infrastructure in a language that is both machine and human
friendly.
Focus on security and easy auditability/review/rewriting of content.
Manage new remote machines instantly, without bootstrapping any
software.
Allow module development in any dynamic language, not just Python.
Be usable as non-root.
Be the easiest IT automation system to use, ever.


Use Ansible
You can install a released version of Ansible via pip, a package manager, or
our release repository. See our
installation guide for details on installing Ansible
on a variety of platforms.
Red Hat offers supported builds of Ansible Engine.
Power users and developers can run the devel branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the devel branch. We recommend getting involved
in the Ansible community if you want to run the devel branch.

Get Involved

Read Community
Information for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.
Join a Working Group, an organized community devoted to a specific technology domain or platform.
Submit a proposed code update through a pull request to the devel branch.
Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.
For a list of email lists, IRC channels and Working Groups, see the
Communication page


Branch Info

The devel branch corresponds to the release actively under development.
The stable-2.X branches correspond to stable releases.
Create a branch based on devel and set up a dev environment if you want to open a PR.
See the Ansible release and maintenance page for information about active branches.


Roadmap
Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The Ansible Roadmap page details what is planned and how to influence the roadmap.

Authors
Ansible was created by Michael DeHaan
and has contributions from over 4700 users (and growing). Thanks everyone!
Ansible is sponsored by Red Hat, Inc.

License
GNU General Public License v3.0 or later
See COPYING to see the full text.
"
44,Python,"    
 
 


scikit-learn
scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.
The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the About us page
for a list of core contributors.
It is currently maintained by a team of volunteers.
Website: http://scikit-learn.org

Installation

Dependencies
scikit-learn requires:

Python (>= 3.5)
NumPy (>= 1.11.0)
SciPy (>= 0.17.0)
joblib (>= 0.11)

Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.
scikit-learn 0.21 and later require Python 3.5 or newer.
Scikit-learn plotting capabilities (i.e., functions start with plot_
and classes end with ""Display"") require Matplotlib (>= 1.5.1). For running the
examples Matplotlib >= 1.5.1 is required. A few examples require
scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0.

User installation
If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using pip
pip install -U scikit-learn

or conda:
conda install scikit-learn

The documentation includes more detailed installation instructions.

Changelog
See the changelog
for a history of notable changes to scikit-learn.

Development
We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
Development Guide
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.

Important links

Official source code repo: https://github.com/scikit-learn/scikit-learn
Download releases: https://pypi.org/project/scikit-learn/
Issue tracker: https://github.com/scikit-learn/scikit-learn/issues


Source code
You can check the latest sources with the command:
git clone https://github.com/scikit-learn/scikit-learn.git


Contributing
To learn more about making a contribution to scikit-learn, please see our
Contributing guide.

Testing
After installation, you can launch the test suite from outside the
source directory (you will need to have pytest >= 3.3.0 installed):
pytest sklearn

See the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing
for more information.

Random number generation can be controlled during testing by setting
the SKLEARN_SEED environment variable.

Submitting a Pull Request
Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: http://scikit-learn.org/stable/developers/index.html

Project History
The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the About us page
for a list of core contributors.
The project is currently maintained by a team of volunteers.
Note: scikit-learn was previously referred to as scikits.learn.

Help and Support

Documentation

HTML documentation (stable release): http://scikit-learn.org
HTML documentation (development version): http://scikit-learn.org/dev/
FAQ: http://scikit-learn.org/stable/faq.html


Communication

Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
IRC channel: #scikit-learn at webchat.freenode.net
Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn
Website: http://scikit-learn.org


Citation
If you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn
"
45,Python,"Scrapy













Overview
Scrapy is a fast high-level web crawling and web scraping framework, used to
crawl websites and extract structured data from their pages. It can be used for
a wide range of purposes, from data mining to monitoring and automated testing.
Check the Scrapy homepage at https://scrapy.org for more information,
including a list of features.

Requirements

Python 3.5+
Works on Linux, Windows, Mac OSX, BSD


Install
The quick way:
pip install scrapy

See the install section in the documentation at
https://docs.scrapy.org/en/latest/intro/install.html for more details.

Documentation
Documentation is available online at https://docs.scrapy.org/ and in the docs
directory.

Releases
You can check https://docs.scrapy.org/en/latest/news.html for the release notes.

Community (blog, twitter, mail list, IRC)
See https://scrapy.org/community/ for details.

Contributing
See https://docs.scrapy.org/en/master/contributing.html for details.

Code of Conduct
Please note that this project is released with a Contributor Code of Conduct
(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).
By participating in this project you agree to abide by its terms.
Please report unacceptable behavior to opensource@scrapinghub.com.

Companies using Scrapy
See https://scrapy.org/companies/ for a list.

Commercial Support
See https://scrapy.org/support/ for details.
"
46,Python,"Big List of Naughty Strings
The Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer walks into a bar.
Why Test Naughty Strings?
Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a zero-width space (U+200B) on Twitter:

Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an ""internal server error"" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.
Usage
blns.txt consists of newline-delimited strings and comments which are preceded with #. The comments divide the strings into sections for easy manual reading and copy/pasting into input forms. For those who want to access the strings programmatically, a blns.json file is provided containing an array with all the comments stripped out (the scripts folder contains a Python script used to generate the blns.json).
Contributions
Feel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255+ characters), as that makes the list much more difficult to view.
Likewise, please do not send pull requests which compromise manual usability of the file. This includes the EICAR test string, which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of blns.txt. Also, do not send a null character (U+0000) string, as it changes the file format on GitHub to binary and renders it unreadable in pull requests. Finally, when adding or removing a string please update all files when you perform a pull request.
Disclaimer
The Big List of Naughty Strings is intended to be used for software you own and manage. Some of the Naughty Strings can indicate security vulnerabilities, and as a result using such strings with third-party software may be a crime. The maintainer is not responsible for any negative actions that result from the use of the list.
Additionally, the Big List of Naughty Strings is not a fully-comprehensive substitute for formal security/penetration testing for your service.
Library / Packages
Various implementations of the Big List of Naughty Strings have made it to various package managers.  Those are maintained by outside parties, but can be found here:



Library
Link




Node
https://www.npmjs.com/package/blns


Node
https://www.npmjs.com/package/big-list-of-naughty-strings


.NET
https://github.com/SimonCropp/NaughtyStrings



Please open a PR to list others.
Maintainer/Creator
Max Woolf (@minimaxir)
Social Media Discussions

June 10, 2015 [Hacker News]: Show HN: Big List of Naughty Strings for testing user-input data
August 17, 2015 [Reddit]: Big list of naughty strings.
February 9, 2016 [Reddit]: Big List of Naughty Strings
January 15, 2017 [Hacker News]: Naughty Strings: A list of strings likely to cause issues as user-input data
January 16, 2017 [Reddit]: Naughty Strings: A list of strings likely to cause issues as user-input data
November 16, 2018 [Hacker News]: Big List of Naughty Strings
November 16, 2018 [Reddit]: Naughty Strings - A list of strings which have a high probability of causing issues when used as user-input data

License
MIT
"
47,Python,"Removed according to regulations.
"
48,Python,"Face Recognition
You can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어.
Recognize and manipulate faces from Python or from the command line with
the world's simplest face recognition library.
Built using dlib's state-of-the-art face recognition
built with deep learning. The model has an accuracy of 99.38% on the
Labeled Faces in the Wild benchmark.
This also provides a simple face_recognition command line tool that lets
you do face recognition on a folder of images from the command line!



Features
Find faces in pictures
Find all the faces that appear in a picture:

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_locations = face_recognition.face_locations(image)
Find and manipulate facial features in pictures
Get the locations and outlines of each person's eyes, nose, mouth and chin.

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)
Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff
like applying digital make-up (think 'Meitu'):

Identify faces in pictures
Recognize who appears in each photo.

import face_recognition
known_image = face_recognition.load_image_file(""biden.jpg"")
unknown_image = face_recognition.load_image_file(""unknown.jpg"")

biden_encoding = face_recognition.face_encodings(known_image)[0]
unknown_encoding = face_recognition.face_encodings(unknown_image)[0]

results = face_recognition.compare_faces([biden_encoding], unknown_encoding)
You can even use this library with other Python libraries to do real-time face recognition:

See this example for the code.
Online Demos
User-contributed shared Jupyter notebook demo (not officially supported): 
Installation
Requirements

Python 3.3+ or Python 2.7
macOS or Linux (Windows not officially supported, but might work)

Installation Options:
Installing on Mac or Linux
First, make sure you have dlib already installed with Python bindings:

How to install dlib from source on macOS or Ubuntu

Then, install this module from pypi using pip3 (or pip2 for Python 2):
pip3 install face_recognition
Alternatively, you can try this library with Docker, see this section.
If you are having trouble with installation, you can also try out a
pre-configured VM.
Installing on an Nvidia Jetson Nano board

Jetson Nano installation instructions

Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.



Installing on Raspberry Pi 2+

Raspberry Pi 2+ installation instructions

Installing on Windows
While Windows isn't officially supported, helpful users have posted instructions on how to install this library:

@masoudr's Windows 10 installation guide (dlib + face_recognition)

Installing a pre-configured Virtual Machine image

Download the pre-configured VM image (for VMware Player or VirtualBox).

Usage
Command-Line Interface
When you install face_recognition, you get two simple command-line
programs:

face_recognition - Recognize faces in a photograph or folder full for
photographs.
face_detection - Find faces in a photograph or folder full for photographs.

face_recognition command line tool
The face_recognition command lets you recognize faces in a photograph or
folder full  for photographs.
First, you need to provide a folder with one picture of each person you
already know. There should be one image file for each person with the
files named according to who is in the picture:

Next, you need a second folder with the files you want to identify:

Then in you simply run the command face_recognition, passing in
the folder of known people and the folder (or single image) with unknown
people and it tells you who is in each image:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
There's one line in the output for each face. The data is comma-separated
with the filename and the name of the person found.
An unknown_person is a face in the image that didn't match anyone in
your folder of known people.
face_detection command line tool
The face_detection command lets you find the location (pixel coordinatates)
of any faces in an image.
Just run the command face_detection, passing in a folder of images
to check (or a single image):
$ face_detection  ./folder_with_pictures/

examples/image1.jpg,65,215,169,112
examples/image2.jpg,62,394,211,244
examples/image2.jpg,95,941,244,792
It prints one line for each face that was detected. The coordinates
reported are the top, right, bottom and left coordinates of the face (in pixels).
Adjusting Tolerance / Sensitivity
If you are getting multiple matches for the same person, it might be that
the people in your photos look very similar and a lower tolerance value
is needed to make face comparisons more strict.
You can do that with the --tolerance parameter. The default tolerance
value is 0.6 and lower numbers make face comparisons more strict:
$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
If you want to see the face distance calculated for each match in order
to adjust the tolerance setting, you can use --show-distance true:
$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None
More Examples
If you simply want to know the names of the people in each photograph but don't
care about file names, you could do this:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2

Barack Obama
unknown_person
Speeding up Face Recognition
Face recognition can be done in parallel if you have a computer with
multiple CPU cores. For example, if your system has 4 CPU cores, you can
process about 4 times as many images in the same amount of time by using
all your CPU cores in parallel.
If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:
$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/
You can also pass in --cpus -1 to use all CPU cores in your system.
Python Module
You can import the face_recognition module and then easily manipulate
faces with just a couple of lines of code. It's super easy!
API Docs: https://face-recognition.readthedocs.io.
Automatically find all the faces in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image)

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
You can also opt-in to a somewhat more accurate deep-learning-based face detection model.
Note: GPU acceleration (via NVidia's CUDA library) is required for good
performance with this model. You'll also want to enable CUDA support
when compliling dlib.
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image, model=""cnn"")

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
If you have a lot of images and a GPU, you can also
find faces in batches.
Automatically locate the facial features of a person in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)

# face_landmarks_list is now an array with the locations of each facial feature in each face.
# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.
See this example
to try it out.
Recognize faces in images and identify who they are
import face_recognition

picture_of_me = face_recognition.load_image_file(""me.jpg"")
my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]

# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!

unknown_picture = face_recognition.load_image_file(""unknown.jpg"")
unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]

# Now we can see the two face encodings are of the same person with `compare_faces`!

results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)

if results[0] == True:
    print(""It's a picture of me!"")
else:
    print(""It's not a picture of me!"")
See this example
to try it out.
Python Code Examples
All the examples are available here.
Face Detection

Find faces in a photograph
Find faces in a photograph (using deep learning)
Find faces in batches of images w/ GPU (using deep learning)
Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)

Facial Features

Identify specific facial features in a photograph
Apply (horribly ugly) digital make-up

Facial Recognition

Find and recognize unknown faces in a photograph based on photographs of known people
Identify and draw boxes around each person in a photo
Compare faces by numeric face distance instead of only True/False matches
Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)
Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)
Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)
Recognize faces on a Raspberry Pi w/ camera
Run a web service to recognize faces via HTTP (Requires Flask to be installed)
Recognize faces with a K-nearest neighbors classifier
Train multiple images per person then recognize faces using a SVM

Creating a Standalone Executable
If you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.
Articles and Guides that cover face_recognition

My article on how Face Recognition works: Modern Face Recognition with Deep Learning

Covers the algorithms and how they generally work


Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock

Covers how to use face recognition in practice


Raspberry Pi Face Recognition by Adrian Rosebrock

Covers how to use this on a Raspberry Pi


Face clustering with Python by Adrian Rosebrock

Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning



How Face Recognition Works
If you want to learn how face location and recognition work instead of
depending on a black box library, read my article.
Caveats

The face recognition model is trained on adults and does not work very well on children. It tends to mix
up children quite easy using the default comparison threshold of 0.6.
Accuracy may vary between ethnic groups. Please see this wiki page for more details.

Deployment to Cloud Hosts (Heroku, AWS, etc)
Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an app
using it to a cloud hosting provider like Heroku or AWS.
To make things easier, there's an example Dockerfile in this repo that shows how to run an app built with
face_recognition in a Docker container. With that, you should be able to deploy
to any service that supports Docker images.
You can try the Docker image locally by running: docker-compose up --build
Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.
Having problems?
If you run into problems, please read the Common Errors section of the wiki before filing a github issue.
Thanks

Many, many thanks to Davis King (@nulhom)
for creating dlib and for providing the trained facial feature detection and face encoding models
used in this library. For more information on the ResNet that powers the face encodings, check out
his blog post.
Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,
pillow, etc, etc that makes this kind of stuff so easy and fun in Python.
Thanks to Cookiecutter and the
audreyr/cookiecutter-pypackage project template
for making Python project packaging way more tolerable.

"
49,Python,"Home Assistant 

Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.
Check out home-assistant.io for a
demo, installation instructions,
tutorials and documentation.


Featured integrations

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the section on architecture and the section on creating your own
components.
If you run into issues while using Home Assistant or during development
of a component, check the Home Assistant help section of our website for further help and information.
"
50,Python,"deepfakes_faceswap


FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.









Jennifer Lawrence/Steve Buscemi FaceSwap using the Villain model

 
Make sure you check out INSTALL.md before getting started.

deepfakes_faceswap
Manifesto

FaceSwap has ethical uses.


How To setup and run the project
Overview

Extract
Train
Convert
GUI


General notes:
Help I need support!

Discord Server
FaceSwap Forum


Donate

Patreon
One time Donations

@torzdf
@andenixa
@kvrooman




How to contribute

For people interested in the generative models
For devs
For non-dev advanced users
For end-users
For haters


About github.com/deepfakes

What is this repo?
Why this repo?
Why is it named 'deepfakes' if it is not /u/deepfakes?
What if /u/deepfakes feels bad about that?


About machine learning

How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?



Manifesto
FaceSwap has ethical uses.
When faceswapping was first developed and published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia because the code was confusing and fragmentary. It required a thorough understanding of complicated AI techniques and took a lot of effort to figure it out. Until one individual brought it together into a single, cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create inappropriate content. Despite the inappropriate uses the software was given originally, it was the first AI code that anyone could download, run and learn by experimentation without having a Ph.D. in math, computer theory, psychology, and more. Before ""deepfakes"" these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers.
""Deepfakes"" changed all that and anyone could participate in AI development. To us, developers, the release of this code opened up a fantastic learning opportunity. It allowed us to build on ideas developed by others, collaborate with a variety of skilled coders, experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses.
Are there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don't even use it to create videos, we just tinker with the code to see what it does. Sadly, the media concentrates only on the unethical uses of this software. That is, unfortunately, the nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in its future. Like any technology, it can be used for good or it can be abused. It is our intention to develop FaceSwap in a way that its potential for abuse is minimized whilst maximizing its potential as a tool for learning, experimenting and, yes, for legitimate faceswapping.
We are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it's time to come out with a standard statement of what this software is and isn't as far as us developers are concerned.

FaceSwap is not for creating inappropriate content.
FaceSwap is not for changing faces without consent or with the intent of hiding its use.
FaceSwap is not for any illicit, unethical, or questionable purposes.
FaceSwap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses.

We are very troubled by the fact that FaceSwap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.
How To setup and run the project
FaceSwap is a Python program that will run on multiple Operating Systems including Windows, Linux, and MacOS.
See INSTALL.md for full installation instructions. You will need a modern GPU with CUDA support for best performance. AMD GPUs are partially supported.
Overview
The project has multiple entry points. You will have to:

Gather photos and/or videos
Extract faces from your raw photos
Train a model on the faces extracted from the photos/videos
Convert your sources with the model

Check out USAGE.md for more detailed instructions.
Extract
From your setup folder, run python faceswap.py extract. This will take photos from src folder and extract faces into extract folder.
Train
From your setup folder, run python faceswap.py train. This will take photos from two folders containing pictures of both faces and train a model that will be saved inside the models folder.
Convert
From your setup folder, run python faceswap.py convert. This will take photos from original folder and apply new faces into modified folder.
GUI
Alternatively, you can run the GUI by running python faceswap.py gui
General notes:

All of the scripts mentioned have -h/--help options with arguments that they will accept. You're smart, you can figure out how this works, right?!

NB: there is a conversion tool for video. This can be accessed by running python tools.py effmpeg -h. Alternatively, you can use ffmpeg to convert video into photos, process images, and convert images back to the video.
Some tips:
Reusing existing models will train much faster than starting from nothing.
If there is not enough training data, start with someone who looks similar, then switch the data.
Help I need support!
Discord Server
Your best bet is to join the FaceSwap Discord server where there are plenty of users willing to help. Please note that, like this repo, this is a SFW Server!
FaceSwap Forum
Alternatively, you can post questions in the FaceSwap Forum. Please do not post general support questions in this repo as they are liable to be deleted without response.
Donate
The developers work tirelessly to improve and develop FaceSwap. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.
Patreon
The best way to support us is through our Patreon page:

One time Donations
Alternatively you can give a one off donation to any of our Devs:
@torzdf
There is very little FaceSwap code that hasn't been touched by torzdf. He is responsible for implementing the GUI, FAN aligner, MTCNN detector and porting the Villain, DFL-H128 and DFaker models to FaceSwap, as well as significantly improving many areas of the code.
Bitcoin: 385a1r9tyZpt5LyZcNk1FALTxC8ZHta7yq
Ethereum: 0x18CBbff5fA7C78de7B949A2b0160A0d1bd649f80
Paypal: 
@andenixa
Creator of the Unbalanced and OHR models, as well as expanding various capabilities within the training process. Andenixa is currently working on new models and will take requests for donations.
Paypal: 
@kvrooman
Responsible for consolidating the converters, adding a lot of code to fix model stability issues, and helping significantly towards making the training process more modular, kvrooman continues to be a very active contributor.
Ethereum: 0x18CBbff5fA7C78de7B949A2b0160A0d1bd649f80
How to contribute
For people interested in the generative models

Go to the 'faceswap-model' to discuss/suggest/commit alternatives to the current algorithm.

For devs

Read this README entirely
Fork the repo
Play with it
Check issues with the 'dev' tag
For devs more interested in computer vision and openCV, look at issues with the 'opencv' tag. Also feel free to add your own alternatives/improvements

For non-dev advanced users

Read this README entirely
Clone the repo
Play with it
Check issues with the 'advuser' tag
Also go to the 'faceswap Forum' and help others.

For end-users

Get the code here and play with it if you can
You can also go to the faceswap Forum and help or get help from others.
Be patient. This is a relatively new technology for developers as well. Much effort is already being put into making this program easy to use for the average user. It just takes time!
Notice Any issue related to running the code has to be opened in the faceswap Forum!

For haters
Sorry, no time for that.
About github.com/deepfakes
What is this repo?
It is a community repository for active users.
Why this repo?
The joshua-wu repo seems not active. Simple bugs like missing http:// in front of urls have not been solved since days.
Why is it named 'deepfakes' if it is not /u/deepfakes?

Because a typosquat would have happened sooner or later as project grows
Because we wanted to recognize the original author
Because it will better federate contributors and users

What if /u/deepfakes feels bad about that?
This is a friendly typosquat, and it is fully dedicated to the project. If /u/deepfakes wants to take over this repo/user and drive the project, he is welcomed to do so (Raise an issue, and he will be contacted on Reddit). Please do not send /u/deepfakes messages for help with the code you find here.
About machine learning
How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?
It's complicated. Here's a good video that makes the process understandable:

Here's a slightly more in depth video that tries to explain the basic functioning of a neural network:

tl;dr: training data + trial and error
"
51,Python,"100-Days-Of-ML-Code
100 Days of Machine Learning Coding as proposed by Siraj Raval
Get the datasets from here
Data PreProcessing | Day 1
Check out the code from here.



Simple Linear Regression | Day 2
Check out the code from here.



Multiple Linear Regression | Day 3
Check out the code from here.



Logistic Regression | Day 4



Logistic Regression | Day 5
Moving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction.
Due to less time I will now be posting an infographic on alternate days.
Also if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) .
Implementing Logistic Regression | Day 6
Check out the Code here
K Nearest Neighbours | Day 7



Math Behind Logistic Regression | Day 8
#100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan.
It gives a detailed description of Logistic Regression. Do check it out.
Support Vector Machines | Day 9
Got an intution on what SVM is and how it is used to solve Classification problem.
SVM and KNN | Day 10
Learned more about how SVM works and implementing the K-NN algorithm.
Implementation of K-NN | Day 11
Implemented the K-NN algorithm for classification. #100DaysOfMLCode
Support Vector Machine Infographic is halfway complete. Will update it tomorrow.
Support Vector Machines | Day 12



Naive Bayes Classifier | Day 13
Continuing with #100DaysOfMLCode today I went through the Naive Bayes classifier.
I am also implementing the SVM in python using scikit-learn. Will update the code soon.
Implementation of SVM | Day 14
Today I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation.
Check the code here.
Naive Bayes Classifier and Black Box Machine Learning | Day 15
Learned about different types of naive bayes classifiers. Also started the lectures by Bloomberg. First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.
Implemented SVM using Kernel Trick | Day 16
Using Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane.
Started Deep learning Specialization on Coursera | Day 17
Completed the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network.
Deep learning Specialization on Coursera | Day 18
Completed the Course 1 of the deep learning specialization. Implemented a neural net in python.
The Learning Problem , Professor Yaser Abu-Mostafa | Day 19
Started Lecture 1 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm.
Started Deep learning Specialization Course 2 | Day 20
Completed the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.
Web Scraping | Day 21
Watched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model.
Is Learning Feasible? | Day 22
Lecture 2 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality.
Decision Trees | Day 23



Introduction To Statistical Learning Theory | Day 24
Lec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.
Implementing Decision Trees | Day 25
Check the code here.
Jumped To Brush up Linear Algebra | Day 26
Found an amazing channel on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication.
Link to the playlist here.
Jumped To Brush up Linear Algebra | Day 27
Continuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices.
Link to the playlist here.
Jumped To Brush up Linear Algebra | Day 28
In the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra.
Topics covered were Dot Product and Cross Product.
Link to the playlist here.
Jumped To Brush up Linear Algebra | Day 29
Completed the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra.
Topics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces.
Link to the playlist here.
Essence of calculus | Day 30
Completing the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it.
Completed about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential.
Link to the playlist here.
Essence of calculus | Day 31
Watched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus.
Link to the playlist here.
Essence of calculus | Day 32
Watched the remaining 4 videos covering topics Like Integration and Higher order derivatives.
Link to the playlist here.
Random Forests | Day 33



Implementing Random Forests | Day 34
Check the code here.
But what is a Neural Network? | Deep learning, chapter 1  | Day 35
An Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept.
Link To the video.
Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36
Part two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended.
Link To the video.
What is backpropagation really doing? | Deep learning, chapter 3 | Day 37
Part three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation.
Link To the video.
Backpropagation calculus | Deep learning, chapter 4 | Day 38
Part four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation.
Link To the video.
Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39
Link To the video.
Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40
Link To the video.
Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41
Link To the video.
Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42
Link To the video.
K Means Clustering | Day 43
Moved to Unsupervised Learning and studied about Clustering.
Working on my website check it out avikjain.me
Also found a wonderful animation that can help to easily understand K - Means Clustering Link



K Means Clustering Implementation | Day 44
Implemented K Means Clustering. Check the code here.
Digging Deeper | NUMPY  | Day 45
Got a new book ""Python Data Science HandBook"" by JK VanderPlas Check the Jupyter notebooks here.
Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays.
Check the code -
Introduction to NumPy
Understanding Data Types in Python
The Basics of NumPy Arrays
Computation on NumPy Arrays: Universal Functions
Digging Deeper | NUMPY | Day 46
Chapter 2 : Aggregations, Comparisions and Broadcasting
Link to Notebook:
Aggregations: Min, Max, and Everything In Between
Computation on Arrays: Broadcasting
Comparisons, Masks, and Boolean Logic
Digging Deeper | NUMPY | Day 47
Chapter 2 : Fancy Indexing, sorting arrays, Struchered Data
Link to Notebook:
Fancy Indexing
Sorting Arrays
Structured Data: NumPy's Structured Arrays
Digging Deeper | PANDAS | Day 48
Chapter 3 : Data Manipulation with Pandas
 Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append.
Link To the Notebooks:
Data Manipulation with Pandas
Introducing Pandas Objects
Data Indexing and Selection
Operating on Data in Pandas
Handling Missing Data
Hierarchical Indexing
Combining Datasets: Concat and Append
Digging Deeper | PANDAS | Day 49
Chapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables.
Combining Datasets: Merge and Join
Aggregation and Grouping
Pivot Tables
Digging Deeper | PANDAS | Day 50
Chapter 3: Vectorized Strings Operations, Working with Time Series
Links to Notebooks:
Vectorized String Operations
Working with Time Series
High-Performance Pandas: eval() and query()
Digging Deeper | MATPLOTLIB | Day 51
Chapter 4: Visualization with Matplotlib
Learned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots.
Links to Notebooks:
Visualization with Matplotlib
Simple Line Plots
Simple Scatter Plots
Visualizing Errors
Density and Contour Plots
Digging Deeper | MATPLOTLIB | Day 52
Chapter 4: Visualization with Matplotlib
Learned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots.
Links to Notebooks:
Histograms, Binnings, and Density
Customizing Plot Legends
Customizing Colorbars
Multiple Subplots
Text and Annotation
Digging Deeper | MATPLOTLIB | Day 53
Chapter 4: Covered Three Dimensional Plotting in Mathplotlib.
Links to Notebooks:
Three-Dimensional Plotting in Matplotlib
Hierarchical Clustering | Day 54
Studied about Hierarchical Clustering.
Check out this amazing Visualization.



"
52,Python,"Certbot is part of EFF’s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Let’s Encrypt—an open certificate authority launched by the EFF, Mozilla, and others—and deploys it to a web server.
Anyone who has gone through the trouble of setting up a secure website knows what a hassle getting and maintaining a certificate is. Certbot and Let’s Encrypt can automate away the pain and let you turn on and manage HTTPS with simple commands. Using Certbot and Let's Encrypt is free, so there’s no need to arrange payment.
How you use Certbot depends on the configuration of your web server. The best way to get started is to use our interactive guide. It generates instructions based on your configuration settings. In most cases, you’ll need root or administrator access to your web server to run Certbot.
Certbot is meant to be run directly on your web server, not on your personal computer. If you’re using a hosted service and don’t have direct access to your web server, you might not be able to use Certbot. Check with your hosting provider for documentation about uploading certificates or using certificates issued by Let’s Encrypt.
Certbot is a fully-featured, extensible client for the Let's
Encrypt CA (or any other CA that speaks the ACME
protocol) that can automate the tasks of obtaining certificates and
configuring webservers to use them. This client runs on Unix-based operating
systems.
To see the changes made to Certbot between versions please refer to our
changelog.
Until May 2016, Certbot was named simply letsencrypt or letsencrypt-auto,
depending on install method. Instructions on the Internet, and some pieces of the
software, may still refer to this older name.

Contributing
If you'd like to contribute to this project please read Developer Guide.
This project is governed by EFF's Public Projects Code of Conduct.

How to run the client
The easiest way to install and run Certbot is by visiting certbot.eff.org,
where you can find the correct instructions for many web server and OS
combinations.  For more information, see Get Certbot.

Understanding the client in more depth
To understand what the client is doing in detail, it's important to
understand the way it uses plugins.  Please see the explanation of
plugins in
the User Guide.

Links
Documentation: https://certbot.eff.org/docs
Software project: https://github.com/certbot/certbot
Notes for developers: https://certbot.eff.org/docs/contributing.html
Main Website: https://certbot.eff.org
Let's Encrypt Website: https://letsencrypt.org
Community: https://community.letsencrypt.org
ACME spec: http://ietf-wg-acme.github.io/acme/
ACME working area in github: https://github.com/ietf-wg-acme/acme
 
  

System Requirements
See https://certbot.eff.org/docs/install.html#system-requirements.

Current Features

Supports multiple web servers:
apache/2.x
nginx/0.8.48+
webroot (adds files to webroot directories in order to prove control of
domains and obtain certs)
standalone (runs its own simple webserver to prove you control a domain)
other server software via third party plugins


The private key is generated locally on your system.
Can talk to the Let's Encrypt CA or optionally to other ACME
compliant services.
Can get domain-validated (DV) certificates.
Can revoke certificates.
Adjustable RSA key bit-length (2048 (default), 4096, ...).
Can optionally install a http -> https redirect, so your site effectively
runs https only (Apache only)
Fully automated.
Configuration changes are logged and can be reverted.
Supports an interactive text UI, or can be driven entirely from the
command line.
Free and Open Source Software, made with Python.

For extensive documentation on using and contributing to Certbot, go to https://certbot.eff.org/docs. If you would like to contribute to the project or run the latest code from git, you should read our developer guide.
"
53,Python,"

""Within C++ is a smaller, simpler, safer language struggling to get out.""
-- Bjarne Stroustrup

The C++ Core Guidelines are a collaborative effort led by Bjarne Stroustrup, much like the C++ language itself. They are the result of many
person-years of discussion and design across a number of organizations. Their design encourages general applicability and broad adoption but
they can be freely copied and modified to meet your organization's needs.
Getting started
The guidelines themselves are found at CppCoreGuidelines. The document is in GH-flavored MarkDown. It is intentionally kept simple, mostly in ASCII, to allow automatic post-processing such as language translation and reformatting. The editors maintain one
version formatted for browsing. Note that it is manually integrated and can be slightly older than the version in the master branch.
The Guidelines are a constantly evolving document without a strict ""release"" cadence. Bjarne Stroustrup periodically reviews the document and increments the version number in the introduction. Checkins that increment the version number are tagged in git.
Many of the guidelines make use of the header-only Guidelines Support Library. One implementation is available at GSL: Guidelines Support Library.
Background and scope
The aim of the guidelines is to help people to use modern C++ effectively. By ""modern C++"" we mean C++11, C++14, and C++17. In other
words, what would you like your code to look like in 5 years' time, given that you can start now? In 10 years' time?
The guidelines are focused on relatively higher-level issues, such as interfaces, resource management, memory management, and concurrency. Such
rules affect application architecture and library design. Following the rules will lead to code that is statically type-safe, has no resource
leaks, and catches many more programming logic errors than is common in code today. And it will run fast -- you can afford to do things right.
We are less concerned with low-level issues, such as naming conventions and indentation style. However, no topic that can help a programmer is
out of bounds.
Our initial set of rules emphasizes safety (of various forms) and simplicity. They may very well be too strict. We expect to have to introduce
more exceptions to better accommodate real-world needs. We also need more rules.
You will find some of the rules contrary to your expectations or even contrary to your experience. If we haven't suggested that you change your
coding style in any way, we have failed! Please try to verify or disprove rules! In particular, we'd really like to have some of our rules
backed up with measurements or better examples.
You will find some of the rules obvious or even trivial. Please remember that one purpose of a guideline is to help someone who is less
experienced or coming from a different background or language to get up to speed.
The rules are designed to be supported by an analysis tool. Violations of rules will be flagged with references (or links) to the relevant rule.
We do not expect you to memorize all the rules before trying to write code.
The rules are meant for gradual introduction into a code base. We plan to build tools for that and hope others will too.
Contributions and LICENSE
Comments and suggestions for improvements are most welcome. We plan to modify and extend this document as our understanding improves and the
language and the set of available libraries improve. More details are found at CONTRIBUTING and LICENSE.
Thanks to DigitalOcean for hosting the Standard C++ Foundation website.
"
54,Python,"Deep Learning Papers Reading Roadmap

If you are a newcomer to the Deep Learning area, the first question you may have is ""Which paper should I start reading from?""


Here is a reading roadmap of Deep Learning papers!

The roadmap is constructed in accordance with the following four guidelines:

From outline to detail
From old to state-of-the-art
from generic to specific areas
focus on state-of-the-art

You will find many papers that are quite new but really worth reading.
I would continue adding papers to this roadmap.

1 Deep Learning History and Basics
1.0 Book
[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. ""Deep learning."" An MIT Press book. (2015). [html] (Deep Learning Bible, you can read this book while reading following papers.) ⭐️⭐️⭐️⭐️⭐️
1.1 Survey
[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. ""Deep learning."" Nature 521.7553 (2015): 436-444. [pdf] (Three Giants' Survey) ⭐️⭐️⭐️⭐️⭐️
1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)
[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. ""A fast learning algorithm for deep belief nets."" Neural computation 18.7 (2006): 1527-1554. [pdf](Deep Learning Eve) ⭐️⭐️⭐️
[3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. ""Reducing the dimensionality of data with neural networks."" Science 313.5786 (2006): 504-507. [pdf] (Milestone, Show the promise of deep learning) ⭐️⭐️⭐️
1.3 ImageNet Evolution（Deep Learning broke out from here）
[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ""Imagenet classification with deep convolutional neural networks."" Advances in neural information processing systems. 2012. [pdf] (AlexNet, Deep Learning Breakthrough) ⭐️⭐️⭐️⭐️⭐️
[5] Simonyan, Karen, and Andrew Zisserman. ""Very deep convolutional networks for large-scale image recognition."" arXiv preprint arXiv:1409.1556 (2014). [pdf] (VGGNet,Neural Networks become very deep!) ⭐️⭐️⭐️
[6] Szegedy, Christian, et al. ""Going deeper with convolutions."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [pdf] (GoogLeNet) ⭐️⭐️⭐️
[7] He, Kaiming, et al. ""Deep residual learning for image recognition."" arXiv preprint arXiv:1512.03385 (2015). [pdf] (ResNet,Very very deep networks, CVPR best paper) ⭐️⭐️⭐️⭐️⭐️
1.4 Speech Recognition Evolution
[8] Hinton, Geoffrey, et al. ""Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups."" IEEE Signal Processing Magazine 29.6 (2012): 82-97. [pdf] (Breakthrough in speech recognition)⭐️⭐️⭐️⭐️
[9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. ""Speech recognition with deep recurrent neural networks."" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (RNN)⭐️⭐️⭐️
[10] Graves, Alex, and Navdeep Jaitly. ""Towards End-To-End Speech Recognition with Recurrent Neural Networks."" ICML. Vol. 14. 2014. [pdf]⭐️⭐️⭐️
[11] Sak, Haşim, et al. ""Fast and accurate recurrent neural network acoustic models for speech recognition."" arXiv preprint arXiv:1507.06947 (2015). [pdf] (Google Speech Recognition System) ⭐️⭐️⭐️
[12] Amodei, Dario, et al. ""Deep speech 2: End-to-end speech recognition in english and mandarin."" arXiv preprint arXiv:1512.02595 (2015). [pdf] (Baidu Speech Recognition System) ⭐️⭐️⭐️⭐️
[13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig ""Achieving Human Parity in Conversational Speech Recognition."" arXiv preprint arXiv:1610.05256 (2016). [pdf] (State-of-the-art in speech recognition, Microsoft) ⭐️⭐️⭐️⭐️

After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.

#2 Deep Learning Method
2.1 Model
[14] Hinton, Geoffrey E., et al. ""Improving neural networks by preventing co-adaptation of feature detectors."" arXiv preprint arXiv:1207.0580 (2012). [pdf] (Dropout) ⭐️⭐️⭐️
[15] Srivastava, Nitish, et al. ""Dropout: a simple way to prevent neural networks from overfitting."" Journal of Machine Learning Research 15.1 (2014): 1929-1958. [pdf] ⭐️⭐️⭐️
[16] Ioffe, Sergey, and Christian Szegedy. ""Batch normalization: Accelerating deep network training by reducing internal covariate shift."" arXiv preprint arXiv:1502.03167 (2015). [pdf] (An outstanding Work in 2015) ⭐️⭐️⭐️⭐️
[17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. ""Layer normalization."" arXiv preprint arXiv:1607.06450 (2016). [pdf] (Update of Batch Normalization) ⭐️⭐️⭐️⭐️
[18] Courbariaux, Matthieu, et al. ""Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1."" [pdf] (New Model,Fast) ⭐️⭐️⭐️
[19] Jaderberg, Max, et al. ""Decoupled neural interfaces using synthetic gradients."" arXiv preprint arXiv:1608.05343 (2016). [pdf] (Innovation of Training Method,Amazing Work) ⭐️⭐️⭐️⭐️⭐️
[20] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. ""Net2net: Accelerating learning via knowledge transfer."" arXiv preprint arXiv:1511.05641 (2015). [pdf] (Modify previously trained network to reduce training epochs) ⭐️⭐️⭐️
[21] Wei, Tao, et al. ""Network Morphism."" arXiv preprint arXiv:1603.01670 (2016). [pdf] (Modify previously trained network to reduce training epochs) ⭐️⭐️⭐️
2.2 Optimization
[22] Sutskever, Ilya, et al. ""On the importance of initialization and momentum in deep learning."" ICML (3) 28 (2013): 1139-1147. [pdf] (Momentum optimizer) ⭐️⭐️
[23] Kingma, Diederik, and Jimmy Ba. ""Adam: A method for stochastic optimization."" arXiv preprint arXiv:1412.6980 (2014). [pdf] (Maybe used most often currently) ⭐️⭐️⭐️
[24] Andrychowicz, Marcin, et al. ""Learning to learn by gradient descent by gradient descent."" arXiv preprint arXiv:1606.04474 (2016). [pdf] (Neural Optimizer,Amazing Work) ⭐️⭐️⭐️⭐️⭐️
[25] Han, Song, Huizi Mao, and William J. Dally. ""Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding."" CoRR, abs/1510.00149 2 (2015). [pdf] (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) ⭐️⭐️⭐️⭐️⭐️
[26] Iandola, Forrest N., et al. ""SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size."" arXiv preprint arXiv:1602.07360 (2016). [pdf] (Also a new direction to optimize NN,DeePhi Tech Startup) ⭐️⭐️⭐️⭐️
2.3 Unsupervised Learning / Deep Generative Model
[27] Le, Quoc V. ""Building high-level features using large scale unsupervised learning."" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (Milestone, Andrew Ng, Google Brain Project, Cat) ⭐️⭐️⭐️⭐️
[28] Kingma, Diederik P., and Max Welling. ""Auto-encoding variational bayes."" arXiv preprint arXiv:1312.6114 (2013). [pdf] (VAE) ⭐️⭐️⭐️⭐️
[29] Goodfellow, Ian, et al. ""Generative adversarial nets."" Advances in Neural Information Processing Systems. 2014. [pdf] (GAN,super cool idea) ⭐️⭐️⭐️⭐️⭐️
[30] Radford, Alec, Luke Metz, and Soumith Chintala. ""Unsupervised representation learning with deep convolutional generative adversarial networks."" arXiv preprint arXiv:1511.06434 (2015). [pdf] (DCGAN) ⭐️⭐️⭐️⭐️
[31] Gregor, Karol, et al. ""DRAW: A recurrent neural network for image generation."" arXiv preprint arXiv:1502.04623 (2015). [pdf] (VAE with attention, outstanding work) ⭐️⭐️⭐️⭐️⭐️
[32] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. ""Pixel recurrent neural networks."" arXiv preprint arXiv:1601.06759 (2016). [pdf] (PixelRNN) ⭐️⭐️⭐️⭐️
[33] Oord, Aaron van den, et al. ""Conditional image generation with PixelCNN decoders."" arXiv preprint arXiv:1606.05328 (2016). [pdf] (PixelCNN) ⭐️⭐️⭐️⭐️
2.4 RNN / Sequence-to-Sequence Model
[34] Graves, Alex. ""Generating sequences with recurrent neural networks."" arXiv preprint arXiv:1308.0850 (2013). [pdf] (LSTM, very nice generating result, show the power of RNN) ⭐️⭐️⭐️⭐️
[35] Cho, Kyunghyun, et al. ""Learning phrase representations using RNN encoder-decoder for statistical machine translation."" arXiv preprint arXiv:1406.1078 (2014). [pdf] (First Seq-to-Seq Paper) ⭐️⭐️⭐️⭐️
[36] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. ""Sequence to sequence learning with neural networks."" Advances in neural information processing systems. 2014. [pdf] (Outstanding Work) ⭐️⭐️⭐️⭐️⭐️
[37] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. ""Neural Machine Translation by Jointly Learning to Align and Translate."" arXiv preprint arXiv:1409.0473 (2014). [pdf] ⭐️⭐️⭐️⭐️
[38] Vinyals, Oriol, and Quoc Le. ""A neural conversational model."" arXiv preprint arXiv:1506.05869 (2015). [pdf] (Seq-to-Seq on Chatbot) ⭐️⭐️⭐️
2.5 Neural Turing Machine
[39] Graves, Alex, Greg Wayne, and Ivo Danihelka. ""Neural turing machines."" arXiv preprint arXiv:1410.5401 (2014). [pdf] (Basic Prototype of Future Computer) ⭐️⭐️⭐️⭐️⭐️
[40] Zaremba, Wojciech, and Ilya Sutskever. ""Reinforcement learning neural Turing machines."" arXiv preprint arXiv:1505.00521 362 (2015). [pdf] ⭐️⭐️⭐️
[41] Weston, Jason, Sumit Chopra, and Antoine Bordes. ""Memory networks."" arXiv preprint arXiv:1410.3916 (2014). [pdf] ⭐️⭐️⭐️
[42] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. ""End-to-end memory networks."" Advances in neural information processing systems. 2015. [pdf] ⭐️⭐️⭐️⭐️
[43] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. ""Pointer networks."" Advances in Neural Information Processing Systems. 2015. [pdf] ⭐️⭐️⭐️⭐️
[44] Graves, Alex, et al. ""Hybrid computing using a neural network with dynamic external memory."" Nature (2016). [pdf] (Milestone,combine above papers' ideas) ⭐️⭐️⭐️⭐️⭐️
2.6 Deep Reinforcement Learning
[45] Mnih, Volodymyr, et al. ""Playing atari with deep reinforcement learning."" arXiv preprint arXiv:1312.5602 (2013). [pdf]) (First Paper named deep reinforcement learning) ⭐️⭐️⭐️⭐️
[46] Mnih, Volodymyr, et al. ""Human-level control through deep reinforcement learning."" Nature 518.7540 (2015): 529-533. [pdf] (Milestone) ⭐️⭐️⭐️⭐️⭐️
[47] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. ""Dueling network architectures for deep reinforcement learning."" arXiv preprint arXiv:1511.06581 (2015). [pdf] (ICLR best paper,great idea) ⭐️⭐️⭐️⭐️
[48] Mnih, Volodymyr, et al. ""Asynchronous methods for deep reinforcement learning."" arXiv preprint arXiv:1602.01783 (2016). [pdf] (State-of-the-art method) ⭐️⭐️⭐️⭐️⭐️
[49] Lillicrap, Timothy P., et al. ""Continuous control with deep reinforcement learning."" arXiv preprint arXiv:1509.02971 (2015). [pdf] (DDPG) ⭐️⭐️⭐️⭐️
[50] Gu, Shixiang, et al. ""Continuous Deep Q-Learning with Model-based Acceleration."" arXiv preprint arXiv:1603.00748 (2016). [pdf] (NAF) ⭐️⭐️⭐️⭐️
[51] Schulman, John, et al. ""Trust region policy optimization."" CoRR, abs/1502.05477 (2015). [pdf] (TRPO) ⭐️⭐️⭐️⭐️
[52] Silver, David, et al. ""Mastering the game of Go with deep neural networks and tree search."" Nature 529.7587 (2016): 484-489. [pdf] (AlphaGo) ⭐️⭐️⭐️⭐️⭐️
2.7 Deep Transfer Learning / Lifelong Learning / especially for RL
[53] Bengio, Yoshua. ""Deep Learning of Representations for Unsupervised and Transfer Learning."" ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [pdf] (A Tutorial) ⭐️⭐️⭐️
[54] Silver, Daniel L., Qiang Yang, and Lianghao Li. ""Lifelong Machine Learning Systems: Beyond Learning Algorithms."" AAAI Spring Symposium: Lifelong Machine Learning. 2013. [pdf] (A brief discussion about lifelong learning) ⭐️⭐️⭐️
[55] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. ""Distilling the knowledge in a neural network."" arXiv preprint arXiv:1503.02531 (2015). [pdf] (Godfather's Work) ⭐️⭐️⭐️⭐️
[56] Rusu, Andrei A., et al. ""Policy distillation."" arXiv preprint arXiv:1511.06295 (2015). [pdf] (RL domain) ⭐️⭐️⭐️
[57] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. ""Actor-mimic: Deep multitask and transfer reinforcement learning."" arXiv preprint arXiv:1511.06342 (2015). [pdf] (RL domain) ⭐️⭐️⭐️
[58] Rusu, Andrei A., et al. ""Progressive neural networks."" arXiv preprint arXiv:1606.04671 (2016). [pdf] (Outstanding Work, A novel idea) ⭐️⭐️⭐️⭐️⭐️
2.8 One Shot Deep Learning
[59] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. ""Human-level concept learning through probabilistic program induction."" Science 350.6266 (2015): 1332-1338. [pdf] (No Deep Learning,but worth reading) ⭐️⭐️⭐️⭐️⭐️
[60] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. ""Siamese Neural Networks for One-shot Image Recognition.""(2015) [pdf] ⭐️⭐️⭐️
[61] Santoro, Adam, et al. ""One-shot Learning with Memory-Augmented Neural Networks."" arXiv preprint arXiv:1605.06065 (2016). [pdf] (A basic step to one shot learning) ⭐️⭐️⭐️⭐️
[62] Vinyals, Oriol, et al. ""Matching Networks for One Shot Learning."" arXiv preprint arXiv:1606.04080 (2016). [pdf] ⭐️⭐️⭐️
[63] Hariharan, Bharath, and Ross Girshick. ""Low-shot visual object recognition."" arXiv preprint arXiv:1606.02819 (2016). [pdf] (A step to large data) ⭐️⭐️⭐️⭐️
3 Applications
3.1 NLP(Natural Language Processing)
[1] Antoine Bordes, et al. ""Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing."" AISTATS(2012) [pdf] ⭐️⭐️⭐️⭐️
[2] Mikolov, et al. ""Distributed representations of words and phrases and their compositionality."" ANIPS(2013): 3111-3119 [pdf] (word2vec) ⭐️⭐️⭐️
[3] Sutskever, et al. ""“Sequence to sequence learning with neural networks."" ANIPS(2014) [pdf] ⭐️⭐️⭐️
[4] Ankit Kumar, et al. ""“Ask Me Anything: Dynamic Memory Networks for Natural Language Processing."" arXiv preprint arXiv:1506.07285(2015) [pdf] ⭐️⭐️⭐️⭐️
[5] Yoon Kim, et al. ""Character-Aware Neural Language Models."" NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [pdf] ⭐️⭐️⭐️⭐️
[6] Jason Weston, et al. ""Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks."" arXiv preprint arXiv:1502.05698(2015) [pdf] (bAbI tasks) ⭐️⭐️⭐️
[7] Karl Moritz Hermann, et al. ""Teaching Machines to Read and Comprehend."" arXiv preprint arXiv:1506.03340(2015) [pdf] (CNN/DailyMail cloze style questions) ⭐️⭐️
[8] Alexis Conneau, et al. ""Very Deep Convolutional Networks for Natural Language Processing."" arXiv preprint arXiv:1606.01781(2016) [pdf] (state-of-the-art in text classification) ⭐️⭐️⭐️
[9] Armand Joulin, et al. ""Bag of Tricks for Efficient Text Classification."" arXiv preprint arXiv:1607.01759(2016) [pdf] (slightly worse than state-of-the-art, but a lot faster) ⭐️⭐️⭐️
3.2 Object Detection
[1] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. ""Deep neural networks for object detection."" Advances in Neural Information Processing Systems. 2013. [pdf] ⭐️⭐️⭐️
[2] Girshick, Ross, et al. ""Rich feature hierarchies for accurate object detection and semantic segmentation."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [pdf] (RCNN) ⭐️⭐️⭐️⭐️⭐️
[3] He, Kaiming, et al. ""Spatial pyramid pooling in deep convolutional networks for visual recognition."" European Conference on Computer Vision. Springer International Publishing, 2014. [pdf] (SPPNet) ⭐️⭐️⭐️⭐️
[4] Girshick, Ross. ""Fast r-cnn."" Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] ⭐️⭐️⭐️⭐️
[5] Ren, Shaoqing, et al. ""Faster R-CNN: Towards real-time object detection with region proposal networks."" Advances in neural information processing systems. 2015. [pdf] ⭐️⭐️⭐️⭐️
[6] Redmon, Joseph, et al. ""You only look once: Unified, real-time object detection."" arXiv preprint arXiv:1506.02640 (2015). [pdf] (YOLO,Oustanding Work, really practical) ⭐️⭐️⭐️⭐️⭐️
[7] Liu, Wei, et al. ""SSD: Single Shot MultiBox Detector."" arXiv preprint arXiv:1512.02325 (2015). [pdf] ⭐️⭐️⭐️
[8] Dai, Jifeng, et al. ""R-FCN: Object Detection via
Region-based Fully Convolutional Networks."" arXiv preprint arXiv:1605.06409 (2016). [pdf] ⭐️⭐️⭐️⭐️
[9] He, Gkioxari, et al. ""Mask R-CNN"" arXiv preprint arXiv:1703.06870 (2017). [pdf] ⭐️⭐️⭐️⭐️
3.3 Visual Tracking
[1] Wang, Naiyan, and Dit-Yan Yeung. ""Learning a deep compact image representation for visual tracking."" Advances in neural information processing systems. 2013. [pdf] (First Paper to do visual tracking using Deep Learning,DLT Tracker) ⭐️⭐️⭐️
[2] Wang, Naiyan, et al. ""Transferring rich feature hierarchies for robust visual tracking."" arXiv preprint arXiv:1501.04587 (2015). [pdf] (SO-DLT) ⭐️⭐️⭐️⭐️
[3] Wang, Lijun, et al. ""Visual tracking with fully convolutional networks."" Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] (FCNT) ⭐️⭐️⭐️⭐️
[4] Held, David, Sebastian Thrun, and Silvio Savarese. ""Learning to Track at 100 FPS with Deep Regression Networks."" arXiv preprint arXiv:1604.01802 (2016). [pdf] (GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods) ⭐️⭐️⭐️⭐️
[5] Bertinetto, Luca, et al. ""Fully-Convolutional Siamese Networks for Object Tracking."" arXiv preprint arXiv:1606.09549 (2016). [pdf] (SiameseFC,New state-of-the-art for real-time object tracking) ⭐️⭐️⭐️⭐️
[6] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. ""Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking."" ECCV (2016) [pdf] (C-COT) ⭐️⭐️⭐️⭐️
[7] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. ""Modeling and Propagating CNNs in a Tree Structure for Visual Tracking."" arXiv preprint arXiv:1608.07242 (2016). [pdf] (VOT2016 Winner,TCNN) ⭐️⭐️⭐️⭐️
3.4 Image Caption
[1] Farhadi,Ali,etal. ""Every picture tells a story: Generating sentences from images"". In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [pdf] ⭐️⭐️⭐️
[2] Kulkarni, Girish, et al. ""Baby talk: Understanding and generating image descriptions"". In Proceedings of the 24th CVPR, 2011. [pdf]⭐️⭐️⭐️⭐️
[3] Vinyals, Oriol, et al. ""Show and tell: A neural image caption generator"". In arXiv preprint arXiv:1411.4555, 2014. [pdf]⭐️⭐️⭐️
[4] Donahue, Jeff, et al. ""Long-term recurrent convolutional networks for visual recognition and description"". In arXiv preprint arXiv:1411.4389 ,2014. [pdf]
[5] Karpathy, Andrej, and Li Fei-Fei. ""Deep visual-semantic alignments for generating image descriptions"". In arXiv preprint arXiv:1412.2306, 2014. [pdf]⭐️⭐️⭐️⭐️⭐️
[6] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. ""Deep fragment embeddings for bidirectional image sentence mapping"". In Advances in neural information processing systems, 2014. [pdf]⭐️⭐️⭐️⭐️
[7] Fang, Hao, et al. ""From captions to visual concepts and back"". In arXiv preprint arXiv:1411.4952, 2014. [pdf]⭐️⭐️⭐️⭐️⭐️
[8] Chen, Xinlei, and C. Lawrence Zitnick. ""Learning a recurrent visual representation for image caption generation"". In arXiv preprint arXiv:1411.5654, 2014. [pdf]⭐️⭐️⭐️⭐️
[9] Mao, Junhua, et al. ""Deep captioning with multimodal recurrent neural networks (m-rnn)"". In arXiv preprint arXiv:1412.6632, 2014. [pdf]⭐️⭐️⭐️
[10] Xu, Kelvin, et al. ""Show, attend and tell: Neural image caption generation with visual attention"". In arXiv preprint arXiv:1502.03044, 2015. [pdf]⭐️⭐️⭐️⭐️⭐️
3.5 Machine Translation

Some milestone papers are listed in RNN / Seq-to-Seq topic.

[1] Luong, Minh-Thang, et al. ""Addressing the rare word problem in neural machine translation."" arXiv preprint arXiv:1410.8206 (2014). [pdf] ⭐️⭐️⭐️⭐️
[2] Sennrich, et al. ""Neural Machine Translation of Rare Words with Subword Units"". In arXiv preprint arXiv:1508.07909, 2015. [pdf]⭐️⭐️⭐️
[3] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. ""Effective approaches to attention-based neural machine translation."" arXiv preprint arXiv:1508.04025 (2015). [pdf] ⭐️⭐️⭐️⭐️
[4] Chung, et al. ""A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation"". In arXiv preprint arXiv:1603.06147, 2016. [pdf]⭐️⭐️
[5] Lee, et al. ""Fully Character-Level Neural Machine Translation without Explicit Segmentation"". In arXiv preprint arXiv:1610.03017, 2016. [pdf]⭐️⭐️⭐️⭐️⭐️
[6] Wu, Schuster, Chen, Le, et al. ""Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"". In arXiv preprint arXiv:1609.08144v2, 2016. [pdf] (Milestone) ⭐️⭐️⭐️⭐️
3.6 Robotics
[1] Koutník, Jan, et al. ""Evolving large-scale neural networks for vision-based reinforcement learning."" Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [pdf] ⭐️⭐️⭐️
[2] Levine, Sergey, et al. ""End-to-end training of deep visuomotor policies."" Journal of Machine Learning Research 17.39 (2016): 1-40. [pdf] ⭐️⭐️⭐️⭐️⭐️
[3] Pinto, Lerrel, and Abhinav Gupta. ""Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours."" arXiv preprint arXiv:1509.06825 (2015). [pdf] ⭐️⭐️⭐️
[4] Levine, Sergey, et al. ""Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection."" arXiv preprint arXiv:1603.02199 (2016). [pdf] ⭐️⭐️⭐️⭐️
[5] Zhu, Yuke, et al. ""Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning."" arXiv preprint arXiv:1609.05143 (2016). [pdf] ⭐️⭐️⭐️⭐️
[6] Yahya, Ali, et al. ""Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search."" arXiv preprint arXiv:1610.00673 (2016). [pdf] ⭐️⭐️⭐️⭐️
[7] Gu, Shixiang, et al. ""Deep Reinforcement Learning for Robotic Manipulation."" arXiv preprint arXiv:1610.00633 (2016). [pdf] ⭐️⭐️⭐️⭐️
[8] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.""Sim-to-Real Robot Learning from Pixels with Progressive Nets."" arXiv preprint arXiv:1610.04286 (2016). [pdf] ⭐️⭐️⭐️⭐️
[9] Mirowski, Piotr, et al. ""Learning to navigate in complex environments."" arXiv preprint arXiv:1611.03673 (2016). [pdf] ⭐️⭐️⭐️⭐️
3.7 Art
[1] Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). ""Inceptionism: Going Deeper into Neural Networks"". Google Research. [html] (Deep Dream)
⭐️⭐️⭐️⭐️
[2] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. ""A neural algorithm of artistic style."" arXiv preprint arXiv:1508.06576 (2015). [pdf] (Outstanding Work, most successful method currently) ⭐️⭐️⭐️⭐️⭐️
[3] Zhu, Jun-Yan, et al. ""Generative Visual Manipulation on the Natural Image Manifold."" European Conference on Computer Vision. Springer International Publishing, 2016. [pdf] (iGAN) ⭐️⭐️⭐️⭐️
[4] Champandard, Alex J. ""Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks."" arXiv preprint arXiv:1603.01768 (2016). [pdf] (Neural Doodle) ⭐️⭐️⭐️⭐️
[5] Zhang, Richard, Phillip Isola, and Alexei A. Efros. ""Colorful Image Colorization."" arXiv preprint arXiv:1603.08511 (2016). [pdf] ⭐️⭐️⭐️⭐️
[6] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. ""Perceptual losses for real-time style transfer and super-resolution."" arXiv preprint arXiv:1603.08155 (2016). [pdf] ⭐️⭐️⭐️⭐️
[7] Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. ""A learned representation for artistic style."" arXiv preprint arXiv:1610.07629 (2016). [pdf] ⭐️⭐️⭐️⭐️
[8] Gatys, Leon and Ecker, et al.""Controlling Perceptual Factors in Neural Style Transfer."" arXiv preprint arXiv:1611.07865 (2016). [pdf] (control style transfer over spatial location,colour information and across spatial scale)⭐️⭐️⭐️⭐️
[9] Ulyanov, Dmitry and Lebedev, Vadim, et al. ""Texture Networks: Feed-forward Synthesis of Textures and Stylized Images."" arXiv preprint arXiv:1603.03417(2016). [pdf] (texture generation and style transfer) ⭐️⭐️⭐️⭐️
3.8 Object Segmentation
[1] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation.” in CVPR, 2015. [pdf] ⭐️⭐️⭐️⭐️⭐️
[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. ""Semantic image segmentation with deep convolutional nets and fully connected crfs."" In ICLR, 2015. [pdf] ⭐️⭐️⭐️⭐️⭐️
[3] Pinheiro, P.O., Collobert, R., Dollar, P. ""Learning to segment object candidates."" In: NIPS. 2015. [pdf] ⭐️⭐️⭐️⭐️
[4] Dai, J., He, K., Sun, J. ""Instance-aware semantic segmentation via multi-task network cascades."" in CVPR. 2016 [pdf] ⭐️⭐️⭐️
[5] Dai, J., He, K., Sun, J. ""Instance-sensitive Fully Convolutional Networks."" arXiv preprint arXiv:1603.08678 (2016). [pdf] ⭐️⭐️⭐️
"
55,Python,"2019年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。持续更新中。

阿里篇
华为篇
百度篇
腾讯篇
美团篇
头条篇
滴滴篇
京东篇
MySQL篇
Redis篇
MongoDB篇
Zookeeper篇
Nginx篇
算法篇
内存篇
cpu篇
磁盘篇
网络通信篇
安全篇
并发篇

阿里篇

1.1.1 如何实现一个高效的单向链表逆序输出？
1.1.2 已知sqrt(2)约等于1.414，要求不用数学库，求sqrt(2)精确到小数点后10位
1.1.3 给定一个二叉搜索树(BST)，找到树中第 K 小的节点
1.1.4 LRU缓存机制
1.1.5 关于epoll和select的区别，以下哪些说法是正确的
1.1.6 从innodb的索引结构分析，为什么索引的 key 长度不能太长
1.1.7 MySQL的数据如何恢复到任意时间点？
1.1.8 NFS 和 SMB 是最常见的两种 NAS（Network Attached Storage）协议，当把一个文件系统同时通过 NFS 和 SMB 协议共享给多个主机访问时，以下哪些说法是错误的
1.1.9 输入 ping IP 后敲回车，发包前会发生什么？
1.2.0 请解释下为什么鹿晗发布恋情的时候，微博系统会崩溃，如何解决？
1.2.1 现有一批邮件需要发送给订阅顾客，且有一个集群（集群的节点数不定，会动态扩容缩容）来负责具体的邮件发送任务，如何让系统尽快地完成发送？
1.2.2 有一批气象观测站，现需要获取这些站点的观测数据，并存储到 Hive 中。但是气象局只提供了 api 查询，每次只能查询单个观测点。那么如果能够方便快速地获取到所有的观测点的数据？
1.2.3 如何实现两金额数据相加（最多小数点两位）
1.2.4 关于并行计算的一些基础开放问题
1.2.5 请计算XILINX公司VU9P芯片的算力相当于多少TOPS，给出计算过程与公式
1.2.6 一颗现代处理器，每秒大概可以执行多少条简单的MOV指令，有哪些主要的影响因素
1.2.7 请分析 MaxCompute 产品与分布式技术的关系、当前大数据计算平台类产品的市场现状和发展趋势
1.2.8 对大数据平台中的元数据管理是怎么理解的，元数据收集管理体系是怎么样的，会对大数据应用有什么样的影响
1.2.9 你理解常见如阿里，和友商大数据平台的技术体系差异以及发展趋势和技术瓶颈，在存储和计算两个方面进行概述
1.3.0 在云计算大数据处理场景中，每天运行着成千上万的任务，每个任务都要进行 IO 读写。存储系统为了更好的服务，经常会保证高优先级的任务优先执行。当多个作业或用户访问存储系统时,如何保证优先级和公平性
1.3.1 最大频率栈
1.3.2 给定一个链表，删除链表的倒数第N个节点，并且返回链表的头结点
1.3.3 如果让你设计一个通用的、支持各种数据库秒级备份和恢复的系统，你会如何设计
1.3.4 如果让你来设计一个支持数据库、NOSQL 和大数据之间数据实时流动的数据流及处理的系统，你会考虑哪些问题？如何设计？
1.3.5 给定一个整数数组和一个整数，返回两个数组的索引，这两个索引指向的数字的加和等于指定的整数。需要最优的算法，分析算法的空间和时间复杂度
1.3.6 假如给你一个新产品，你将从哪些方面来保障它的质量？
1.3.7 请评估一下程序的执行结果？

华为篇

2.1.0 static有什么用途？（请至少说明两种）
2.1.1 引用与指针有什么区别？
2.1.2 描述实时系统的基本特性
2.1.3 全局变量和局部变量在内存中是否有区别？如果有，是什么区别？
2.1.4 什么是平衡二叉树？
2.1.5 堆栈溢出一般是由什么原因导致的？
2.1.6 什么函数不能声明为虚函数？
2.1.7 冒泡排序算法的时间复杂度是什么？
2.1.8 写出float x 与“零值”比较的if语句
2.1.9 Internet采用哪种网络协议？该协议的主要层次结构？
2.2.0 Internet物理地址和IP地址转换采用什么协议？
2.2.1 IP地址的编码分为哪俩部分？
2.2.2 用户输入M,N值，从1至N开始顺序循环数数，每数到M输出该数值，直至全部输出。写出C程序。
2.2.3 不能做switch()的参数类型是
2.2.4 int A[nSize]，其中隐藏着若干0，其余非0整数，写一个函数int Func(int* A, int nSize)，使A把0移至后面，非0整数移至数组前面并保持有序，返回值为原数据中第一个元素为0的下标。
2.2.5 写一个程序, 要求功能：求出用1，2，5这三个数不同个数组合的和为100的组合个数
2.2.6 实现一个函数，把一个字符串中的字符从小写转为大写
2.2.7 随机输入一个数，判断它是不是对称数（回文数）（如3，121，12321，45254）。不能用字符串库函数
2.2.8 求2~2000的所有素数.有足够的内存,要求尽量快
2.2.9 A,B,C,D四个进程，A向buf里面写数据，B,C,D向buf里面读数据，当A写完，且B，C，D都读一次后，A才能再写。用P，V操作实现。
2.3.0 将单向链表reverse，如ABCD变成DCBA，只能搜索链表一次。
2.3.1 将二叉树的两个孩子换位置，即左变右，右变左。不能用递规。
2.3.2 以下属于物理层的设备是？
2.3.3 在以太网中，是根据（）地址来区分不同的设备的？
2.3.4 以下为传输层协议的是？
2.3.5 以下对MAC地址描述正确的是？
2.3.6 以下属于数据链路层功能的是？
2.3.7 IEEE802.3u标准是指？
2.3.8 如果要将两计算机通过双绞线直接连接，正确的线序是？
2.3.9 在V.35和V.24规程中，控制信号RTS表示？
2.4.0 路由器作为网络互连设备，必须具备以下哪些特点？
2.4.1 路由器的作用有？
2.4.2 调用上一条历史命令的快捷键是？
2.4.3 交换机工作在OSI七层的哪一层？
2.4.4 以下对CSMA/CD描述正确的是？
2.4.5 以下对STORE ANDFORWARD描述正确的是？
2.4.6 以下对交换机工作方式描述正确的是？
2.4.7 VLAN的主要作用有？
2.4.8 在交换机中用户权限分为几个级别？
2.4.9 在路由器的配置过程中查询以S开头所有命令的方法是？
2.5.0 第一次配置路由器时可以使用的方法为？
2.5.1 在何种状态下可以为路由器改名？
2.5.2 某公司申请到一个C类IP地址，但要连接6个的子公司，最大的一个子公司有 26台计算机，每个子公司在一个网段中，则子网掩码应设为？
2.5.3 与10.110.12.29mask 255.255.255.224属于同一网段的主机IP地址是？
2.5.4 ARP协议的作用是？
2.5.5 当路由器接收的IP报文的TTL值等于1时，采取的策略是？
2.5.6 在NetWare 网络中，客户需要访问某个类型的服务器时，首先要发送一个 （）广播报文来寻找服务器？
2.5.7 IPX地址网络地址有（ ）个字节？
2.5.8 对于帧中继描述正确的是？
2.5.9 对于INVERSE ARP的描述正确的是？

百度篇

3.1.0 在函数内定义一个字符数组，用gets函数输入字符串的时候，如果输入越界，为什么程序会崩溃？
3.1.1 C++中引用与指针的区别
3.1.2 C/C++程序的内存分区
3.1.3 快速排序的思想、时间复杂度、实现以及优化方法
3.1.4 IO模型——IO多路复用机制?
3.1.5 常用的Linux命令
3.1.6 C中变量的存储类型有哪些？
3.1.7 动态规划的本质
3.1.8 实践中如何优化MySQL?
3.1.9 什么情况下设置了索引但无法使用?
3.2.0 SQL语句的优化
3.2.1 数据库索引的底层实现原理和优化
3.2.2 HTTP和HTTPS的主要区别?
3.2.3 如何设计一个高并发的系统?
3.2.4 两条相交的单向链表，如何求他们的第一个公共节点?
3.2.5 求单向局部循环链表的环入口?
3.2.6 IP地址如何在数据库中存储?
3.2.7 new/delete和malloc/free的底层实现?
3.2.8 overload、override、overwrite的介绍?
3.2.9 小端/大端机器?
3.3.0 守护进程
3.3.1 多线程的优缺点
3.3.2 长连接与短连接
3.3.3 二分图应用于最佳匹配问题（游客对房间的满意度之和最大问题）
3.3.4 class与struct的区别？
3.3.5 虚函数和纯虚函数
3.3.6 menset()函数
3.3.7 实现一个函数，对一个正整数n，算得到1需要的最少操作次数。操作规则为：如果n为偶数，将其除以2；如果n为奇数，可以加1或减1；一直处理下去。
3.3.8 找到满足条件的数组
3.3.9 一个大的含有50M个URL的记录，一个小的含有500个URL的记录，找出两个记录里相同的URL
3.4.0 海量日志数据，提取出某日访问百度次数最多的那个IP
3.4.1 有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。如何按照query的频度排序？
3.4.2 蚂蚁爬杆问题
3.4.3 当在浏览器中输入一个url后回车，后台发生了什么？比如输入url后，你看到了百度的首页，那么这一切是如何发生的呢？
3.4.4 判断两棵树是否相等，请实现两棵树是否相等的比较，相等返回1，否则返回其他值，并说明算法复杂度
3.4.5 三个警察和三个囚徒的过河问题
3.4.6 从300万字符串中找到最热门的10条
3.4.7 如何找出字典中的兄弟单词。给定一个单词a，如果通过交换单词中字母的顺序可以得到另外的单词b，那么定义b是a的兄弟单词。现在给定一个字典，用户输入一个单词，如何根据字典找出这个单词有多少个兄弟单词？
3.4.8 找出数组中出现次数超过一半的数，现在有一个数组，已知一个数出现的次数超过了一半，请用O(n)的复杂度的算法找出这个数。
3.4.9 找出被修改过的数字
3.5.0 设计DNS服务器中cache的数据结构。要求设计一个DNS的Cache结构，要求能够满足每秒5000以上的查询，满足IP数据的快速插入，查询的速度要快。（题目还给出了一系列的数据，比如：站点数总共为5000万，IP地址有1000万，等等）
3.5.1 找出给定字符串对应的序号
3.5.2 找出第k大的数字所在的位置。写一段程序，找出数组中第k大小的数，输出数所在的位置。例如{2，4，3，4，7}中，第一大的数是7，位置在4。第二大、第三大的数都是4，位置在1、3随便输出哪一个均可。
3.5.3 给40亿个不重复的unsigned int的整数，没排过序的，然后再给几个数，如何快速判断这几个数是否在那40亿个数当中?
3.5.4 在一个文件中有10G个整数，乱序排列，要求找出中位数。内存限制为2G。
3.5.5 时分秒针在一天之类重合多少次？（24小时）
3.5.6 将多个集合合并成没有交集的集合。
3.5.7 平面内有11个点，由它们连成48条不同的直线，由这些点可连成多少个三角形？

腾讯篇

Java基础
4.1.0 JAVA中的几种基本数据类型是什么，各自占用多少字节。
4.1.1 String类能被继承吗，为什么。
4.1.2 String，Stringbuffer，StringBuilder的区别。
4.1.3 ArrayList和LinkedList有什么区别。
4.1.4 讲讲类的实例化顺序，比如父类静态数据，构造函数，字段，子类静态数据，构造函数，字段，当new的时候，他们的执行顺序。
4.1.5 用过哪些Map类，都有什么区别，HashMap是线程安全的吗,并发下使用的Map是什么，他们内部原理分别是什么，比如存储方式，hashcode，扩容，默认容量等。
4.1.6 JAVA8的ConcurrentHashMap为什么放弃了分段锁，有什么问题吗，如果你来设计，你如何设计。
4.1.7 有没有有顺序的Map实现类，如果有，他们是怎么保证有序的。
4.1.8 抽象类和接口的区别，类可以继承多个类么，接口可以继承多个接口么,类可以实现多个接口么。
4.1.9 继承和聚合的区别在哪。
4.2.0 IO模型有哪些，讲讲你理解的nio ，他和bio，aio的区别是啥，谈谈reactor模型。
4.2.1 反射的原理，反射创建类实例的三种方式是什么。
4.2.2 反射中，Class.forName和ClassLoader区别 。
4.2.3 描述动态代理的几种实现方式，分别说出相应的优缺点。
4.2.4 动态代理与cglib实现的区别。
4.2.5 为什么CGlib方式可以对接口实现代理。
4.2.6 final的用途。
4.2.7 写出三种单例模式实现 。
4.2.8 如何在父类中为子类自动完成所有的hashcode和equals实现？这么做有何优劣。
4.2.9 请结合OO设计理念，谈谈访问修饰符public、private、protected、default在应用设计中的作用。
4.3.0 深拷贝和浅拷贝区别。
4.3.1 数组和链表数据结构描述，各自的时间复杂度。
4.3.2 error和exception的区别，CheckedException，RuntimeException的区别。
4.3.3 请列出5个运行时异常。
4.3.4 在自己的代码中，如果创建一个java.lang.String类，这个类是否可以被类加载器加载？为什么。
4.3.5 说一说你对java.lang.Object对象中hashCode和equals方法的理解。在什么场景下需要重新实现这两个方法。
4.3.6 在jdk1.5中，引入了泛型，泛型的存在是用来解决什么问题。
4.3.7 这样的a.hashcode() 有什么用，与a.equals(b)有什么关系。
4.3.8 有没有可能2个不相等的对象有相同的hashcode。
4.3.9 Java中的HashSet内部是如何工作的。
4.4.0 什么是序列化，怎么序列化，为什么序列化，反序列化会遇到什么问题，如何解决。
4.4.1 java8的新特性。
JVM
4.4.2 什么情况下会发生栈内存溢出。
4.4.3 JVM的内存结构，Eden和Survivor比例。
4.4.4 JVM内存为什么要分成新生代，老年代，持久代。新生代中为什么要分为Eden和Survivor。
4.4.5 JVM中一次完整的GC流程是怎样的，对象如何晋升到老年代，说说你知道的几种主要的JVM参数。
4.4.6 你知道哪几种垃圾收集器，各自的优缺点，重点讲下cms和G1，包括原理，流程，优缺点。
4.4.7 垃圾回收算法的实现原理。
4.4.8 当出现了内存溢出，你怎么排错。
4.4.9 JVM内存模型的相关知识了解多少，比如重排序，内存屏障，happen-before，主内存，工作内存等。
4.5.0 简单说说你了解的类加载器，可以打破双亲委派么，怎么打破。
4.5.1 讲讲JAVA的反射机制。
4.5.2 你们线上应用的JVM参数有哪些。
4.5.3 g1和cms区别,吞吐量优先和响应优先的垃圾收集器选择。
4.5.4 怎么打出线程栈信息。
开源框架
4.5.5 简单讲讲tomcat结构，以及其类加载器流程，线程模型等。
4.5.6 tomcat如何调优，涉及哪些参数 。
4.5.7 讲讲Spring加载流程。
4.5.8 Spring AOP的实现原理。
4.5.9 讲讲Spring事务的传播属性。
4.6.0 Spring如何管理事务的。
4.6.1 Spring怎么配置事务（具体说出一些关键的xml 元素）。
4.6.2 说说你对Spring的理解，非单例注入的原理？它的生命周期？循环注入的原理，aop的实现原理，说说aop中的几个术语，它们是怎么相互工作的。
4.6.3 Springmvc 中DispatcherServlet初始化过程。
4.6.4 netty的线程模型，netty如何基于reactor模型上实现的。
4.6.5 为什么选择netty。
4.6.6 什么是TCP粘包，拆包。解决方式是什么。
4.6.7 netty的fashwheeltimer的用法，实现原理，是否出现过调用不够准时，怎么解决。
4.6.8 netty的心跳处理在弱网下怎么办。
4.6.9 netty的通讯协议是什么样的。
4.7.0 springmvc用到的注解，作用是什么，原理。
4.7.1 springboot启动机制。
操作系统
4.7.2 Linux系统下你关注过哪些内核参数，说说你知道的。
4.7.3 Linux下IO模型有几种，各自的含义是什么。
4.7.4 epoll和poll有什么区别。
4.7.5 平时用到哪些Linux命令。
4.7.6 用一行命令查看文件的最后五行。
4.7.7 用一行命令输出正在运行的java进程。
4.7.8 介绍下你理解的操作系统中线程切换过程。
4.7.9 进程和线程的区别。
4.8.0 top 命令之后有哪些内容，有什么作用。
4.8.1 线上CPU爆高，请问你如何找到问题所在。

美团篇

5.1.0 java虚拟机内存模型
5.1.1 内存溢出一般发生在哪个区？永久代会不会导致内存溢出？
5.1.2 动态加载类的框架了解哪些？
5.1.3 动态代理一般有哪几种实现方式？动态代理的应用场景有哪些？
5.1.4 栈会不会溢出？栈溢出一般抛什么异常？jvm在哪里设置栈的大小？设置的参数是什么？
5.1.5 用过哪些命令查看jvm的状态、堆栈信息？
5.1.6 jvm的垃圾回收机制？
5.1.7 java类加载机制？如何实现自定义类加载器？findClass与loadClass的区别？
5.1.8 String、StringBuffer、StringBuilder的区别？对应的使用场景？
5.1.9 如何实现不可变的类？
5.2.0 浅复制和深复制？怎样实现深复制？
5.2.1 HashMap、HashTable、ConcurrentHashMap的区别？
5.2.2 CAS是一种什么样的同步机制？
5.2.3 NIO的原理，包括哪几个组件？
5.2.4 简单介绍一下java的反射机制？反射在哪些地方有应用场景？
5.2.5 spring加载bean的流程？
5.2.6 java线程池？线程池构造函数的几个参数含义？keepAliveTime解释一下？
5.2.7 一个接口，要去调用另外5个接口，每一个接口都会返回数据给这个调用接口，调用接口要对数据进行合并并返回给上层。这样一种场景可能用到并发包下的哪些类？你会怎么去实现这样的业务场景？
5.2.8 CountDownLatch和CyclicBarrier的区别？
5.2.9 线程加锁有哪些方式？synchronized和lock的区别？
5.3.0 volatile关键字的作用？为什么使用AtomicLong而不使用Long?AtomicLong的底层是怎么实现的？
5.3.1 mysql的存储引擎有哪几种？
5.3.2 sql优化有哪些着手点？组合索引的最左前缀原则的含义？
5.3.3 springmvc处理请求的流程？
5.3.4 spring的事务怎么使用？事务回滚？自定义异常？
5.3.5 脏读？幻读？
5.3.6 tcp四次挥手的过程？TIME_WAIT为什么至少设置两倍的MSL时间？
5.3.7 get和post请求的区别？
5.3.8 cookie和session的请求？
5.3.9 了解哪些开源的中间件？缓存？消息？分布式框架？
5.4.0 用到过哪些设计模式？单例模式的实现？
5.4.1 数据库的事务实现原理、操作过程、如何做到事物之间的独立性等问题
5.4.2 数据库的脏读，幻读，不可重复读出现的原因原理，解决办法
5.4.3 数据库的隔离级别、MVCC
5.4.4 乐观锁、悲观锁、互斥锁、读写锁的原理实现与区别
5.4.5 线程的生命周期
5.4.6 一致性hash算法原理与应用
5.4.7 CAP原则
5.4.8 CAS操作
5.4.9 分布式raft算法

头条篇

6.1.0 5个人去一个海岛寻宝，最后一共找到了100枚金币。他们约定了一个分配方案。
6.1.1 给你一个有序整数数组，数组中的数可以是正数、负数、零，请实现一个函数，这个函数返回一个整数：返回这个数组所有数的平方值中有多少种不同的取值。
6.1.2 一个环有10个节点，编号0-9。从0点出发，走N步又能回到0点，共有多少种走法？
6.1.3 一个乱序数组，求第K大的数。排序方式使用字典序。
6.1.4 一棵二叉树，求最大通路长度。（即最大左右子树高度之和）
6.1.5 进程和线程的区别，使用线程真的能节省时间？
6.1.6 go协程的调度方式，使用协程真的能节省时间？
6.1.7 水平触发边沿触发的区别？在边沿触发下，一个socket有500的数据，已读取200然后不再处理，是不是剩下的300就永远无法读取？
6.1.8 有函数如下，输入1，返回什么？
6.1.9 设计http协议，A端发送 AAAA，至少让B端知道AAAA已发送完成。
6.2.0 流量总入口为api_gateway，api_gateway挂了会导致全部挂挂，用什么机制增大可用性？
6.2.1 mysql为什么要用b+树，不用平衡二叉树做索引结构？
6.2.2 创建数据库索引应该怎么考虑？
6.2.3 使用int 做primary key和使用string 有什么优劣？
6.2.4 数据库分表的方法？
6.2.5 表结构，订单纪录如下，写一个语句，求卖的最好的 top 10 product_id。
6.2.6 微服务，A服务请求B服务B1接口，B1接口又请求A服务A2接口。会不会有问题？
6.2.7 不使用高级工具，只使用Linux自带的工具，你会如何debug?
6.2.8 如何预估一个mysql语句的性能？
6.2.9 go函数中，返回值未命名，发生了panic，但是在函数内recover了。函数返回什么值？
6.3.0 socket中，在tcp协议层面，数据分为10个报文发放。1-7次很顺利，第8次丢失。这次通信一定失败吗？如果第8次数据会重发，那在接收端是不是：先读取到1-7次的数据，然后读取到8-10次的数据?还是9-10次的数据会先到达？
6.3.1 free -h，buffers 和cached有什么不同
6.3.2 后台进程有什么特点，如果要你设计一个进程是后台进程，你会考虑什么
6.3.3 僵尸进程是什么，如果产生一个僵尸进程，如何查找僵尸进程
6.3.4 孤儿进程是什么
6.3.5 一个进程有20个线程，在某个线程中调用fork，新的进程会有20个线程吗？
6.3.6 tcp/ip 流量控制和拥塞控制
6.3.7 301/302有什么区别？应用上有什么异同。
6.3.8 50X相关错误码的内涵是什么？
6.3.9 close wait和time wait是什么？如何排查？有什么意义？
6.4.0 http req和resp的中数据有哪些
6.4.1 什么是连接的半打开，半关闭状态
6.4.2 假如一个业务依赖单点redis，此redis故障将导致业务不可用，如何改进
6.4.3 redis sharding有哪些做法
6.4.4 当大量数据要求用redis保存，单机单点难以满足需要，设计（换寻找）一个负载均衡的方案
6.4.5 当redis 采用hash做sharding，现在有8个节点，负载方案是 pos = hash(key) % 8，然后保存在pos节点上。这样做有什么好处坏处？当8个节点要扩充到10个节点，应该怎么办？有什么更方便扩充的方案吗？（一致性hash, presharding）
6.4.6 如何保证redis和数据库数据的一致性。比如用户名既保存在数据库，又保存在redis做缓存。有如下操作 update_db(username); update_redis(username)。但是执行update_db后故障，update_redis没有执行。有什么简单办法解决这个问题。
6.5.0 数据库表包含三列：广告编号ad_id，广告开始投放时间ad_start，广告投放结束时间ad_end。用一行SQL语句查询给定时间段内存在的广告。
6.5.1 讲讲MapReduce的原理。
6.5.2 举出几种进程通信、线程通信的方式。
6.5.3 对列表中每一个元素找出比它大的第一个元素：输入一个listin，返回一个listout。对于任意listin[x]，将满足 y > x 且 listin[y] > listin[x] 的第一个 listin[y] 值作为 listout[x] 的值。时间复杂度限制为O(n)。
滴滴篇

7.1.0 B+树、B-树的区别?
7.1.1 数据库隔离级别，幻读和不可重复读的区别？
7.1.2 有hell, well, hello, world等字符串组，现在问能否拼接成helloworld，代码实现。
7.1.3 快排算法实现
7.1.4 线程安全的单例模式
7.1.5 25匹马赛跑，有一个赛场，只有五个赛道，没有计时器，只能通过目测来记录快慢，求出第三3快的马要多少场比赛？
7.1.6 kmp算法next数组的求解思路
7.1.7 数组中有三个数字出现超过3/4，求这三个数字？
7.1.8 1到n+2个数组中缺了两个数，如何用O(n)时间，O(1)空间找到这两个数字。
7.1.9 一条线段长为1，随机选两个点，将改线段分为三段，三段能成三角形的概率是多少？
7.2.0 有一个教授，他三个学生，脑袋背后分别各写了一个数字，其中一个数字是另外两个数字的和，经过几轮后，有一个学生猜出了自己的数字请问是什么原因？
7.2.1 B+树做索引时，B+树通常高度为多少层？要参考哪些条件？

京东篇

8.1.0 一般sql注入怎么发现触点的，从源码阐述sqlmap如何测试注入点的。
8.1.1 masscan扫描端口时靠什么检测，为什么这么快? 请详述.
8.1.2 你写过哪些小工具，你为你使用过的工具做过什么修改.
8.1.3 如何提高采用python编写的扫描速度，谈谈对GIL锁的了解.
8.1.4 你觉得你发现的那个漏洞影响比较大.
8.1.5 常见的web漏洞有哪些.
8.1.6 有没有玩过硬件安全，研究程度如何.
8.1.7 反爬虫，如果是你如何进行反爬虫，如何绕过反爬措施。 使用无头浏览器被检测到了，如何绕过
8.1.8 nmap扫描如何进行扫描。发包与协议，握手和不握手，哪些协议握手，哪些不握手. 如何不直接接触目标服务器探测对方端口是否开放
8.1.9 有没有自己编写过yara扫描模块，如果要解决扫描{k1:v1, k2:v2, k3:v3} ，保证同时在k1中的v1里出现特定值，k2中出现v2特定值，以及k3,v3。怎么实现
8.2.0 xss什么原理，如何自己实现一个beef类似的xss平台. 既然这样实现，面临的跨域如何解决?
8.2.1 ip 频率限制, ip信誉度模型？
8.2.2 SCTP协议是什么？如何使用 SCTP 优化网络？

mysql篇

9.1.0 主键 超键 候选键 外键
9.1.1 数据库事务的四个特性及含义
9.1.2 视图的作用，视图可以更改么？
9.1.3 drop,delete与truncate的区别
9.1.4 索引的工作原理及其种类
9.1.5 连接的种类
9.1.6 数据库范式
9.1.7 数据库优化的思路
9.1.8 存储过程与触发器的区别

redis篇

10.1.0 使用Redis有哪些好处？
10.1.1 redis相比memcached有哪些优势？
10.1.2 redis常见性能问题和解决方案
10.1.3 MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据
10.1.4 Memcache与Redis的区别都有哪些？
10.1.5 Redis 常见的性能问题都有哪些？如何解决？
10.1.6 redis 最适合的场景
10.1.7 Redis的同步机制了解么？
10.1.8 是否使用过Redis集群，集群的原理是什么？
10.1.9 redis集群如何保证一致性？

MongoDB篇

11.1.0 什么是MongoDB？
11.1.1 MongoDB是由哪种语言写的？
11.1.2 MongoDB的优势有哪些？
11.1.3 什么是数据库？
11.1.4 什么是集合？
11.1.5 什么是文档？
11.1.6 MongoDB和关系型数据库术语对比图
11.1.7 什么是“mongod”？
11.1.8 “mongod”参数有什么？
11.1.9 什么是“mongo”？
11.2.0 MongoDB哪个命令可以切换数据库？
11.2.1 什么是非关系型数据库？
11.2.2 非关系型数据库有哪些类型？
11.2.3 为什么用MongoDB？
11.2.4 在哪些场景使用MongoDB？
11.2.5 MongoDB中的命名空间是什么意思?
11.2.6 哪些语言支持MongoDB?
11.2.7 在MongoDB中如何创建一个新的数据库？
11.2.8 在MongoDB中如何查看数据库列表？
11.2.9 MongoDB中的分片是什么意思？
11.3.0 如何查看使用MongoDB的连接？
11.3.1 什么是复制？
11.3.2 在MongoDB中如何在集合中插入一个文档？
11.3.3 在MongoDB中如何除去一个数据库？
11.3.4 在MongoDB中如何创建一个集合？
11.3.5 在MongoDB中如何查看一个已经创建的集合？
11.3.6 在MongoDB中如何删除一个集合？
11.3.7 为什么要在MongoDB中使用分析器？
11.3.8 MongoDB支持主键外键关系吗？
11.3.9 MongoDB支持哪些数据类型？
11.4.0 为什么要在MongoDB中用""Code""数据类型？
11.4.1 为什么要在MongoDB中用""Regular Expression""数据类型？
11.4.2 为什么在MongoDB中使用""Object ID""数据类型？
11.4.3 如何在集合中插入一个文档？
11.4.4 “ObjectID”有哪些部分组成？
11.4.5 在MongoDB中什么是索引？
11.4.6 如何添加索引？
11.4.7 MongoDB有哪些可替代产品？
11.4.8 如何查询集合中的文档？
11.4.9 用什么方法可以格式化输出结果？
11.5.0 如何使用""AND""或""OR""条件循环查询集合中的文档？
11.5.1 在MongoDB中如何更新数据？
11.5.2 如何删除文档？
11.5.3 在MongoDB中如何排序？
11.5.4 什么是聚合？
11.5.5 在MongoDB中什么是副本集？
11.5.6 Mongodb存储特性与内部原理?

Zookeeper篇

12.1.0 zookeeper是什么？
12.1.1 zookeeper提供了什么？
12.1.2 zookeeper文件系统
12.1.3 zookeeper的四种类型的znode
12.1.4 zookeeper通知机制
12.1.5 zookeeper有哪些应用场景？
12.1.6 zk的命名服务
12.1.7 zk的配置管理服务
12.1.8 zk的集群管理
12.1.9 zk的分布式锁
12.2.0 zk队列管理
12.2.1 zk数据复制
12.2.2 zk的工作原理
12.2.3 zk是如何保证事物的顺序一致性
12.2.4 zk集群下server工作状态
12.2.5 zk是如何选举Leader的？
12.2.6 zk同步流程
12.2.7 分布式通知和协调
12.2.8 zk的session机制

nginx篇

13.1.0 请解释一下什么是Nginx?
13.1.1 请列举Nginx的一些特性?
13.1.2 请列举Nginx和Apache 之间的不同点?
13.1.3 请解释Nginx如何处理HTTP请求。
13.1.4 在Nginx中，如何使用未定义的服务器名称来阻止处理请求?
13.1.5 使用“反向代理服务器”的优点是什么?
13.1.6 请列举Nginx服务器的最佳用途。
13.1.7 请解释Nginx服务器上的Master和Worker进程分别是什么?
13.1.8 请解释你如何通过不同于80的端口开启Nginx?
13.1.9  请解释是否有可能将Nginx的错误替换为502错误、503?
13.2.0 在Nginx中，解释如何在URL中保留双斜线?
13.2.1 请解释ngx_http_upstream_module的作用是什么?
13.2.2 请解释什么是C10K问题，后来是怎么解决的？
13.2.3 请陈述stub_status和sub_filter指令的作用是什么?
13.2.4 解释Nginx是否支持将请求压缩到上游?
13.2.5 解释如何在Nginx中获得当前的时间?
13.2.6 用Nginx服务器解释-s的目的是什么?
13.2.7 解释如何在Nginx服务器上添加模块?
13.2.8 nginx中多个work进程是如何监听同一个端口的？如何处理客户连接的惊群问题？
13.2.9 nginx程序的热更新是如何做的？


获取大牛视频资料，决胜校招，Linux项目C/C++精讲群：725377106
C/C++ Linux技术交流群：762073882
若群已满，添加QQ：936204305 , 备注github
关注公众号，更多权威架构设计方案。 另附企业内推，架构设计资料，相关视频资料

鸣谢
感谢各位贡献patch的朋友， 还很多在issue里面出谋划策的朋友，为此衷心感谢。使得该repo能够在github趋势榜，持续一周时间问鼎排行榜。










































































加入 gitter 讨论组
https://gitter.im/im0voice/interview_internal_reference
"
56,Python,"python-patterns
A collection of design patterns and idioms in Python.
Current Patterns
Creational Patterns:



Pattern
Description




abstract_factory
use a generic function with specific factories


borg
a singleton with shared-state among instances


builder
instead of using multiple constructors, builder object receives parameters and returns constructed objects


factory
delegate a specialized function/method to create instances


lazy_evaluation
lazily-evaluated property pattern in Python


pool
preinstantiate and maintain a group of instances of the same type


prototype
use a factory and clones of a prototype for new instances (if instantiation is expensive)



Structural Patterns:



Pattern
Description




3-tier
data<->business logic<->presentation separation (strict relationships)


adapter
adapt one interface to another using a white-list


bridge
a client-provider middleman to soften interface changes


composite
lets clients treat individual objects and compositions uniformly


decorator
wrap functionality with other functionality in order to affect outputs


facade
use one class as an API to a number of others


flyweight
transparently reuse existing instances of objects with similar/identical state


front_controller
single handler requests coming to the application


mvc
model<->view<->controller (non-strict relationships)


proxy
an object funnels operations to something else



Behavioral Patterns:



Pattern
Description




chain_of_responsibility
apply a chain of successive handlers to try and process the data


catalog
general methods will call different specialized methods based on construction parameter


chaining_method
continue callback next object method


command
bundle a command and arguments to call later


iterator
traverse a container and access the container's elements


mediator
an object that knows how to connect other objects and act as a proxy


memento
generate an opaque token that can be used to go back to a previous state


observer
provide a callback for notification of events/changes to data


publish_subscribe
a source syndicates events/data to 0+ registered listeners


registry
keep track of all subclasses of a given class


specification
business rules can be recombined by chaining the business rules together using boolean logic


state
logic is organized into a discrete number of potential states and the next state that can be transitioned to


strategy
selectable operations over the same data


template
an object imposes a structure but takes pluggable components


visitor
invoke a callback for all items of a collection



Design for Testability Patterns:



Pattern
Description




dependency_injection
3 variants of dependency injection



Fundamental Patterns:



Pattern
Description




delegation_pattern
an object handles a request by delegating to a second object (the delegate)



Others:



Pattern
Description




blackboard
architectural model, assemble different sub-system knowledge to build a solution, AI approach - non gang of four pattern


graph_search
graphing algorithms - non gang of four pattern


hsm
hierarchical state machine - non gang of four pattern



Videos
Design Patterns in Python by Peter Ullrich
Sebastian Buczyński - Why you don't need design patterns in Python?
You Don't Need That!
Pluggable Libs Through Design Patterns
Contributing
When an implementation is added or modified, please review the following guidelines:
Output
All files with example patterns have ### OUTPUT ### section at the bottom
(migration to OUTPUT = """"""..."""""" is in progress).
Run append_output.sh (e.g. ./append_output.sh borg.py) to generate/update it.
Docstrings
Add module level description in form of a docstring with links to corresponding references or other useful information.
Add ""Examples in Python ecosystem"" section if you know some. It shows how patterns could be applied to real-world problems.
facade.py has a good example of detailed description,
but sometimes the shorter one as in template.py would suffice.
In some cases class-level docstring with doctest would also help (see adapter.py)
but readable OUTPUT section is much better.
Python 2 compatibility
To see Python 2 compatible versions of some patterns please check-out the legacy tag.
Update README
When everything else is done - update corresponding part of README.
Travis CI
Please run tox or tox -e ci37 before submitting a patch to be sure your changes will pass CI.
You can also run flake8 or pytest commands manually. Examples can be found in tox.ini.
Contributing via issue triage 
You can triage issues and pull requests which may include reproducing bug reports or asking for vital information, such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to python-patterns on CodeTriage.
"
57,Python,"



pandas: powerful Python data analysis toolkit

  

Latest Release














Package Status







License







Build Status















Coverage






Downloads







Gitter







What is it?
pandas is a Python package providing fast, flexible, and expressive data
structures designed to make working with ""relational"" or ""labeled"" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, real world data analysis in Python. Additionally, it has
the broader goal of becoming the most powerful and flexible open source data
analysis / manipulation tool available in any language. It is already well on
its way towards this goal.
Main Features
Here are just a few of the things that pandas does well:

Easy handling of missing data (represented as
NaN) in floating point as well as non-floating point data
Size mutability: columns can be inserted and
deleted from DataFrame and higher dimensional
objects
Automatic and explicit data alignment: objects can
be explicitly aligned to a set of labels, or the user can simply
ignore the labels and let Series, DataFrame, etc. automatically
align the data for you in computations
Powerful, flexible group by functionality to perform
split-apply-combine operations on data sets, for both aggregating
and transforming data
Make it easy to convert ragged,
differently-indexed data in other Python and NumPy data structures
into DataFrame objects
Intelligent label-based slicing, fancy
indexing, and subsetting of
large data sets
Intuitive merging and joining data
sets
Flexible reshaping and pivoting of
data sets
Hierarchical labeling of axes (possible to have multiple
labels per tick)
Robust IO tools for loading data from flat files
(CSV and delimited), Excel files, databases,
and saving/loading data from the ultrafast HDF5 format
Time series-specific functionality: date range
generation and frequency conversion, moving window statistics,
moving window linear regressions, date shifting and lagging, etc.

Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas
Binary installers for the latest released version are available at the Python
package index and on conda.
# conda
conda install pandas
# or PyPI
pip install pandas
Dependencies

NumPy
python-dateutil
pytz

See the full installation instructions for minimum supported versions of required, recommended and optional dependencies.
Installation from sources
To install pandas from source you need Cython in addition to the normal
dependencies above. Cython can be installed from pypi:
pip install cython
In the pandas directory (same one where you found this file after
cloning the git repo), execute:
python setup.py install
or for installing in development mode:
python -m pip install -e . --no-build-isolation --no-use-pep517
If you have make, you can also use make develop to run the same command.
or alternatively
python setup.py develop
See the full instructions for installing from source.
License
BSD 3
Documentation
The official documentation is hosted on PyData.org: https://pandas.pydata.org/pandas-docs/stable
Background
Work on pandas started at AQR (a quantitative hedge fund) in 2008 and
has been under active development since then.
Getting Help
For usage questions, the best place to go to is StackOverflow.
Further, general questions and discussions can also take place on the pydata mailing list.
Discussion and Development
Most development discussion is taking place on github in this repo. Further, the pandas-dev mailing list can also be used for specialized discussions or design issues, and a Gitter channel is available for quick development related questions.
Contributing to pandas 
All contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.
A detailed overview on how to contribute can be found in the contributing guide. There is also an overview on GitHub.
If you are simply looking to start working with the pandas codebase, navigate to the GitHub ""issues"" tab and start looking through interesting issues. There are a number of issues listed under Docs and good first issue where you could start out.
You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to pandas on CodeTriage.
Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!
Feel free to ask questions on the mailing list or on Gitter.
As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: Contributor Code of Conduct
"
58,Python,"Detectron is deprecated. Please see detectron2, a ground-up rewrite of Detectron in PyTorch.
Detectron
Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.
At FAIR, Detectron has enabled numerous research projects, including: Feature Pyramid Networks for Object Detection, Mask R-CNN, Detecting and Recognizing Human-Object Interactions, Focal Loss for Dense Object Detection, Non-local Neural Networks, Learning to Segment Every Thing, Data Distillation: Towards Omni-Supervised Learning, DensePose: Dense Human Pose Estimation In The Wild, and Group Normalization.


Example Mask R-CNN output.

Introduction
The goal of Detectron is to provide a high-quality, high-performance
codebase for object detection research. It is designed to be flexible in order
to support rapid implementation and evaluation of novel research. Detectron
includes implementations of the following object detection algorithms:

Mask R-CNN -- Marr Prize at ICCV 2017
RetinaNet -- Best Student Paper Award at ICCV 2017
Faster R-CNN
RPN
Fast R-CNN
R-FCN

using the following backbone network architectures:

ResNeXt{50,101,152}
ResNet{50,101,152}
Feature Pyramid Networks (with ResNet/ResNeXt)
VGG16

Additional backbone architectures may be easily implemented. For more details about these models, please see References below.
Update

4/2018: Support Group Normalization - see GN/README.md

License
Detectron is released under the Apache 2.0 license. See the NOTICE file for additional details.
Citing Detectron
If you use Detectron in your research or wish to refer to the baseline results published in the Model Zoo, please use the following BibTeX entry.
@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}

Model Zoo and Baselines
We provide a large set of baseline results and trained models available for download in the Detectron Model Zoo.
Installation
Please find installation instructions for Caffe2 and Detectron in INSTALL.md.
Quick Start: Using Detectron
After installation, please see GETTING_STARTED.md for brief tutorials covering inference and training with Detectron.
Getting Help
To start, please check the troubleshooting section of our installation instructions as well as our FAQ. If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.
If bugs are found, we appreciate pull requests (including adding Q&A's to FAQ.md and improving our installation instructions and troubleshooting documents). Please see CONTRIBUTING.md for more information about contributing to Detectron.
References

Data Distillation: Towards Omni-Supervised Learning.
Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He.
Tech report, arXiv, Dec. 2017.
Learning to Segment Every Thing.
Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick.
Tech report, arXiv, Nov. 2017.
Non-Local Neural Networks.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
Tech report, arXiv, Nov. 2017.
Mask R-CNN.
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
IEEE International Conference on Computer Vision (ICCV), 2017.
Focal Loss for Dense Object Detection.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
IEEE International Conference on Computer Vision (ICCV), 2017.
Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
Tech report, arXiv, June 2017.
Detecting and Recognizing Human-Object Interactions.
Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.
Tech report, arXiv, Apr. 2017.
Feature Pyramid Networks for Object Detection.
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Aggregated Residual Transformations for Deep Neural Networks.
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
R-FCN: Object Detection via Region-based Fully Convolutional Networks.
Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.
Conference on Neural Information Processing Systems (NIPS), 2016.
Deep Residual Learning for Image Recognition.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Conference on Neural Information Processing Systems (NIPS), 2015.
Fast R-CNN.
Ross Girshick.
IEEE International Conference on Computer Vision (ICCV), 2015.

"
59,Python,"








AI learning
组织介绍

合作or侵权，请联系: apachecn@163.com
我们不是 Apache 的官方组织/机构/团体，只是 Apache 技术栈（以及 AI）的爱好者！
ApacheCN - 学习机器学习群【629470233】


欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远

路线图

入门只看: 步骤 1 => 2 => 3，你可以当大牛！
中级补充 - 资料库: https://github.com/apachecn/ai-roadmap

1.机器学习 - 基础
基本介绍

资料来源: Machine Learning in Action(机器学习实战-个人笔记)
统一数据地址: https://github.com/apachecn/data
书籍下载地址: https://github.com/apachecn/data/tree/master/book
机器学习下载地址: https://github.com/apachecn/data/tree/master/机器学习
深度学习数据地址: https://github.com/apachecn/data/tree/master/深度学习
推荐系统数据地址: https://github.com/apachecn/data/tree/master/推荐系统
视频网站：优酷 ／bilibili / Acfun / 网易云课堂，可直接在线播放。（最下方有相应链接）
-- 推荐 红色石头: 台湾大学林轩田机器学习笔记
-- 推荐 机器学习笔记: https://feisky.xyz/machine-learning

学习文档


模块
章节
类型
负责人(GitHub)
QQ


机器学习实战
 第 1 章: 机器学习基础
介绍
@毛红动
1306014226


机器学习实战
第 2 章: KNN 近邻算法
分类
@尤永江
279393323


机器学习实战
第 3 章: 决策树
分类
@景涛
844300439


机器学习实战
第 4 章: 朴素贝叶斯
分类
@wnma3mz@分析
1003324213244970749


机器学习实战
第 5 章: Logistic回归
分类
@微光同尘
529925688


机器学习实战
第 6 章: SVM 支持向量机
分类
@王德红
934969547


网上组合内容
第 7 章: 集成方法（随机森林和 AdaBoost）
分类
@片刻
529815144


机器学习实战
第 8 章: 回归
回归
@微光同尘
529925688


机器学习实战
第 9 章: 树回归
回归
@微光同尘
529925688


机器学习实战
第 10 章: K-Means 聚类
聚类
@徐昭清
827106588


机器学习实战
第 11 章: 利用 Apriori 算法进行关联分析
频繁项集
@刘海飞
1049498972


机器学习实战
第 12 章: FP-growth 高效发现频繁项集
频繁项集
@程威
842725815


机器学习实战
第 13 章: 利用 PCA 来简化数据
工具
@廖立娟
835670618


机器学习实战
第 14 章: 利用 SVD 来简化数据
工具
@张俊皓
714974242


机器学习实战
第 15 章: 大数据与 MapReduce
工具
@wnma3mz
1003324213


Ml项目实战
第 16 章: 推荐系统（已迁移）
项目
推荐系统（迁移后地址）



第一期的总结
2017-04-08: 第一期的总结
总结
总结
529815144


网站视频

知乎问答-爆炸啦-机器学习该怎么入门？

当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。
我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上10部《机器学习》相关视频，外加国内本土风格的教程：7月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说：《机器学习实战》还不错，通俗易懂，你去试试？？
我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 ""理论+推导""，在我眼中变成了几个 ""加减乘除+循环""，我想这不就是像我这样的程序员想要的入门教程么？
很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是：没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！
最近几天，GitHub 涨了 300颗 star，加群的200人， 现在还在不断的增加++，我想大家可能都是感同身受吧！
很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是""资源收藏家""，也许新手要的就是 MachineLearning(机器学习) 学习路线图。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！

视频怎么看？



理论科班出身-建议去学习 Andrew Ng 的视频（Ng 的视频绝对是权威，这个毋庸置疑）
编码能力强 - 建议看我们的《机器学习实战-教学版》
编码能力弱 - 建议看我们的《机器学习实战-讨论版》，不过在看理论的时候，看 教学版-理论部分；讨论版的废话太多，不过在讲解代码的时候是一行一行讲解的；所以，根据自己的需求，自由的组合。


【免费】数学教学视频 - 可汗学院 入门篇


@于振梓 推荐: 可汗学院-网易公开课




概率
统计
线性代数




可汗学院(概率)
可汗学院(统计学)
可汗学院(线性代数)




机器学习视频 - ApacheCN 教学版










AcFun
B站






优酷
网易云课堂








【免费】机器/深度学习视频 - 吴恩达




机器学习
深度学习




吴恩达机器学习
神经网络和深度学习



2.深度学习
入门基础

反向传递: https://www.cnblogs.com/charlotte77/p/5629865.html
CNN原理: http://www.cnblogs.com/charlotte77/p/7759802.html
RNN原理: https://blog.csdn.net/qq_39422642/article/details/78676567
LSTM原理: https://blog.csdn.net/weixin_42111770/article/details/80900575

Pytorch - 教程
-- 待更新
TensorFlow 2.0 - 教程
-- 待更新

目录结构:


安装指南
Kears 快速入门
实战项目 1 电影情感分类
实战项目 2 汽车燃油效率
实战项目 3 优化 过拟合和欠拟合
实战项目 4 古诗词自动生成

3.自然语言处理
学习过程中-内心复杂的变化！！！
自从学习NLP以后，才发现国内与国外的典型区别:
1. 对资源的态度是完全相反的:
  1) 国内：就好像为了名气，举办工作装逼的会议，就是没有干货，全部都是象征性的PPT介绍，不是针对在做的各位
  2）国外：就好像是为了推动nlp进步一样，分享者各种干货资料和具体的实现。（特别是: python自然语言处理）
2. 论文的实现：
  1) 各种高大上的论文实现，却还是没看到一个像样的GitHub项目！（可能我的搜索能力差了点，一直没找到）
  2）国外就不举例了，我看不懂！
3. 开源的框架
  1）国外的开源框架： tensorflow/pytorch 文档+教程+视频（官方提供）
  2) 国内的开源框架: 额额，还真举例不出来！但是牛逼吹得不比国外差！（MXNet虽然有众多国人参与开发，但不能算是国内开源框架。基于MXNet的动手学深度学习(http://zh.d2l.ai & https://discuss.gluon.ai/t/topic/753)中文教程,已经由沐神(李沐)以及阿斯顿·张讲授录制，公开发布(文档+第一季教程+视频）。)
每一次深入都要去翻墙，每一次深入都要Google，每一次看着国内的说：哈工大、讯飞、中科大、百度、阿里多牛逼，但是资料还是得国外去找！
有时候真的挺恨的！真的有点瞧不起自己国内的技术环境！

当然谢谢国内很多博客大佬，特别是一些入门的Demo和基本概念。【深入的水平有限，没看懂】


【入门须知】必须了解: https://github.com/apachecn/AiLearning/tree/master/docs/nlp
【入门教程】强烈推荐: PyTorch 自然语言处理: https://github.com/apachecn/NLP-with-PyTorch
Python 自然语言处理 第二版: https://usyiyi.github.io/nlp-py-2e-zh
推荐一个liuhuanyong大佬整理的nlp全面知识体系: https://liuhuanyong.github.io
开源 - 词向量库集合:

https://www.cnblogs.com/Darwin2000/p/5786984.html
https://ai.tencent.com/ailab/nlp/embedding.html
https://blog.csdn.net/xiezj007/article/details/85073890
https://github.com/Embedding/Chinese-Word-Vectors
https://github.com/brightmart/nlp_chinese_corpus
https://github.com/codemayq/chinese_chatbot_corpus
https://github.com/candlewill/Dialog_Corpus



1.使用场景 （百度公开课）

第一部分 入门介绍


1.) 自然语言处理入门介绍


第二部分 机器翻译


2.) 机器翻译


第三部分 篇章分析


3.1.) 篇章分析-内容概述
3.2.) 篇章分析-内容标签
3.3.) 篇章分析-情感分析
3.4.) 篇章分析-自动摘要


第四部分 UNIT-语言理解与交互技术


4.) UNIT-语言理解与交互技术

应用领域
中文分词：

构建DAG图
动态规划查找，综合正反向（正向加权反向输出）求得DAG最大概率路径
使用了SBME语料训练了一套 HMM + Viterbi 模型，解决未登录词问题

1.文本分类（Text Classification）
文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。
下面是一些很好的初学者文本分类数据集。

路透社Newswire主题分类（路透社-21578）。1987年路透社出现的一系列新闻文件，按类别编制索引。另见RCV1，RCV2和TRC2。
IMDB电影评论情感分类（斯坦福）。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。
新闻组电影评论情感分类（康奈尔）。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。

有关更多信息，请参阅帖子：
单标签文本分类的数据集。

情感分析

比赛地址: https://www.kaggle.com/c/word2vec-nlp-tutorial

方案一(0.86)：WordCount + 朴素 Bayes
方案二(0.94)：LDA + 分类模型（knn/决策树/逻辑回归/svm/xgboost/随机森林）

a) 决策树效果不是很好，这种连续特征不太适合的
b) 通过参数调整 200 个topic，信息量保存效果较优（计算主题）


方案三(0.72)：word2vec + CNN

说实话：没有一个好的机器，是调不出来一个好的结果 (: 逃



通过AUC 来评估模型的效果
2.语言模型（Language Modeling）
语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。
它是语音识别和机器翻译等任务中的前置任务。
下面是一些很好的初学者语言建模数据集。

古腾堡项目，一系列免费书籍，可以用纯文本检索各种语言。
还有更多正式的语料库得到了很好的研究; 例如：
布朗大学现代美国英语标准语料库。大量英语单词样本。
谷歌10亿字语料库。


新词发现


中文分词新词发现
python3利用互信息和左右信息熵的中文分词新词发现
https://github.com/zhanzecheng/Chinese_segment_augment


句子相似度识别


项目地址: https://www.kaggle.com/c/quora-question-pairs
解决方案: word2vec + Bi-GRU


文本纠错


bi-gram + levenshtein

3.图像字幕（Image Captioning）
mage字幕是为给定图像生成文本描述的任务。
下面是一些很好的初学者图像字幕数据集。

上下文中的公共对象（COCO）。包含超过12万张带描述的图像的集合
Flickr 8K。从flickr.com获取的8千个描述图像的集合。
Flickr 30K。从flickr.com获取的3万个描述图像的集合。
欲了解更多，请看帖子：

探索图像字幕数据集，2016年
4.机器翻译（Machine Translation）
机器翻译是将文本从一种语言翻译成另一种语言的任务。
下面是一些很好的初学者机器翻译数据集。

加拿大第36届议会的协调国会议员。成对的英语和法语句子。
欧洲议会诉讼平行语料库1996-2011。句子对一套欧洲语言。
有大量标准数据集用于年度机器翻译挑战; 看到：

统计机器翻译

机器翻译


Encoder + Decoder(Attention)
参考案例: http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html

5.问答系统（Question Answering）
问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。
下面是一些很好的初学者问题回答数据集。

斯坦福问题回答数据集（SQuAD）。回答有关维基百科文章的问题。
Deepmind问题回答语料库。从每日邮报回答有关新闻文章的问题。
亚马逊问答数据。回答有关亚马逊产品的问题。
有关更多信息，请参阅帖子：

数据集：我如何获得问答网站的语料库，如Quora或Yahoo Answers或Stack Overflow来分析答案质量？
6.语音识别（Speech Recognition）
语音识别是将口语的音频转换为人类可读文本的任务。
下面是一些很好的初学者语音识别数据集。

TIMIT声学 - 语音连续语音语料库。不是免费的，但因其广泛使用而上市。口语美国英语和相关的转录。
VoxForge。用于构建用于语音识别的开源数据库的项目。
LibriSpeech ASR语料库。从LibriVox收集的大量英语有声读物。

7.自动文摘（Document Summarization）
文档摘要是创建较大文档的简短有意义描述的任务。
下面是一些很好的初学者文档摘要数据集。

法律案例报告数据集。收集了4000份法律案件及其摘要。
TIPSTER文本摘要评估会议语料库。收集了近200份文件及其摘要。
英语新闻文本的AQUAINT语料库。不是免费的，而是广泛使用的。新闻文章的语料库。
欲了解更多信息：

文档理解会议（DUC）任务。
在哪里可以找到用于文本摘要的良好数据集？

命名实体识别


Bi-LSTM CRF
参考案例: http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html
CRF推荐文档: https://www.jianshu.com/p/55755fc649b1


文本摘要


抽取式
word2vec + textrank
word2vec推荐文档: https://www.zhihu.com/question/44832436/answer/266068967
textrank推荐文档: https://blog.csdn.net/BaiHuaXiu123/article/details/77847232

Graph图计算【慢慢更新】

数据集: data/nlp/graph
学习资料: spark graphX实战.pdf 【文件太大不方便提供，自己百度】

知识图谱

知识图谱，我只认 SimmerChan: 【知识图谱-给AI装个大脑】
说实话，我是看这博主老哥写的博客长大的，写的真的是深入浅出。我很喜欢，所以就分享给大家，希望你们也喜欢。

进一步阅读
如果您希望更深入，本节提供了其他数据集列表。

维基百科研究中使用的文本数据集
数据集：计算语言学家和自然语言处理研究人员使用的主要文本语料库是什么？
斯坦福统计自然语言处理语料库
按字母顺序排列的NLP数据集列表
该机构NLTK
在DL4J上打开深度学习数据
NLP数据集
国内开放数据集: https://bosonnlp.com/dev/resource

项目负责人

Ml 第一期 (2017-02-27)


@片刻
@那伊抹微笑
@瑶妹
2017-04-08_第一期的总结


Ml 第二期 (2017-08-14)


@片刻
@那伊抹微笑
@瑶妹
@Mike


Ml 第三期 (2018-04-16)

项目贡献者

Ml 第一期 (2017-02-27)


@侯法超
@hello19883
@徐鑫
@ibe


Ml 第二期 (2017-08-14)


@Arithmetic
@Veyron C
@Cugtyt
@BBruceyuan


Ml 第三期 (2018-04-16)

群管理员换届

@瑶妹
@飞龙
@片刻
@伪文艺.
@那伊抹微笑
@LAMDA-健忘症 永久留任-非常感谢对群的贡献


Ml 第一届 (2017-09-01)


@易漠
@Mike
@Books
@李孟禹
@张假飞
@Glassy
@红色石头
@微光同尘


Ml 第二届 (2018-07-04)


@张假飞
@李孟禹
@小明教主
@平淡的天
@凌少skierゞ
@じ☆νЁ坐看云起
古柳-DesertsX
woodchuck
自由精灵
楚盟
99杆清台
时空守望者@
只想发论文的渣渣
目标: ml劝退专家


Ml 第三届 (2019-01-01)


只会喊666的存在
codefun007.xyz
荼靡
大鱼
青鸟
古柳-DesertsX
Edge
Alluka
不发篇paper不改名片
FontTian
Bigjing
仁 礼 智 爱
可啪的小乖受
老古董
时空守望者
我好菜啊
Messi 19
萌Jay小公举


Ml 第四届 (2019-06-01)


佛学爱好者
楚盟
codefun007.xyz
大鱼-群花-声优
大海
Edge
if only
李孟禹
平静
任务做不完
仁礼智爱
园时空守望者@
坐看云起
阿花君霸占路人
烦焖鸡
古柳-DesertsX
青鸟(服务员)
小明教主
zhiqing
SrL.z

欢迎贡献者不断的追加
免责声明 - 【只供学习参考】

ApacheCN 纯粹出于学习目的与个人兴趣翻译本书
ApacheCN 保留对此版本译文的署名权及其它相关权利

协议

以各项目协议为准。
ApacheCN 账号下没有协议的项目，一律视为 CC BY-NC-SA 4.0。


资料来源:

【比赛收集平台】: https://github.com/iphysresearch/DataSciComp
https://github.com/pbharrin/machinelearninginaction
https://machinelearningmastery.com/datasets-natural-language-processing

感谢信
最近无意收到群友推送的链接，发现得到大佬高度的认可，并在热心的推广
在此感谢:

量子位: https://www.zhihu.com/question/20472776/answer/691646493
人工智能前沿讲习: https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ

赞助我们



特别赞助商(欢迎“私聊”赞助)











"
60,Java,"
 
 
 
 





 算法 
操作系统
 网络 
面向对象
  数据库  
   Java   
系统设计
   工具   
编码实践
   后记   




✏️
💻
☁️
🎨
💾
☕️
💡
🔧
🍉
📝








✏️ 算法

剑指 Offer 题解
Leetcode 题解
算法
笔试面试题库

💻 操作系统

计算机操作系统
Linux

☁️ 网络

计算机网络
HTTP
Socket

🎨 面向对象

面向对象思想
设计模式

💾 数据库

数据库系统原理
SQL
Leetcode-Database 题解
MySQL
Redis

☕️ Java

Java 基础
Java 容器
Java 并发
Java 虚拟机
Java I/O

💡 系统设计

系统设计基础
分布式
集群
攻击技术
缓存
消息队列

🔧 工具

Git
Docker
构建工具
正则表达式

🍉 编码实践

代码可读性
代码风格规范

📝 后记

 我的面经  /  我的简历  /  简历模版  /  内推  /  专栏  /  QQ 群



排版
笔记内容按照 中文文案排版指北 进行排版，以保证内容的可读性。
不使用 ![]() 这种方式来引用图片，而是用 <img> 标签。一方面是为了能够控制图片以合适的大小显示，另一方面是因为 GFM 不支持 <center> ![]() </center> 这种方法让图片居中显示，只能使用 <div align=""center""> <img src=""""/> </div> 达到居中的效果。
在线排版工具：Text-Typesetting。
License
本仓库的内容不是将网上的资料随意拼凑而来，除了少部分引用书上和技术文档的原文（这部分内容都在末尾的参考链接中加了出处），其余都是我的原创。在您引用本仓库内容或者对内容进行修改演绎时，请署名并以相同方式共享，谢谢。
转载文章请在开头明显处标明该页面地址，公众号等其它转载请联系 zhengyc101@163.com。
Logo：logomakr

致谢
感谢以下人员对本仓库做出的贡献，当然不仅仅只有这些贡献者，这里就不一一列举了。如果你希望被添加到这个名单中，并且提交过 Issue 或者 PR，请与我联系。




































"
61,Java,"
Design patterns implemented in Java




Introduction
Design patterns are the best formalized practices a programmer can use to
solve common problems when designing an application or system.
Design patterns can speed up the development process by providing tested, proven
development paradigms.
Reusing design patterns help prevent subtle issues that cause major
problems, and it also improves code readability for coders and architects who
are familiar with the patterns.
Getting started
This site showcases Java Design Patterns. The solutions have been developed by
experienced programmers and architects from the open source community. The
patterns can be browsed by their high level descriptions or by looking at their
source code. The source code examples are well commented and can be thought as
programming tutorials how to implement a specific pattern. We use the most
popular battle-proven open source Java technologies.
Before you dive into the material, you should be familiar with various
software design principles.
All designs should be as simple as possible. You should start with KISS, YAGNI,
and Do The Simplest Thing That Could Possibly Work principles. Complexity and
patterns should only be introduced when they are needed for practical
extensibility.
Once you are familiar with these concepts you can start drilling down into
patterns by any of the following approaches

Using difficulty tags, Difficulty-Beginner, Difficulty-Intermediate & Difficulty-Expert.
Using pattern categories, Creational, Behavioral, and others.
Search for a specific pattern. Can't find one? Please report a new pattern here.

Hopefully you find the object oriented solutions presented on this site useful
in your architectures and have as much fun learning them as we had developing them.
How to contribute
If you are willing to contribute to the project you will find the relevant information in
our developer wiki. We will help
you and answer your questions in the Gitter chatroom.
License
This project is licensed under the terms of the MIT license.
"
62,Java,"


There is an English version of README here. just click it！
我会尽力将 LeetCode 上所有的题目都用动画的形式演示出来，计划用 3 到 4 年时间去完成它，期待与你见证这一天！
文章最新首发于微信公众号 五分钟学算法 ，您可以关注获取最新的文章。
我已经将所有文章同步到了我的个人博客，如果国内访问 GitHub 较慢（图片裂开），可以访问这个地址：https://www.cxyxiaowu.com/likou/leetcode。
汇总



序号
题目&题解
动画




0
十大经典排序算法动画与解析，看我就够了！（配代码完全版）



1
两数之和



2
两数相加



3
无重复字符的最长子串



4
寻找两个有序数组的中位数



9
回文数



10
正则表达式匹配



11
盛最多水的容器



15
三数之和



19
删除链表的倒数第 N 个节点



20
有效的括号



21
合并两个有序链表



23
合并 K 个排序链表



24
两两交换链表中的节点



25
K 个一组翻转链表



26
删除排序数组中的重复项



38
报数



41
缺失的第一个正数



66
加一



75
颜色分类



86
分割链表



92
反转链表 II



94
二叉树的中序遍历



101
对称二叉树



102
二叉树的层序遍历



103
二叉树的锯齿形层次遍历



107
二叉树的层次遍历 II



118
杨辉三角



119
杨辉三角II



110
平衡二叉树



121
买卖股票的最佳时机



122
买卖股票的最佳时机II



123
买卖股票的最佳时机III



125
验证回文串



131
分割回文串



136
只出现一次的数字



138
复制带随机指针



139
单词拆分



141
环形链表



144
二叉树的前序遍历



145
二叉树的后序遍历



146
LRU缓存机制



150
逆波兰表达式求值



153
寻找旋转排序数组中的最小值



164
最大间距



167
两数之和 II - 输入有序数组



169
求众数



172
阶乘后的零



187
重复的 DNA 序列



191
位1的个数



199
二叉树的右视图



201
数字范围按位与



203
移除链表元素



206
反转链表



209
长度最小的子数组



219
存在重复元素 II



231
2的幂



237
删除链表中的节点



239
滑动窗口最大值



242
有效的字母异位词



268
缺失数字



279
完全平方数



283
移动零



295
数据流的中位数



301
删除无效的括号



319
灯泡开关



326
3 的幂



328
奇偶链表



342
4的幂



344
反转字符串



347
前K个高频元素



349
两个数组的交集



350
两个数组的交集 II



445
两数相加 II



447
回旋镖的数量



454
四数相加 II



642
设计一个搜索自动完成系统



690
员工的重要性



877
石子游戏



1025
除数博弈



1099
小于 K 的两数之和




几篇学习算法的经验贴
六千字干货文：到底要怎么去学算法？
微信大佬总结的算法学习经验
LeetCode 刷 500 道题，笔试/面试稳吗？谈谈算法的学习
邮箱：misterbigbooo@gmail.com
喜欢就 star❤️ 一下吧！
和我交流

"
63,Java,"RxJava: Reactive Extensions for the JVM



RxJava is a Java VM implementation of Reactive Extensions: a library for composing asynchronous and event-based programs by using observable sequences.
It extends the observer pattern to support sequences of data/events and adds operators that allow you to compose sequences together declaratively while abstracting away concerns about things like low-level threading, synchronization, thread-safety and concurrent data structures.
Version 3.x (Javadoc)

single dependency: Reactive-Streams
continued support for Java 6+ & Android 2.3+
fixed API mistakes and many limits of RxJava 2
intended to be a replacement for RxJava 2 with relatively few binary incompatible changes
Java 8 lambda-friendly API
non-opinionated about the source of concurrency (threads, pools, event loops, fibers, actors, etc.)
async or synchronous execution
virtual time and schedulers for parameterized concurrency
test and diagnostic support via test schedulers, test consumers and plugin hooks

Learn more about RxJava in general on the Wiki Home.
Version 2.x
The 2.x version will be supported with bugfixes and important documentation updates until
December 31, 2020. No new features will be added to 2.x.
Version 1.x
The 1.x version is end-of-life as of March 31, 2018. No further development, support, maintenance, PRs and updates will happen. The Javadoc of the very last version, 1.3.8, will remain accessible.
Getting started
Setting up the dependency
The first step is to include RxJava 3 into your project, for example, as a Gradle compile dependency:
implementation ""io.reactivex.rxjava3:rxjava:3.x.y""
(Please replace x and y with the latest version numbers: 
)
Hello World
The second is to write the Hello World program:
package rxjava.examples;

import io.reactivex.rxjava3.core.*;

public class HelloWorld {
    public static void main(String[] args) {
        Flowable.just(""Hello world"").subscribe(System.out::println);
    }
}
If your platform doesn't support Java 8 lambdas (yet), you have to create an inner class of Consumer manually:
import io.reactivex.rxjava3.functions.Consumer;

Flowable.just(""Hello world"")
  .subscribe(new Consumer<String>() {
      @Override public void accept(String s) {
          System.out.println(s);
      }
  });
Note that RxJava 3 components now live under io.reactivex.rxjava3 and the base classes and interfaces live under io.reactivex.rxjava3.core.
Base classes
RxJava 3 features several base classes you can discover operators on:

io.reactivex.rxjava3.core.Flowable: 0..N flows, supporting Reactive-Streams and backpressure
io.reactivex.rxjava3.core.Observable: 0..N flows, no backpressure,
io.reactivex.rxjava3.core.Single: a flow of exactly 1 item or an error,
io.reactivex.rxjava3.core.Completable: a flow without items but only a completion or error signal,
io.reactivex.rxjava3.core.Maybe: a flow with no items, exactly one item or an error.

Some terminology
Upstream, downstream
The dataflows in RxJava consist of a source, zero or more intermediate steps followed by a data consumer or combinator step (where the step is responsible to consume the dataflow by some means):
source.operator1().operator2().operator3().subscribe(consumer);

source.flatMap(value -> source.operator1().operator2().operator3());
Here, if we imagine ourselves on operator2, looking to the left towards the source is called the upstream. Looking to the right towards the subscriber/consumer is called the downstream. This is often more apparent when each element is written on a separate line:
source
  .operator1()
  .operator2()
  .operator3()
  .subscribe(consumer)
Objects in motion
In RxJava's documentation, emission, emits, item, event, signal, data and message are considered synonyms and represent the object traveling along the dataflow.
Backpressure
When the dataflow runs through asynchronous steps, each step may perform different things with different speed. To avoid overwhelming such steps, which usually would manifest itself as increased memory usage due to temporary buffering or the need for skipping/dropping data, so-called backpressure is applied, which is a form of flow control where the steps can express how many items are they ready to process. This allows constraining the memory usage of the dataflows in situations where there is generally no way for a step to know how many items the upstream will send to it.
In RxJava, the dedicated Flowable class is designated to support backpressure and Observable is dedicated to the non-backpressured operations (short sequences, GUI interactions, etc.). The other types, Single, Maybe and Completable don't support backpressure nor should they; there is always room to store one item temporarily.
Assembly time
The preparation of dataflows by applying various intermediate operators happens in the so-called assembly time:
Flowable<Integer> flow = Flowable.range(1, 5)
.map(v -> v * v)
.filter(v -> v % 3 == 0)
;
At this point, the data is not flowing yet and no side-effects are happening.
Subscription time
This is a temporary state when subscribe() is called on a flow that establishes the chain of processing steps internally:
flow.subscribe(System.out::println)
This is when the subscription side-effects are triggered (see doOnSubscribe). Some sources block or start emitting items right away in this state.
Runtime
This is the state when the flows are actively emitting items, errors or completion signals:
Observable.create(emitter -> {
     while (!emitter.isDisposed()) {
         long time = System.currentTimeMillis();
         emitter.onNext(time);
         if (time % 2 != 0) {
             emitter.onError(new IllegalStateException(""Odd millisecond!""));
             break;
         }
     }
})
.subscribe(System.out::println, Throwable::printStackTrace);
Practically, this is when the body of the given example above executes.
Simple background computation
One of the common use cases for RxJava is to run some computation, network request on a background thread and show the results (or error) on the UI thread:
import io.reactivex.rxjava3.schedulers.Schedulers;

Flowable.fromCallable(() -> {
    Thread.sleep(1000); //  imitate expensive computation
    return ""Done"";
})
  .subscribeOn(Schedulers.io())
  .observeOn(Schedulers.single())
  .subscribe(System.out::println, Throwable::printStackTrace);

Thread.sleep(2000); // <--- wait for the flow to finish
This style of chaining methods is called a fluent API which resembles the builder pattern. However, RxJava's reactive types are immutable; each of the method calls returns a new Flowable with added behavior. To illustrate, the example can be rewritten as follows:
Flowable<String> source = Flowable.fromCallable(() -> {
    Thread.sleep(1000); //  imitate expensive computation
    return ""Done"";
});

Flowable<String> runBackground = source.subscribeOn(Schedulers.io());

Flowable<String> showForeground = runBackground.observeOn(Schedulers.single());

showForeground.subscribe(System.out::println, Throwable::printStackTrace);

Thread.sleep(2000);
Typically, you can move computations or blocking IO to some other thread via subscribeOn. Once the data is ready, you can make sure they get processed on the foreground or GUI thread via observeOn.
Schedulers
RxJava operators don't work with Threads or ExecutorServices directly but with so-called Schedulers that abstract away sources of concurrency behind a uniform API. RxJava 3 features several standard schedulers accessible via Schedulers utility class.

Schedulers.computation(): Run computation intensive work on a fixed number of dedicated threads in the background. Most asynchronous operators use this as their default Scheduler.
Schedulers.io(): Run I/O-like or blocking operations on a dynamically changing set of threads.
Schedulers.single(): Run work on a single thread in a sequential and FIFO manner.
Schedulers.trampoline(): Run work in a sequential and FIFO manner in one of the participating threads, usually for testing purposes.

These are available on all JVM platforms but some specific platforms, such as Android, have their own typical Schedulers defined: AndroidSchedulers.mainThread(), SwingScheduler.instance() or JavaFXSchedulers.gui().
In addition, there is an option to wrap an existing Executor (and its subtypes such as ExecutorService) into a Scheduler via Schedulers.from(Executor). This can be used, for example, to have a larger but still fixed pool of threads (unlike computation() and io() respectively).
The Thread.sleep(2000); at the end is no accident. In RxJava the default Schedulers run on daemon threads, which means once the Java main thread exits, they all get stopped and background computations may never happen. Sleeping for some time in this example situations lets you see the output of the flow on the console with time to spare.
Concurrency within a flow
Flows in RxJava are sequential in nature split into processing stages that may run concurrently with each other:
Flowable.range(1, 10)
  .observeOn(Schedulers.computation())
  .map(v -> v * v)
  .blockingSubscribe(System.out::println);
This example flow squares the numbers from 1 to 10 on the computation Scheduler and consumes the results on the ""main"" thread (more precisely, the caller thread of blockingSubscribe). However, the lambda v -> v * v doesn't run in parallel for this flow; it receives the values 1 to 10 on the same computation thread one after the other.
Parallel processing
Processing the numbers 1 to 10 in parallel is a bit more involved:
Flowable.range(1, 10)
  .flatMap(v ->
      Flowable.just(v)
        .subscribeOn(Schedulers.computation())
        .map(w -> w * w)
  )
  .blockingSubscribe(System.out::println);
Practically, parallelism in RxJava means running independent flows and merging their results back into a single flow. The operator flatMap does this by first mapping each number from 1 to 10 into its own individual Flowable, runs them and merges the computed squares.
Note, however, that flatMap doesn't guarantee any order and the items from the inner flows may end up interleaved. There are alternative operators:

concatMap that maps and runs one inner flow at a time and
concatMapEager which runs all inner flows ""at once"" but the output flow will be in the order those inner flows were created.

Alternatively, the Flowable.parallel() operator and the ParallelFlowable type help achieve the same parallel processing pattern:
Flowable.range(1, 10)
  .parallel()
  .runOn(Schedulers.computation())
  .map(v -> v * v)
  .sequential()
  .blockingSubscribe(System.out::println);
Dependent sub-flows
flatMap is a powerful operator and helps in a lot of situations. For example, given a service that returns a Flowable, we'd like to call another service with values emitted by the first service:
Flowable<Inventory> inventorySource = warehouse.getInventoryAsync();

inventorySource
    .flatMap(inventoryItem -> erp.getDemandAsync(inventoryItem.getId())
            .map(demand -> ""Item "" + inventoryItem.getName() + "" has demand "" + demand))
    .subscribe(System.out::println);
Continuations
Sometimes, when an item has become available, one would like to perform some dependent computations on it. This is sometimes called continuations and, depending on what should happen and what types are involved, may involve various operators to accomplish.
Dependent
The most typical scenario is to given a value, invoke another service, await and continue with its result:
service.apiCall()
.flatMap(value -> service.anotherApiCall(value))
.flatMap(next -> service.finalCall(next))
It is often the case also that later sequences would require values from earlier mappings. This can be achieved by moving the outer flatMap into the inner parts of the previous flatMap for example:
service.apiCall()
.flatMap(value ->
    service.anotherApiCall(value)
    .flatMap(next -> service.finalCallBoth(value, next))
)
Here, the original value will be available inside the inner flatMap, courtesy of lambda variable capture.
Non-dependent
In other scenarios, the result(s) of the first source/dataflow is irrelevant and one would like to continue with a quasi independent another source. Here, flatMap works as well:
Observable continued = sourceObservable.flatMapSingle(ignored -> someSingleSource)
continued.map(v -> v.toString())
  .subscribe(System.out::println, Throwable::printStackTrace);
however, the continuation in this case stays Observable instead of the likely more appropriate Single. (This is understandable because
from the perspective of flatMapSingle, sourceObservable is a multi-valued source and thus the mapping may result in multiple values as well).
Often though there is a way that is somewhat more expressive (and also lower overhead) by using Completable as the mediator and its operator andThen to resume with something else:
sourceObservable
  .ignoreElements()           // returns Completable
  .andThen(someSingleSource)
  .map(v -> v.toString())
The only dependency between the sourceObservable and the someSingleSource is that the former should complete normally in order for the latter to be consumed.
Deferred-dependent
Sometimes, there is an implicit data dependency between the previous sequence and the new sequence that, for some reason, was not flowing through the ""regular channels"". One would be inclined to write such continuations as follows:
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.just(count.get()))
  .subscribe(System.out::println);
Unfortunately, this prints 0 because Single.just(count.get()) is evaluated at assembly time when the dataflow hasn't even run yet. We need something that defers the evaluation of this Single source until runtime when the main source completes:
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.defer(() -> Single.just(count.get())))
  .subscribe(System.out::println);
or
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.fromCallable(() -> count.get()))
  .subscribe(System.out::println);
Type conversions
Sometimes, a source or service returns a different type than the flow that is supposed to work with it. For example, in the inventory example above, getDemandAsync could return a Single<DemandRecord>. If the code example is left unchanged, this will result in a compile-time error (however, often with a misleading error message about lack of overload).
In such situations, there are usually two options to fix the transformation: 1) convert to the desired type or 2) find and use an overload of the specific operator supporting the different type.
Converting to the desired type
Each reactive base class features operators that can perform such conversions, including the protocol conversions, to match some other type. The following matrix shows the available conversion options:




Flowable
Observable
Single
Maybe
Completable




Flowable

toObservable
first, firstOrError, single, singleOrError, last, lastOrError1
firstElement, singleElement, lastElement
ignoreElements


Observable
toFlowable2

first, firstOrError, single, singleOrError, last, lastOrError1
firstElement, singleElement, lastElement
ignoreElements


Single
toFlowable3
toObservable

toMaybe
ignoreElement


Maybe
toFlowable3
toObservable
toSingle

ignoreElement


Completable
toFlowable
toObservable
toSingle
toMaybe




1: When turning a multi-valued source into a single-valued source, one should decide which of the many source values should be considered as the result.
2: Turning an Observable into Flowable requires an additional decision: what to do with the potential unconstrained flow
of the source Observable? There are several strategies available (such as buffering, dropping, keeping the latest) via the BackpressureStrategy parameter or via standard Flowable operators such as onBackpressureBuffer, onBackpressureDrop, onBackpressureLatest which also
allow further customization of the backpressure behavior.
3: When there is only (at most) one source item, there is no problem with backpressure as it can be always stored until the downstream is ready to consume.
Using an overload with the desired type
Many frequently used operator has overloads that can deal with the other types. These are usually named with the suffix of the target type:



Operator
Overloads




flatMap
flatMapSingle, flatMapMaybe, flatMapCompletable, flatMapIterable


concatMap
concatMapSingle, concatMapMaybe, concatMapCompletable, concatMapIterable


switchMap
switchMapSingle, switchMapMaybe, switchMapCompletable



The reason these operators have a suffix instead of simply having the same name with different signature is type erasure. Java doesn't consider signatures such as operator(Function<T, Single<R>>) and operator(Function<T, Maybe<R>>) different (unlike C#) and due to erasure, the two operators would end up as duplicate methods with the same signature.
Operator naming conventions
Naming in programming is one of the hardest things as names are expected to be not long, expressive, capturing and easily memorable. Unfortunately, the target language (and pre-existing conventions) may not give too much help in this regard (unusable keywords, type erasure, type ambiguities, etc.).
Unusable keywords
In the original Rx.NET, the operator that emits a single item and then completes is called Return(T). Since the Java convention is to have a lowercase letter start a method name, this would have been return(T) which is a keyword in Java and thus not available. Therefore, RxJava chose to name this operator just(T). The same limitation exists for the operator Switch, which had to be named switchOnNext. Yet another example is Catch which was named onErrorResumeNext.
Type erasure
Many operators that expect the user to provide some function returning a reactive type can't be overloaded because the type erasure around a Function<T, X> turns such method signatures into duplicates. RxJava chose to name such operators by appending the type as suffix as well:
Flowable<R> flatMap(Function<? super T, ? extends Publisher<? extends R>> mapper)

Flowable<R> flatMapMaybe(Function<? super T, ? extends MaybeSource<? extends R>> mapper)
Type ambiguities
Even though certain operators have no problems from type erasure, their signature may turn up being ambiguous, especially if one uses Java 8 and lambdas. For example, there are several overloads of concatWith taking the various other reactive base types as arguments (for providing convenience and performance benefits in the underlying implementation):
Flowable<T> concatWith(Publisher<? extends T> other);

Flowable<T> concatWith(SingleSource<? extends T> other);
Both Publisher and SingleSource appear as functional interfaces (types with one abstract method) and may encourage users to try to provide a lambda expression:
someSource.concatWith(s -> Single.just(2))
.subscribe(System.out::println, Throwable::printStackTrace);
Unfortunately, this approach doesn't work and the example does not print 2 at all. In fact, since version 2.1.10, it doesn't
even compile because at least 4 concatWith overloads exist and the compiler finds the code above ambiguous.
The user in such situations probably wanted to defer some computation until the someSource has completed, thus the correct
unambiguous operator should have been defer:
someSource.concatWith(Single.defer(() -> Single.just(2)))
.subscribe(System.out::println, Throwable::printStackTrace);
Sometimes, a suffix is added to avoid logical ambiguities that may compile but produce the wrong type in a flow:
Flowable<T> merge(Publisher<? extends Publisher<? extends T>> sources);

Flowable<T> mergeArray(Publisher<? extends T>... sources);
This can get also ambiguous when functional interface types get involved as the type argument T.
Error handling
Dataflows can fail, at which point the error is emitted to the consumer(s). Sometimes though, multiple sources may fail at which point there is a choice whether or not wait for all of them to complete or fail. To indicate this opportunity, many operator names are suffixed with the DelayError words (while others feature a delayError or delayErrors boolean flag in one of their overloads):
Flowable<T> concat(Publisher<? extends Publisher<? extends T>> sources);

Flowable<T> concatDelayError(Publisher<? extends Publisher<? extends T>> sources);
Of course, suffixes of various kinds may appear together:
Flowable<T> concatArrayEagerDelayError(Publisher<? extends T>... sources);
Base class vs base type
The base classes can be considered heavy due to the sheer number of static and instance methods on them. RxJava 3's design was heavily influenced by the Reactive Streams specification, therefore, the library features a class and an interface per each reactive type:



Type
Class
Interface
Consumer




0..N backpressured
Flowable
Publisher1
Subscriber


0..N unbounded
Observable
ObservableSource2
Observer


1 element or error
Single
SingleSource
SingleObserver


0..1 element or error
Maybe
MaybeSource
MaybeObserver


0 element or error
Completable
CompletableSource
CompletableObserver



1The org.reactivestreams.Publisher is part of the external Reactive Streams library. It is the main type to interact with other reactive libraries through a standardized mechanism governed by the Reactive Streams specification.
2The naming convention of the interface was to append Source to the semi-traditional class name. There is no FlowableSource since Publisher is provided by the Reactive Streams library (and subtyping it wouldn't have helped with interoperation either). These interfaces are, however, not standard in the sense of the Reactive Streams specification and are currently RxJava specific only.
R8 and ProGuard settings
By default, RxJava itself doesn't require any ProGuard/R8 settings and should work without problems. Unfortunately, the Reactive Streams dependency since version 1.0.3 has embedded Java 9 class files in its JAR that can cause warnings with the plain ProGuard:
Warning: org.reactivestreams.FlowAdapters$FlowPublisherFromReactive: can't find superclass or interface java.util.concurrent.Flow$Publisher
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveProcessor: can't find superclass or interface java.util.concurrent.Flow$Processor
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscriber: can't find superclass or interface java.util.concurrent.Flow$Subscriber
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscription: can't find superclass or interface java.util.concurrent.Flow$Subscription
Warning: org.reactivestreams.FlowAdapters: can't find referenced class java.util.concurrent.Flow$Publisher

It is recommended one sets up the following -dontwarn entry in the application's proguard-ruleset file:
-dontwarn java.util.concurrent.Flow*

For R8, the RxJava jar includes the META-INF/proguard/rxjava3.pro with the same no-warning clause and should apply automatically.
Further reading
For further details, consult the wiki.
Communication

Google Group: RxJava
Twitter: @RxJava
GitHub Issues
StackOverflow: rx-java and rx-java2
Gitter.im

Versioning
Version 3.x is in development. Bugfixes will be applied to both 2.x and 3.x branches, but new features will only be added to 3.x.
Minor 3.x increments (such as 3.1, 3.2, etc) will occur when non-trivial new functionality is added or significant enhancements or bug fixes occur that may have behavioral changes that may affect some edge cases (such as dependence on behavior resulting from a bug). An example of an enhancement that would classify as this is adding reactive pull backpressure support to an operator that previously did not support it. This should be backwards compatible but does behave differently.
Patch 3.x.y increments (such as 3.0.0 -> 3.0.1, 3.3.1 -> 3.3.2, etc) will occur for bug fixes and trivial functionality (like adding a method overload). New functionality marked with an @Beta or @Experimental annotation can also be added in the patch releases to allow rapid exploration and iteration of unstable new functionality.
@Beta
APIs marked with the @Beta annotation at the class or method level are subject to change. They can be modified in any way, or even removed, at any time. If your code is a library itself (i.e. it is used on the CLASSPATH of users outside your control), you should not use beta APIs, unless you repackage them (e.g. using ProGuard, shading, etc).
@Experimental
APIs marked with the @Experimental annotation at the class or method level will almost certainly change. They can be modified in any way, or even removed, at any time. You should not use or rely on them in any production code. They are purely to allow broad testing and feedback.
@Deprecated
APIs marked with the @Deprecated annotation at the class or method level will remain supported until the next major release but it is recommended to stop using them.
io.reactivex.rxjava3.internal.*
All code inside the io.reactivex.rxjava3.internal.* packages are considered private API and should not be relied upon at all. It can change at any time.
Full Documentation

Wiki
Javadoc
Latest snaphot Javadoc
Javadoc of a specific release version: http://reactivex.io/RxJava/3.x/javadoc/3.x.y/

Binaries
Binaries and dependency information for Maven, Ivy, Gradle and others can be found at http://search.maven.org.
Example for Gradle:
compile 'io.reactivex.rxjava3:rxjava:x.y.z'
and for Maven:
<dependency>
    <groupId>io.reactivex.rxjava3</groupId>
    <artifactId>rxjava</artifactId>
    <version>x.y.z</version>
</dependency>
and for Ivy:
<dependency org=""io.reactivex.rxjava3"" name=""rxjava"" rev=""x.y.z"" />
Snapshots are available via https://oss.jfrog.org/libs-snapshot/io/reactivex/rxjava3/rxjava/
repositories {
    maven { url 'https://oss.jfrog.org/libs-snapshot' }
}

dependencies {
    compile 'io.reactivex.rxjava3:rxjava:3.0.0-SNAPSHOT'
}
Build
To build:
$ git clone git@github.com:ReactiveX/RxJava.git
$ cd RxJava/
$ ./gradlew build

Further details on building can be found on the Getting Started page of the wiki.
Bugs and Feedback
For bugs, questions and discussions please use the Github Issues.
LICENSE
Copyright (c) 2016-present, RxJava Contributors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
64,Java,"Interviews

Your personal guide to Software Engineering technical interviews. Video
solutions to the following interview problems with detailed explanations can be found here.

Maintainer - Kevin Naughton Jr.

Translations

简体中文

Table of Contents

YouTube
Instagram
Articles
Online Judges
Live Coding Practice
Data Structures
Algorithms
Greedy Algorithms
Bitmasks
Runtime Analysis
Video Lectures
Interview Books
Computer Science News
Directory Tree

YouTube

Kevin Naughton Jr.

Instagram

Programeme

Articles

Starting Work

Online Judges

LeetCode
Virtual Judge
CareerCup
HackerRank
CodeFights
Kattis
HackerEarth
Codility
Code Forces
Code Chef
Sphere Online Judge - SPOJ
InterviewBit

Live Coding Practice

Pramp
Gainlo
Refdash
Interviewing.io

Data Structures
Linked List

A Linked List is a linear collection of data elements, called nodes, each
pointing to the next node by means of a pointer. It is a data structure
consisting of a group of nodes which together represent a sequence.
Singly-linked list: linked list in which each node points to the next node and the last node points to null
Doubly-linked list: linked list in which each node has two pointers, p and n, such that p points to the previous node and n points to the next node; the last node's n pointer points to null
Circular-linked list: linked list in which each node points to the next node and the last node points back to the first node
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Stack

A Stack is a collection of elements, with two principle operations: push, which adds to the collection, and
pop, which removes the most recently added element
Last in, first out data structure (LIFO): the most recently added object is the first to be removed
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Queue

A Queue is a collection of elements, supporting two principle operations: enqueue, which inserts an element
into the queue, and dequeue, which removes an element from the queue
First in, first out data structure (FIFO): the oldest added object is the first to be removed
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Tree

A Tree is an undirected, connected, acyclic graph

Binary Tree

A Binary Tree is a tree data structure in which each node has at most two children, which are referred to as
the left child and right child
Full Tree: a tree in which every node has either 0 or 2 children
Perfect Binary Tree: a binary tree in which all interior nodes have two children and all leave have the same depth
Complete Tree: a binary tree in which every level except possibly the last is full and all nodes in the last
level are as far left as possible

Binary Search Tree

A binary search tree, sometimes called BST, is a type of binary tree which maintains the property that the value in each
node must be greater than or equal to any value stored in the left sub-tree, and less than or equal to any value stored
in the right sub-tree
Time Complexity:

Access: O(log(n))
Search: O(log(n))
Insert: O(log(n))
Remove: O(log(n))




Trie

A trie, sometimes called a radix or prefix tree, is a kind of search tree that is used to store a dynamic set or associative
array where the keys are usually Strings. No node in the tree stores the key associated with that node; instead, its position
in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the String associated
with that node, and the root is associated with the empty String.


Fenwick Tree

A Fenwick tree, sometimes called a binary indexed tree, is a tree in concept, but in practice is implemented as an implicit data
structure using an array. Given an index in the array representing a vertex, the index of a vertex's parent or child is calculated
through bitwise operations on the binary representation of its index. Each element of the array contains the pre-calculated sum of
a range of values, and by combining that sum with additional ranges encountered during an upward traversal to the root, the prefix
sum is calculated
Time Complexity:

Range Sum: O(log(n))
Update: O(log(n))




Segment Tree

A Segment tree, is a tree data structure for storing intervals, or segments. It allows querying which of the stored segments contain
a given point
Time Complexity:

Range Query: O(log(n))
Update: O(log(n))




Heap

A Heap is a specialized tree based structure data structure that satisfies the heap property: if A is a parent node of
B, then the key (the value) of node A is ordered with respect to the key of node B with the same ordering applying across the entire heap.
A heap can be classified further as either a ""max heap"" or a ""min heap"". In a max heap, the keys of parent nodes are always greater
than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than
or equal to those of the children and the lowest key is in the root node
Time Complexity:

Access Max / Min: O(1)
Insert: O(log(n))
Remove Max / Min: O(log(n))




Hashing

Hashing is used to map data of an arbitrary size to data of a fixed size. The values returned by a hash
function are called hash values, hash codes, or simply hashes. If two keys map to the same value, a collision occurs
Hash Map: a hash map is a structure that can map keys to values. A hash map uses a hash function to compute
an index into an array of buckets or slots, from which the desired value can be found.
Collision Resolution
Separate Chaining: in separate chaining, each bucket is independent, and contains a list of entries for each index. The
time for hash map operations is the time to find the bucket (constant time), plus the time to iterate through the list
Open Addressing: in open addressing, when a new entry is inserted, the buckets are examined, starting with the
hashed-to-slot and proceeding in some sequence, until an unoccupied slot is found. The name open addressing refers to
the fact that the location of an item is not always determined by its hash value


Graph

A Graph is an ordered pair of G = (V, E) comprising a set V of vertices or nodes together with a set E of edges or arcs,
which are 2-element subsets of V (i.e. an edge is associated with two vertices, and that association takes the form of the
unordered pair comprising those two vertices)
Undirected Graph: a graph in which the adjacency relation is symmetric. So if there exists an edge from node u to node
v (u -> v), then it is also the case that there exists an edge from node v to node u (v -> u)
Directed Graph: a graph in which the adjacency relation is not symmetric. So if there exists an edge from node u to node v
(u -> v), this does not imply that there exists an edge from node v to node u (v -> u)


Algorithms
Sorting
Quicksort

Stable: No
Time Complexity:

Best Case: O(nlog(n))
Worst Case: O(n^2)
Average Case: O(nlog(n))




Mergesort

Mergesort is also a divide and conquer algorithm. It continuously divides an array into two halves, recurses on both the
left subarray and right subarray and then merges the two sorted halves
Stable: Yes
Time Complexity:

Best Case: O(nlog(n))
Worst Case: O(nlog(n))
Average Case: O(nlog(n))




Bucket Sort

Bucket Sort is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket
is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm
Time Complexity:

Best Case: Ω(n + k)
Worst Case: O(n^2)
Average Case:Θ(n + k)




Radix Sort

Radix Sort is a sorting algorithm that like bucket sort, distributes elements of an array into a number of buckets. However, radix
sort differs from bucket sort by 're-bucketing' the array after the initial pass as opposed to sorting each bucket and merging
Time Complexity:

Best Case: Ω(nk)
Worst Case: O(nk)
Average Case: Θ(nk)



Graph Algorithms
Depth First Search

Depth First Search is a graph traversal algorithm which explores as far as possible along each branch before backtracking
Time Complexity: O(|V| + |E|)


Breadth First Search

Breadth First Search is a graph traversal algorithm which explores the neighbor nodes first, before moving to the next
level neighbors
Time Complexity: O(|V| + |E|)


Topological Sort

Topological Sort is the linear ordering of a directed graph's nodes such that for every edge from node u to node v, u
comes before v in the ordering
Time Complexity: O(|V| + |E|)

Dijkstra's Algorithm

Dijkstra's Algorithm is an algorithm for finding the shortest path between nodes in a graph
Time Complexity: O(|V|^2)


Bellman-Ford Algorithm

Bellman-Ford Algorithm is an algorithm that computes the shortest paths from a single source node to all other nodes in a weighted graph
Although it is slower than Dijkstra's, it is more versatile, as it is capable of handling graphs in which some of the edge weights are
negative numbers
Time Complexity:

Best Case: O(|E|)
Worst Case: O(|V||E|)




Floyd-Warshall Algorithm

Floyd-Warshall Algorithm is an algorithm for finding the shortest paths in a weighted graph with positive or negative edge weights, but
no negative cycles
A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of nodes
Time Complexity:

Best Case: O(|V|^3)
Worst Case: O(|V|^3)
Average Case: O(|V|^3)



Prim's Algorithm

Prim's Algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. In other words, Prim's find a
subset of edges that forms a tree that includes every node in the graph
Time Complexity: O(|V|^2)


Kruskal's Algorithm

Kruskal's Algorithm is also a greedy algorithm that finds a minimum spanning tree in a graph. However, in Kruskal's, the graph does not
have to be connected
Time Complexity: O(|E|log|V|)


Greedy Algorithms

Greedy Algorithms are algorithms that make locally optimal choices at each step in the hope of eventually reaching the globally optimal solution
Problems must exhibit two properties in order to implement a Greedy solution:
Optimal Substructure

An optimal solution to the problem contains optimal solutions to the given problem's subproblems


The Greedy Property

An optimal solution is reached by ""greedily"" choosing the locally optimal choice without ever reconsidering previous choices


Example - Coin Change

Given a target amount V cents and a list of denominations of n coins, i.e. we have coinValue[i] (in cents) for coin types i from [0...n - 1],
what is the minimum number of coins that we must use to represent amount V? Assume that we have an unlimited supply of coins of any type
Coins - Penny (1 cent), Nickel (5 cents), Dime (10 cents), Quarter (25 cents)
Assume V = 41. We can use the Greedy algorithm of continuously selecting the largest coin denomination less than or equal to V, subtract that
coin's value from V, and repeat.
V = 41 | 0 coins used
V = 16 | 1 coin used (41 - 25 = 16)
V = 6  | 2 coins used (16 - 10 = 6)
V = 1  | 3 coins used (6 - 5 = 1)
V = 0  | 4 coins used (1 - 1 = 0)
Using this algorithm, we arrive at a total of 4 coins which is optimal



Bitmasks

Bitmasking is a technique used to perform operations at the bit level. Leveraging bitmasks often leads to faster runtime complexity and
helps limit memory usage
Test kth bit: s & (1 << k);
Set kth bit: s |= (1 << k);
Turn off kth bit: s &= ~(1 << k);
Toggle kth bit: s ^= (1 << k);
Multiple by 2n: s << n;
Divide by 2n: s >> n;
Intersection: s & t;
Union: s | t;
Set Subtraction: s & ~t;
Extract lowest set bit: s & (-s);
Extract lowest unset bit: ~s & (s + 1);
Swap Values:
x ^= y; y ^= x; x ^= y;

Runtime Analysis
Big O Notation

Big O Notation is used to describe the upper bound of a particular algorithm. Big O is used to describe worst case scenarios


Little O Notation

Little O Notation is also used to describe an upper bound of a particular algorithm; however, Little O provides a bound
that is not asymptotically tight

Big Ω Omega Notation

Big Omega Notation is used to provide an asymptotic lower bound on a particular algorithm


Little ω Omega Notation

Little Omega Notation is used to provide a lower bound on a particular algorithm that is not asymptotically tight

Theta Θ Notation

Theta Notation is used to provide a bound on a particular algorithm such that it can be ""sandwiched"" between
two constants (one for an upper limit and one for a lower limit) for sufficiently large values


Video Lectures

Data Structures

UC Berkeley Data Structures
MIT Advanced Data Structures


Algorithms

MIT Introduction to Algorithms
MIT Advanced Algorithms
UC Berkeley Algorithms



Interview Books

Competitive Programming 3 - Steven Halim & Felix Halim
Cracking The Coding Interview - Gayle Laakmann McDowell
Cracking The PM Interview - Gayle Laakmann McDowell & Jackie Bavaro
Introduction to Algorithms -  Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest & Clifford Stein

Computer Science News

Hacker News
Lobsters

Directory Tree
.
├── Array
│   ├── bestTimeToBuyAndSellStock.java
│   ├── findTheCelebrity.java
│   ├── gameOfLife.java
│   ├── increasingTripletSubsequence.java
│   ├── insertInterval.java
│   ├── longestConsecutiveSequence.java
│   ├── maximumProductSubarray.java
│   ├── maximumSubarray.java
│   ├── mergeIntervals.java
│   ├── missingRanges.java
│   ├── productOfArrayExceptSelf.java
│   ├── rotateImage.java
│   ├── searchInRotatedSortedArray.java
│   ├── spiralMatrixII.java
│   ├── subsetsII.java
│   ├── subsets.java
│   ├── summaryRanges.java
│   ├── wiggleSort.java
│   └── wordSearch.java
├── Backtracking
│   ├── androidUnlockPatterns.java
│   ├── generalizedAbbreviation.java
│   └── letterCombinationsOfAPhoneNumber.java
├── BinarySearch
│   ├── closestBinarySearchTreeValue.java
│   ├── firstBadVersion.java
│   ├── guessNumberHigherOrLower.java
│   ├── pow(x,n).java
│   └── sqrt(x).java
├── BitManipulation
│   ├── binaryWatch.java
│   ├── countingBits.java
│   ├── hammingDistance.java
│   ├── maximumProductOfWordLengths.java
│   ├── numberOf1Bits.java
│   ├── sumOfTwoIntegers.java
│   └── utf-8Validation.java
├── BreadthFirstSearch
│   ├── binaryTreeLevelOrderTraversal.java
│   ├── cloneGraph.java
│   ├── pacificAtlanticWaterFlow.java
│   ├── removeInvalidParentheses.java
│   ├── shortestDistanceFromAllBuildings.java
│   ├── symmetricTree.java
│   └── wallsAndGates.java
├── DepthFirstSearch
│   ├── balancedBinaryTree.java
│   ├── battleshipsInABoard.java
│   ├── convertSortedArrayToBinarySearchTree.java
│   ├── maximumDepthOfABinaryTree.java
│   ├── numberOfIslands.java
│   ├── populatingNextRightPointersInEachNode.java
│   └── sameTree.java
├── Design
│   └── zigzagIterator.java
├── DivideAndConquer
│   ├── expressionAddOperators.java
│   └── kthLargestElementInAnArray.java
├── DynamicProgramming
│   ├── bombEnemy.java
│   ├── climbingStairs.java
│   ├── combinationSumIV.java
│   ├── countingBits.java
│   ├── editDistance.java
│   ├── houseRobber.java
│   ├── paintFence.java
│   ├── paintHouseII.java
│   ├── regularExpressionMatching.java
│   ├── sentenceScreenFitting.java
│   ├── uniqueBinarySearchTrees.java
│   └── wordBreak.java
├── HashTable
│   ├── binaryTreeVerticalOrderTraversal.java
│   ├── findTheDifference.java
│   ├── groupAnagrams.java
│   ├── groupShiftedStrings.java
│   ├── islandPerimeter.java
│   ├── loggerRateLimiter.java
│   ├── maximumSizeSubarraySumEqualsK.java
│   ├── minimumWindowSubstring.java
│   ├── sparseMatrixMultiplication.java
│   ├── strobogrammaticNumber.java
│   ├── twoSum.java
│   └── uniqueWordAbbreviation.java
├── LinkedList
│   ├── addTwoNumbers.java
│   ├── deleteNodeInALinkedList.java
│   ├── mergeKSortedLists.java
│   ├── palindromeLinkedList.java
│   ├── plusOneLinkedList.java
│   ├── README.md
│   └── reverseLinkedList.java
├── Queue
│   └── movingAverageFromDataStream.java
├── README.md
├── Sort
│   ├── meetingRoomsII.java
│   └── meetingRooms.java
├── Stack
│   ├── binarySearchTreeIterator.java
│   ├── decodeString.java
│   ├── flattenNestedListIterator.java
│   └── trappingRainWater.java
├── String
│   ├── addBinary.java
│   ├── countAndSay.java
│   ├── decodeWays.java
│   ├── editDistance.java
│   ├── integerToEnglishWords.java
│   ├── longestPalindrome.java
│   ├── longestSubstringWithAtMostKDistinctCharacters.java
│   ├── minimumWindowSubstring.java
│   ├── multiplyString.java
│   ├── oneEditDistance.java
│   ├── palindromePermutation.java
│   ├── README.md
│   ├── reverseVowelsOfAString.java
│   ├── romanToInteger.java
│   ├── validPalindrome.java
│   └── validParentheses.java
├── Tree
│   ├── binaryTreeMaximumPathSum.java
│   ├── binaryTreePaths.java
│   ├── inorderSuccessorInBST.java
│   ├── invertBinaryTree.java
│   ├── lowestCommonAncestorOfABinaryTree.java
│   ├── sumOfLeftLeaves.java
│   └── validateBinarySearchTree.java
├── Trie
│   ├── addAndSearchWordDataStructureDesign.java
│   ├── implementTrie.java
│   └── wordSquares.java
└── TwoPointers
    ├── 3Sum.java
    ├── 3SumSmaller.java
    ├── mergeSortedArray.java
    ├── minimumSizeSubarraySum.java
    ├── moveZeros.java
    ├── removeDuplicatesFromSortedArray.java
    ├── reverseString.java
    └── sortColors.java

18 directories, 124 files

"
65,Java,"OkHttp
See the project website for documentation and APIs.
HTTP is the way modern applications network. It’s how we exchange data & media. Doing HTTP
efficiently makes your stuff load faster and saves bandwidth.
OkHttp is an HTTP client that’s efficient by default:

HTTP/2 support allows all requests to the same host to share a socket.
Connection pooling reduces request latency (if HTTP/2 isn’t available).
Transparent GZIP shrinks download sizes.
Response caching avoids the network completely for repeat requests.

OkHttp perseveres when the network is troublesome: it will silently recover from common connection
problems. If your service has multiple IP addresses OkHttp will attempt alternate addresses if the
first connect fails. This is necessary for IPv4+IPv6 and services hosted in redundant data
centers. OkHttp supports modern TLS features (TLS 1.3, ALPN, certificate pinning). It can be
configured to fall back for broad connectivity.
Using OkHttp is easy. Its request/response API is designed with fluent builders and immutability. It
supports both synchronous blocking calls and async calls with callbacks.
Get a URL
This program downloads a URL and prints its contents as a string. Full source.
OkHttpClient client = new OkHttpClient();

String run(String url) throws IOException {
  Request request = new Request.Builder()
      .url(url)
      .build();

  try (Response response = client.newCall(request).execute()) {
    return response.body().string();
  }
}
Post to a Server
This program posts data to a service. Full source.
public static final MediaType JSON
    = MediaType.get(""application/json; charset=utf-8"");

OkHttpClient client = new OkHttpClient();

String post(String url, String json) throws IOException {
  RequestBody body = RequestBody.create(json, JSON);
  Request request = new Request.Builder()
      .url(url)
      .post(body)
      .build();
  try (Response response = client.newCall(request).execute()) {
    return response.body().string();
  }
}
Further examples are on the OkHttp Recipes page.
Requirements
OkHttp works on Android 5.0+ (API level 21+) and on Java 8+.
OkHttp depends on Okio for high-performance I/O and the Kotlin standard library. Both are small libraries with strong backward-compatibility.
We highly recommend you keep OkHttp up-to-date. As with auto-updating web browsers, staying current
with HTTPS clients is an important defense against potential security problems. We
track the dynamic TLS ecosystem and adjust OkHttp to improve connectivity and
security.
OkHttp uses your platform's built-in TLS implementation. On Java platforms OkHttp also supports
Conscrypt, which integrates BoringSSL with Java. OkHttp will use Conscrypt if it is
the first security provider:
Security.insertProviderAt(Conscrypt.newProvider(), 1);
The OkHttp 3.12.x branch supports Android 2.3+ (API level 9+) and Java 7+. These platforms lack
support for TLS 1.2 and should not be used. But because upgrading is difficult we will backport
critical fixes to the 3.12.x branch through December 31, 2020.
Releases
Our change log has release history.
The latest release is available on Maven Central.
implementation(""com.squareup.okhttp3:okhttp:4.2.2"")
Snapshot builds are available. R8 and ProGuard rules are available.
MockWebServer
OkHttp includes a library for testing HTTP, HTTPS, and HTTP/2 clients.
The latest release is available on Maven Central.
testImplementation(""com.squareup.okhttp3:mockwebserver:4.2.2"")
License
Copyright 2019 Square, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
66,Java,"互联网 Java 工程师进阶知识完全扫盲©












本项目大部分内容来自中华石杉，版权归作者所有，内容涵盖高并发、分布式、高可用、微服务、海量数据处理等领域知识。我(@yanglbme)对这部分知识做了一个系统的整理，方便学习查阅。配合《大型网站技术架构——李智慧》、《Redis 设计与实现——黄健宏》、《Redis 深度历险——钱文品》、《亿级流量网站架构核心技术——张开涛》食用，效果更佳。
学习之前，先来看看 Issues 讨论区的技术面试官是怎么说的吧。本项目也欢迎各位开发者朋友到 Issues 讨论区分享自己的一些想法和实践经验，参与或加入开源组织请看这里，你也访问 GitHub Page  详细了解一下 Doocs。
另外，我还将在这里更新内容，感兴趣的朋友可以进来看看。
高并发架构
消息队列

为什么使用消息队列？消息队列有什么优点和缺点？Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么优点和缺点？
如何保证消息队列的高可用？
如何保证消息不被重复消费？（如何保证消息消费的幂等性）
如何保证消息的可靠性传输？（如何处理消息丢失的问题）
如何保证消息的顺序性？
如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？
如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。

搜索引擎

es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？
es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？
es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？
es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？

缓存

在项目中缓存是如何使用的？缓存如果使用不当会造成什么后果？
Redis 和 Memcached 有什么区别？Redis 的线程模型是什么？为什么单线程的 Redis 比多线程的 Memcached 效率要高得多？
Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？
Redis 的过期策略都有哪些？手写一下 LRU 代码实现？
如何保证 Redis 高并发、高可用？Redis 的主从复制原理能介绍一下么？Redis 的哨兵原理能介绍一下么？
Redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？
Redis 集群模式的工作原理能说一下么？在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？如何动态增加和删除一个节点？
了解什么是 redis 的雪崩、穿透和击穿？Redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 Redis 的穿透？
如何保证缓存与数据库的双写一致性？
Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？
生产环境中的 Redis 是怎么部署的？

分库分表

为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？
现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？
如何设计可以动态扩容缩容的分库分表方案？
分库分表之后，id 主键如何处理？

读写分离

如何实现 MySQL 的读写分离？MySQL 主从复制原理是啥？如何解决 MySQL 主从同步的延时问题？

高并发系统

如何设计一个高并发系统？

分布式系统
面试连环炮
系统拆分

为什么要进行系统拆分？如何进行系统拆分？拆分后不用 Dubbo 可以吗？

分布式服务框架

说一下 Dubbo 的工作原理？注册中心挂了可以继续通信吗？
Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？
Dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？
Dubbo 的 spi 思想是什么？
如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？
分布式服务接口的幂等性如何设计（比如不能重复扣款）？
分布式服务接口请求的顺序性如何保证？
如何自己设计一个类似 Dubbo 的 RPC 框架？

分布式锁

Zookeeper 都有哪些应用场景？
使用 Redis 如何设计分布式锁？使用 Zookeeper 来设计分布式锁可以吗？以上两种分布式锁的实现方式哪种效率比较高？

分布式事务

分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？

分布式会话

集群部署时的分布式 Session 如何实现？

高可用架构

Hystrix 介绍
电商网站详情页系统架构
Hystrix 线程池技术实现资源隔离
Hystrix 信号量机制实现资源隔离
Hystrix 隔离策略细粒度控制
深入 Hystrix 执行时内部原理
基于 request cache 请求缓存技术优化批量商品数据查询接口
基于本地缓存的 fallback 降级机制
深入 Hystrix 断路器执行原理
深入 Hystrix 线程池隔离与接口限流
基于 timeout 机制为服务接口调用超时提供安全保护

高可用系统

如何设计一个高可用系统？

限流

如何限流？在工作中是怎么做的？说一下具体的实现？

熔断

如何进行熔断？
熔断框架都有哪些？具体实现原理知道吗？
熔断框架如何做技术选型？选用 Sentinel 还是 Hystrix？

降级

如何进行降级？

微服务架构

微服务架构整个章节内容属额外新增，后续抽空更新，也欢迎读者们参与补充完善
关于微服务架构的描述
从单体式架构迁移到微服务架构
微服务的事件驱动数据管理
选择微服务部署策略

Spring Cloud 微服务架构

什么是微服务？微服务之间是如何独立通讯的？
Spring Cloud 和 Dubbo 有哪些区别？
Spring Boot 和 Spring Cloud，谈谈你对它们的理解？
什么是服务熔断？什么是服务降级？
微服务的优缺点分别是什么？说一下你在项目开发中碰到的坑？
你所知道的微服务技术栈都有哪些？
微服务治理策略
Eureka 和 Zookeeper 都可以提供服务注册与发现的功能，它们有什么区别？
......

海量数据处理

如何从大量的 URL 中找出相同的 URL？
如何从大量数据中找出高频词？
如何找出某一天访问百度网站最多的 IP？
如何在大量的数据中找出不重复的整数？
如何在大量的数据中判断一个数是否存在？
如何查询最热门的查询串？
如何统计不同电话号码的个数？
如何从 5 亿个数中找出中位数？
如何按照 query 的频度排序？
如何找出排名前 500 的数？


贡献者
感谢以下所有朋友对 GitHub 技术社区 Doocs 所做出的贡献，参与项目维护请戳这儿。

公众号
GitHub 技术社区 Doocs 旗下唯一公众号“Doocs 开源社区”，欢迎关注，专注于分享有价值的文章；当然，也可以加我个人微信（备注：GitHub）。





公众平台





个人微信




"
67,Java,"Guava: Google Core Libraries for Java


Guava is a set of core libraries that includes new collection types (such as
multimap and multiset), immutable collections, a graph library, and utilities
for concurrency, I/O, hashing, primitives, strings, and more!
Guava comes in two flavors.

The JRE flavor requires JDK 1.8 or higher.
If you need support for JDK 1.7 or Android, use the Android flavor. You can
find the Android Guava source in the android directory.

Adding Guava to your build
Guava's Maven group ID is com.google.guava and its artifact ID is guava.
Guava provides two different ""flavors"": one for use on a (Java 8+) JRE and one
for use on Android or Java 7 or by any library that wants to be compatible with
either of those. These flavors are specified in the Maven version field as
either 28.1-jre or 28.1-android. For more about depending on Guava, see
using Guava in your build.
To add a dependency on Guava using Maven, use the following:
<dependency>
  <groupId>com.google.guava</groupId>
  <artifactId>guava</artifactId>
  <version>28.1-jre</version>
  <!-- or, for Android: -->
  <version>28.1-android</version>
</dependency>
To add a dependency using Gradle:
dependencies {
  // Pick one:

  // 1. Use Guava in your implementation only:
  implementation(""com.google.guava:guava:28.1-jre"")

  // 2. Use Guava types in your public API:
  api(""com.google.guava:guava:28.1-jre"")

  // 3. Android - Use Guava in your implementation only:
  implementation(""com.google.guava:guava:28.1-android"")

  // 4. Android - Use Guava types in your public API:
  api(""com.google.guava:guava:28.1-android"")
}
For more information on when to use api and when to use implementation,
consult the
Gradle documentation on API and implementation separation.
Snapshots
Snapshots of Guava built from the master branch are available through Maven
using version HEAD-jre-SNAPSHOT, or HEAD-android-SNAPSHOT for the Android
flavor.

Snapshot API Docs: guava
Snapshot API Diffs: guava

Learn about Guava

Our users' guide, Guava Explained
A nice collection of
other helpful links

Links

GitHub project
Issue tracker: Report a defect or feature request
StackOverflow: Ask ""how-to"" and ""why-didn't-it-work"" questions
guava-announce: Announcements of releases and upcoming significant changes
guava-discuss: For open-ended questions and discussion

IMPORTANT WARNINGS


APIs marked with the @Beta annotation at the class or method level are
subject to change. They can be modified in any way, or even removed, at any
time. If your code is a library itself (i.e. it is used on the CLASSPATH of
users outside your own control), you should not use beta APIs, unless you
repackage them. If your code is a library, we strongly recommend using
the Guava Beta Checker to ensure that you do not use any @Beta APIs!


APIs without @Beta will remain binary-compatible for the indefinite
future. (Previously, we sometimes removed such APIs after a deprecation
period. The last release to remove non-@Beta APIs was Guava 21.0.) Even
@Deprecated APIs will remain (again, unless they are @Beta). We have no
plans to start removing things again, but officially, we're leaving our
options open in case of surprises (like, say, a serious security problem).


Guava has one dependency that is needed at runtime:
com.google.guava:failureaccess:1.0.1


Serialized forms of ALL objects are subject to change unless noted
otherwise. Do not persist these and assume they can be read by a future
version of the library.


Our classes are not designed to protect against a malicious caller. You
should not use them for communication between trusted and untrusted code.


For the mainline flavor, we unit-test the libraries using only OpenJDK 1.8
on Linux. Some features, especially in com.google.common.io, may not work
correctly in other environments. For the Android flavor, our unit tests run
on API level 15 (Ice Cream Sandwich).


"
68,Java,"Retrofit
Type-safe HTTP client for Android and Java by Square, Inc.
For more information please see the website.
Download
Download the latest JAR or grab from Maven central at the coordinates com.squareup.retrofit2:retrofit:2.7.0.
Snapshots of the development version are available in Sonatype's snapshots repository.
Retrofit requires at minimum Java 8+ or Android API 21+.
R8 / ProGuard
If you are using R8 the shrinking and obfuscation rules are included automatically.
ProGuard users must manually add the options from
this file.
(Note: You might also need rules for OkHttp and Okio which are dependencies of this library)
License
Copyright 2013 Square, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
69,Java," Spring Framework 
This is the home of the Spring Framework: the foundation for all Spring projects. Collectively the Spring Framework and the family of Spring projects are often referred to simply as ""Spring"".
Spring provides everything required beyond the Java programming language for creating enterprise applications for a wide range of scenarios and architectures. Please read the Overview section as reference for a more complete introduction.
Code of Conduct
This project is governed by the Spring Code of Conduct. By participating, you are expected to uphold this code of conduct. Please report unacceptable behavior to spring-code-of-conduct@pivotal.io.
Access to Binaries
For access to artifacts or a distribution zip, see the Spring Framework Artifacts wiki page.
Documentation
The Spring Framework maintains reference documentation (published and source), Github wiki pages, and an
API reference. There are also guides and tutorials across Spring projects.
Build from Source
See the Build from Source Wiki page and the CONTRIBUTING.md file.
Stay in Touch
Follow @SpringCentral, @SpringFramework, and its team members on Twitter. In-depth articles can be found at The Spring Blog, and releases are announced via our news feed.
License
The Spring Framework is released under version 2.0 of the Apache License.
"
70,Java,"Guava: Google Core Libraries for Java


Guava is a set of core libraries that includes new collection types (such as
multimap and multiset), immutable collections, a graph library, and utilities
for concurrency, I/O, hashing, primitives, strings, and more!
Guava comes in two flavors.

The JRE flavor requires JDK 1.8 or higher.
If you need support for JDK 1.7 or Android, use the Android flavor. You can
find the Android Guava source in the android directory.

Adding Guava to your build
Guava's Maven group ID is com.google.guava and its artifact ID is guava.
Guava provides two different ""flavors"": one for use on a (Java 8+) JRE and one
for use on Android or Java 7 or by any library that wants to be compatible with
either of those. These flavors are specified in the Maven version field as
either 28.1-jre or 28.1-android. For more about depending on Guava, see
using Guava in your build.
To add a dependency on Guava using Maven, use the following:
<dependency>
  <groupId>com.google.guava</groupId>
  <artifactId>guava</artifactId>
  <version>28.1-jre</version>
  <!-- or, for Android: -->
  <version>28.1-android</version>
</dependency>
To add a dependency using Gradle:
dependencies {
  // Pick one:

  // 1. Use Guava in your implementation only:
  implementation(""com.google.guava:guava:28.1-jre"")

  // 2. Use Guava types in your public API:
  api(""com.google.guava:guava:28.1-jre"")

  // 3. Android - Use Guava in your implementation only:
  implementation(""com.google.guava:guava:28.1-android"")

  // 4. Android - Use Guava types in your public API:
  api(""com.google.guava:guava:28.1-android"")
}
For more information on when to use api and when to use implementation,
consult the
Gradle documentation on API and implementation separation.
Snapshots
Snapshots of Guava built from the master branch are available through Maven
using version HEAD-jre-SNAPSHOT, or HEAD-android-SNAPSHOT for the Android
flavor.

Snapshot API Docs: guava
Snapshot API Diffs: guava

Learn about Guava

Our users' guide, Guava Explained
A nice collection of
other helpful links

Links

GitHub project
Issue tracker: Report a defect or feature request
StackOverflow: Ask ""how-to"" and ""why-didn't-it-work"" questions
guava-announce: Announcements of releases and upcoming significant changes
guava-discuss: For open-ended questions and discussion

IMPORTANT WARNINGS


APIs marked with the @Beta annotation at the class or method level are
subject to change. They can be modified in any way, or even removed, at any
time. If your code is a library itself (i.e. it is used on the CLASSPATH of
users outside your own control), you should not use beta APIs, unless you
repackage them. If your code is a library, we strongly recommend using
the Guava Beta Checker to ensure that you do not use any @Beta APIs!


APIs without @Beta will remain binary-compatible for the indefinite
future. (Previously, we sometimes removed such APIs after a deprecation
period. The last release to remove non-@Beta APIs was Guava 21.0.) Even
@Deprecated APIs will remain (again, unless they are @Beta). We have no
plans to start removing things again, but officially, we're leaving our
options open in case of surprises (like, say, a serious security problem).


Guava has one dependency that is needed at runtime:
com.google.guava:failureaccess:1.0.1


Serialized forms of ALL objects are subject to change unless noted
otherwise. Do not persist these and assume they can be read by a future
version of the library.


Our classes are not designed to protect against a malicious caller. You
should not use them for communication between trusted and untrusted code.


For the mainline flavor, we unit-test the libraries using only OpenJDK 1.8
on Linux. Some features, especially in com.google.common.io, may not work
correctly in other environments. For the Android flavor, our unit tests run
on API level 15 (Ice Cream Sandwich).


"
71,Java,"Retrofit
Type-safe HTTP client for Android and Java by Square, Inc.
For more information please see the website.
Download
Download the latest JAR or grab from Maven central at the coordinates com.squareup.retrofit2:retrofit:2.7.0.
Snapshots of the development version are available in Sonatype's snapshots repository.
Retrofit requires at minimum Java 8+ or Android API 21+.
R8 / ProGuard
If you are using R8 the shrinking and obfuscation rules are included automatically.
ProGuard users must manually add the options from
this file.
(Note: You might also need rules for OkHttp and Okio which are dependencies of this library)
License
Copyright 2013 Square, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
72,Java," Spring Framework 
This is the home of the Spring Framework: the foundation for all Spring projects. Collectively the Spring Framework and the family of Spring projects are often referred to simply as ""Spring"".
Spring provides everything required beyond the Java programming language for creating enterprise applications for a wide range of scenarios and architectures. Please read the Overview section as reference for a more complete introduction.
Code of Conduct
This project is governed by the Spring Code of Conduct. By participating, you are expected to uphold this code of conduct. Please report unacceptable behavior to spring-code-of-conduct@pivotal.io.
Access to Binaries
For access to artifacts or a distribution zip, see the Spring Framework Artifacts wiki page.
Documentation
The Spring Framework maintains reference documentation (published and source), Github wiki pages, and an
API reference. There are also guides and tutorials across Spring projects.
Build from Source
See the Build from Source Wiki page and the CONTRIBUTING.md file.
Stay in Touch
Follow @SpringCentral, @SpringFramework, and its team members on Twitter. In-depth articles can be found at The Spring Blog, and releases are announced via our news feed.
License
The Spring Framework is released under version 2.0 of the Apache License.
"
73,Java,"Apache Dubbo Project









Apache Dubbo is a high-performance, Java based open source RPC framework. Please visit official site for quick start and documentations, as well as Wiki for news, FAQ, and release notes.
We are now collecting dubbo user info in order to help us to improve Dubbo better, pls. kindly help us by providing yours on issue#1012: Wanted: who's using dubbo, thanks :)
Architecture

Features

Transparent interface based RPC
Intelligent load balancing
Automatic service registration and discovery
High extensibility
Runtime traffic routing
Visualized service governance

Getting started
The following code snippet comes from Dubbo Samples. You may clone the sample project and step into dubbo-samples-api sub directory before read on.
# git clone https://github.com/apache/dubbo-samples.git
# cd dubbo-samples/dubbo-samples-api
There's a README file under dubbo-samples-api directory. Read it and try this sample out by following the instructions.
Maven dependency
<properties>
    <dubbo.version>2.7.4.1</dubbo.version>
</properties>
    
<dependencies>
    <dependency>
        <groupId>org.apache.dubbo</groupId>
        <artifactId>dubbo</artifactId>
        <version>${dubbo.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.dubbo</groupId>
        <artifactId>dubbo-dependencies-zookeeper</artifactId>
        <version>${dubbo.version}</version>
        <type>pom</type>
    </dependency>
</dependencies>
Define service interfaces
package org.apache.dubbo.samples.api;

public interface GreetingService {
    String sayHi(String name);
}
See api/GreetingService.java on GitHub.
Implement service interface for the provider
package org.apache.dubbo.samples.provider;

import org.apache.dubbo.samples.api.GreetingsService;

public class GreetingsServiceImpl implements GreetingsService {
    @Override
    public String sayHi(String name) {
        return ""hi, "" + name;
    }
}
See provider/GreetingServiceImpl.java on GitHub.
Start service provider
package org.apache.dubbo.samples.provider;


import org.apache.dubbo.config.ApplicationConfig;
import org.apache.dubbo.config.RegistryConfig;
import org.apache.dubbo.config.ServiceConfig;
import org.apache.dubbo.samples.api.GreetingsService;

import java.util.concurrent.CountDownLatch;

public class Application {
    private static String zookeeperHost = System.getProperty(""zookeeper.address"", ""127.0.0.1"");

    public static void main(String[] args) throws Exception {
        ServiceConfig<GreetingsService> service = new ServiceConfig<>();
        service.setApplication(new ApplicationConfig(""first-dubbo-provider""));
        service.setRegistry(new RegistryConfig(""zookeeper://"" + zookeeperHost + "":2181""));
        service.setInterface(GreetingsService.class);
        service.setRef(new GreetingsServiceImpl());
        service.export();

        System.out.println(""dubbo service started"");
        new CountDownLatch(1).await();
    }
}
See provider/Application.java on GitHub.
Build and run the provider
# mvn clean package
# mvn -Djava.net.preferIPv4Stack=true -Dexec.mainClass=org.apache.dubbo.samples.provider.Application exec:java
Call remote service in consumer
package org.apache.dubbo.samples.client;


import org.apache.dubbo.config.ApplicationConfig;
import org.apache.dubbo.config.ReferenceConfig;
import org.apache.dubbo.config.RegistryConfig;
import org.apache.dubbo.samples.api.GreetingsService;

public class Application {
    private static String zookeeperHost = System.getProperty(""zookeeper.address"", ""127.0.0.1"");

    public static void main(String[] args) {
        ReferenceConfig<GreetingsService> reference = new ReferenceConfig<>();
        reference.setApplication(new ApplicationConfig(""first-dubbo-consumer""));
        reference.setRegistry(new RegistryConfig(""zookeeper://"" + zookeeperHost + "":2181""));
        reference.setInterface(GreetingsService.class);
        GreetingsService service = reference.get();
        String message = service.sayHi(""dubbo"");
        System.out.println(message);
    }
}
See consumer/Application.java on GitHub.
Build and run the consumer
# mvn clean package
# mvn -Djava.net.preferIPv4Stack=true -Dexec.mainClass=org.apache.dubbo.samples.client.Application exec:java
The consumer will print out hi, dubbo on the screen.
Next steps

Your first Dubbo application - A 101 tutorial to reveal more details, with the same code above.
Dubbo user manual - How to use Dubbo and all its features.
Dubbo developer guide - How to involve in Dubbo development.
Dubbo admin manual - How to admin and manage Dubbo services.

Building
If you want to try out the cutting-edge features, you can build with the following commands. (Java 1.8 is required to build the master branch)
  mvn clean install

Contact


Mailing list:

dev list: for dev/user discussion. subscribe, unsubscribe, archive,  guide



Bugs: Issues


Gitter: Gitter channel


Twitter: @ApacheDubbo


Contributing
See CONTRIBUTING for details on submitting patches and the contribution workflow.
How can I contribute?

Take a look at issues with tag called Good first issue or Help wanted.
Join the discussion on mailing list, subscription guide.
Answer questions on issues.
Fix bugs reported on issues, and send us pull request.
Review the existing pull request.
Improve the website, typically we need

blog post
translation on documentation
use cases about how Dubbo is being used in enterprise system.


Improve the dubbo-admin/dubbo-monitor.
Contribute to the projects listed in ecosystem.
Any form of contribution that is not mentioned above.
If you would like to contribute, please send an email to dev@dubbo.apache.org to let us know!

Reporting bugs
Please follow the template for reporting any issues.
Reporting a security vulnerability
Please report security vulnerability to us privately.
Dubbo ecosystem

Dubbo Ecosystem Entry - A GitHub group dubbo to gather all Dubbo relevant projects not appropriate in apache group yet
Dubbo Website - Apache Dubbo official website
Dubbo Samples - samples for Apache Dubbo
Dubbo Spring Boot - Spring Boot Project for Dubbo
Dubbo Admin - The reference implementation for Dubbo admin

Language

Node.js
Python
PHP
Go
Erlang

License
Apache Dubbo is under the Apache 2.0 license. See the LICENSE file for details.
"
74,Java,"





⚡️ A powerful & easy to use chart library for Android ⚡️
Charts is the iOS version of this library
Table of Contents

Quick Start

Gradle
Maven


Documentation
Examples
Questions
Donate
Social Media
More Examples
License
Creators

Realtime Graphing Solution | SciChart


MPAndroidChart is free software, as a result dynamic & realtime data is not officially supported. If you are looking for an enterprise-grade chart solution with extreme realtime performance and tech support, we recommend
SciChart Android.

All MPAndroidChart users are entitled to a special discount of 5% off the SciChart store, using the following discount code: MPANDROIDCHART

Bi-Weekly Coding Newsletter
Sign up for my coding newsletter to get quick updates on Kotlin and Android development related topics.
Quick Start 📈
Add the library to your Android project, then check out the examples below!
Gradle Setup
repositories {
    maven { url 'https://jitpack.io' }
}

dependencies {
    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0'
}
Maven Setup
<!-- <repositories> section of pom.xml -->
<repository>
    <id>jitpack.io</id>
   <url>https://jitpack.io</url>
</repository>

<!-- <dependencies> section of pom.xml -->
<dependency>
    <groupId>com.github.PhilJay</groupId>
    <artifactId>MPAndroidChart</artifactId>
    <version>v3.1.0</version>
</dependency>

Documentation 📔
See the documentation for examples and general use of MPAndroidChart.
See the javadocs for more advanced documentation.

Examples 👀
Download the MPAndroidChart Example App or look at the source code.


Questions & Issues 🤔
This repository's issue tracker is only for bugs and feature requests. The maintainers ask that you refrain from asking questions about how to use MPAndroidChart through the issue tracker.
Please read the documentation first, then ask all your questions on stackoverflow.com for the fastest answer.

Donations ❤️
This project needs you! If you would like to support this project's further development, the creator of this project or the continuous maintenance of this project, feel free to donate. Your donation is highly appreciated (and I love food, coffee and beer). Thank you!
My Bitcoin Wallet (Bitcoin only)
1G8G6tqQ3oh38BvDH3xq8o6gGVMvBTkcUg
My Ethereum Wallet (Ethereum only)
0x04ef098bf9f91871391363e3caf791afa3adc39b
Lightning Network (tippin.me)
PayPal

Donate 5 $: Thank's for creating this project, here's a coffee (or some beer) for you!
Donate 10 $: Wow, I am stunned. Let me take you to the movies!
Donate 15 $: I really appreciate your work, let's grab some lunch!
Donate 25 $: That's some awesome stuff you did right there, dinner is on me!
Or you can also choose what you want to donate, all donations are awesome!


Social Media 🔥
If you like this library, please tell others about it 💕 💕



You can follow me on Twitter @PhilippJahoda or sign up for my coding newsletter.

More Examples 👍

LineChart (with legend, simple design)


LineChart (with legend, simple design)


LineChart (cubic lines)


LineChart (gradient fill)


BarChart (with legend, simple design)


BarChart (grouped DataSets)


Horizontal-BarChart


Combined-Chart (bar- and linechart in this case)


PieChart (with selection, ...)


ScatterChart (with squares, triangles, circles, ... and more)


CandleStickChart (for financial data)


BubbleChart (area covered by bubbles indicates the yValue)


RadarChart (spider web chart)


License 📄
Copyright 2019 Philipp Jahoda
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Special Thanks ❤️
These people rock!

danielgindi - Daniel Gindi
mikegr - Michael Greifeneder
tony - Tony
almic - Mick A.
jitpack.io - JitPack.io

"
75,Java,"Glide
 
| View Glide's documentation | 简体中文文档 | Report an issue with Glide
Glide is a fast and efficient open source media management and image loading framework for Android that wraps media
decoding, memory and disk caching, and resource pooling into a simple and easy to use interface.

Glide supports fetching, decoding, and displaying video stills, images, and animated GIFs. Glide includes a flexible API
that allows developers to plug in to almost any network stack. By default Glide uses a custom HttpUrlConnection based
stack, but also includes utility libraries plug in to Google's Volley project or Square's OkHttp library instead.
Glide's primary focus is on making scrolling any kind of a list of images as smooth and fast as possible, but Glide is
also effective for almost any case where you need to fetch, resize, and display a remote image.
Download
For detailed instructions and requirements, see Glide's download and setup docs page.
You can download a jar from GitHub's releases page.
Or use Gradle:
repositories {
  mavenCentral()
  google()
}

dependencies {
  implementation 'com.github.bumptech.glide:glide:4.10.0'
  annotationProcessor 'com.github.bumptech.glide:compiler:4.10.0'
}
Or Maven:
<dependency>
  <groupId>com.github.bumptech.glide</groupId>
  <artifactId>glide</artifactId>
  <version>4.10.0</version>
</dependency>
<dependency>
  <groupId>com.github.bumptech.glide</groupId>
  <artifactId>compiler</artifactId>
  <version>4.10.0</version>
  <optional>true</optional>
</dependency>
For info on using the bleeding edge, see the Snapshots docs page.
ProGuard
Depending on your ProGuard (DexGuard) config and usage, you may need to include the following lines in your proguard.cfg (see the Download and Setup docs page for more details):
-keep public class * implements com.bumptech.glide.module.GlideModule
-keep public class * extends com.bumptech.glide.module.AppGlideModule
-keep public enum com.bumptech.glide.load.ImageHeaderParser$** {
  **[] $VALUES;
  public *;
}

# for DexGuard only
-keepresourcexmlelements manifest/application/meta-data@value=GlideModule
How do I use Glide?
Check out the documentation for pages on a variety of topics, and see the javadocs.
For Glide v3, see the wiki.
Simple use cases will look something like this:
// For a simple view:
@Override public void onCreate(Bundle savedInstanceState) {
  ...
  ImageView imageView = (ImageView) findViewById(R.id.my_image_view);

  Glide.with(this).load(""http://goo.gl/gEgYUd"").into(imageView);
}

// For a simple image list:
@Override public View getView(int position, View recycled, ViewGroup container) {
  final ImageView myImageView;
  if (recycled == null) {
    myImageView = (ImageView) inflater.inflate(R.layout.my_image_view, container, false);
  } else {
    myImageView = (ImageView) recycled;
  }

  String url = myUrls.get(position);

  Glide
    .with(myFragment)
    .load(url)
    .centerCrop()
    .placeholder(R.drawable.loading_spinner)
    .into(myImageView);

  return myImageView;
}
Status
Version 4 is now released and stable. Updates are released periodically with new features and bug fixes.
Comments/bugs/questions/pull requests are always welcome! Please read CONTRIBUTING.md on how to report issues.
Compatibility

Minimum Android SDK: Glide v4 requires a minimum API level of 14.
Compile Android SDK: Glide v4 requires you to compile against API 26 or later.

If you need to support older versions of Android, consider staying on Glide v3, which works on API 10, but is not actively maintained.

OkHttp 3.x: There is an optional dependency available called okhttp3-integration, see the docs page.
Volley: There is an optional dependency available called volley-integration, see the docs page.
Round Pictures: CircleImageView/CircularImageView/RoundedImageView are known to have issues with TransitionDrawable (.crossFade() with .thumbnail() or .placeholder()) and animated GIFs, use a BitmapTransformation (.circleCrop() will be available in v4) or .dontAnimate() to fix the issue.
Huge Images (maps, comic strips): Glide can load huge images by downsampling them, but does not support zooming and panning ImageViews as they require special resource optimizations (such as tiling) to work without OutOfMemoryErrors.

Build
Building Glide with gradle is fairly straight forward:
git clone https://github.com/bumptech/glide.git
cd glide
./gradlew jar
Note: Make sure your Android SDK has the Android Support Repository installed, and that your $ANDROID_HOME environment
variable is pointing at the SDK or add a local.properties file in the root project with a sdk.dir=... line.
Samples
Follow the steps in the Build section to set up the project and then:
./gradlew :samples:flickr:run
./gradlew :samples:giphy:run
./gradlew :samples:svg:run
./gradlew :samples:contacturi:run
You may also find precompiled APKs on the releases page.
Development
Follow the steps in the Build section to setup the project and then edit the files however you wish.
Android Studio cleanly imports both Glide's source and tests and is the recommended way to work with Glide.
To open the project in Android Studio:

Go to File menu or the Welcome Screen
Click on Open...
Navigate to Glide's root directory.
Select setting.gradle

For more details, see the Contributing docs page.
Getting Help
To report a specific problem or feature request, open a new issue on Github. For questions, suggestions, or
anything else, email Glide's discussion group, or join our IRC channel: irc.freenode.net#glide-library.
Contributing
Before submitting pull requests, contributors must sign Google's individual contributor license agreement.
Thanks

The Android team and Jake Wharton for the disk cache implementation Glide's disk cache is based on.
Dave Smith for the GIF decoder gist Glide's GIF decoder is based on.
Chris Banes for his gradle-mvn-push script.
Corey Hall for Glide's amazing logo.
Everyone who has contributed code and reported issues!

Author
Sam Judd - @sjudd on GitHub, @samajudd on Twitter
License
BSD, part MIT and Apache 2.0. See the LICENSE file for details.
Disclaimer
This is not an official Google product.
"
76,Java,"Lottie for Android, iOS, React Native, Web, and Windows


Lottie is a mobile library for Android and iOS that parses Adobe After Effects animations exported as json with Bodymovin and renders them natively on mobile!
For the first time, designers can create and ship beautiful animations without an engineer painstakingly recreating it by hand. They say a picture is worth 1,000 words so here are 13,000:
Sponsors
Lottie is maintained and improved on nights and weekends. If you use Lottie in your app, please consider sponsoring it to help ensure that we can continue to improve the project we love.
Click the sponsor button above to learn more

Lead Sponsors

View documentation, FAQ, help, examples, and more at airbnb.io/lottie





Download
Gradle is the only supported build configuration, so just add the dependency to your project build.gradle file:
dependencies {
  implementation 'com.airbnb.android:lottie:$lottieVersion'
}
The latest Lottie version is:

Lottie 2.8.0 and above only supports projects that have been migrated to androidx. For more information, read Google's migration guide.
"
77,Java,"mall









前言
mall项目致力于打造一个完整的电商系统，采用现阶段流行技术实现。
项目文档

文档地址：http://www.macrozheng.com
备用地址：https://macrozheng.github.io/mall-learning

项目介绍
mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。
项目演示
后台管理系统
前端项目mall-admin-web地址：https://github.com/macrozheng/mall-admin-web
项目演示地址： http://www.macrozheng.com/admin/index.html

前台商城系统
前端项目mall-app-web地址：敬请期待......
项目演示地址：http://www.macrozheng.com/app/index.html

组织结构
mall
├── mall-common -- 工具类及通用代码
├── mall-mbg -- MyBatisGenerator生成的数据库操作代码
├── mall-security -- SpringSecurity封装公用模块
├── mall-admin -- 后台商城管理系统接口
├── mall-search -- 基于Elasticsearch的商品搜索系统
├── mall-portal -- 前台商城系统接口
└── mall-demo -- 框架搭建时的测试代码
技术选型
后端技术



技术
说明
官网




SpringBoot
容器+MVC框架
https://spring.io/projects/spring-boot


SpringSecurity
认证和授权框架
https://spring.io/projects/spring-security


MyBatis
ORM框架
http://www.mybatis.org/mybatis-3/zh/index.html


MyBatisGenerator
数据层代码生成
http://www.mybatis.org/generator/index.html


PageHelper
MyBatis物理分页插件
http://git.oschina.net/free/Mybatis_PageHelper


Swagger-UI
文档生产工具
https://github.com/swagger-api/swagger-ui


Hibernator-Validator
验证框架
http://hibernate.org/validator/


Elasticsearch
搜索引擎
https://github.com/elastic/elasticsearch


RabbitMq
消息队列
https://www.rabbitmq.com/


Redis
分布式缓存
https://redis.io/


MongoDb
NoSql数据库
https://www.mongodb.com/


Docker
应用容器引擎
https://www.docker.com/


Druid
数据库连接池
https://github.com/alibaba/druid


OSS
对象存储
https://github.com/aliyun/aliyun-oss-java-sdk


JWT
JWT登录支持
https://github.com/jwtk/jjwt


LogStash
日志收集工具
https://github.com/logstash/logstash-logback-encoder


Lombok
简化对象封装工具
https://github.com/rzwitserloot/lombok



前端技术



技术
说明
官网




Vue
前端框架
https://vuejs.org/


Vue-router
路由框架
https://router.vuejs.org/


Vuex
全局状态管理框架
https://vuex.vuejs.org/


Element
前端UI框架
https://element.eleme.io/


Axios
前端HTTP框架
https://github.com/axios/axios


v-charts
基于Echarts的图表框架
https://v-charts.js.org/


Js-cookie
cookie管理工具
https://github.com/js-cookie/js-cookie


nprogress
进度条控件
https://github.com/rstacruz/nprogress



架构图
系统架构图

业务架构图

模块介绍
后台管理系统 mall-admin

商品管理：功能结构图-商品.jpg
订单管理：功能结构图-订单.jpg
促销管理：功能结构图-促销.jpg
内容管理：功能结构图-内容.jpg
用户管理：功能结构图-用户.jpg

前台商城系统 mall-portal
功能结构图-前台.jpg
开发进度

环境搭建
开发工具



工具
说明
官网




IDEA
开发IDE
https://www.jetbrains.com/idea/download


RedisDesktop
redis客户端连接工具
https://redisdesktop.com/download


Robomongo
mongo客户端连接工具
https://robomongo.org/download


SwitchHosts
本地host管理
https://oldj.github.io/SwitchHosts/


X-shell
Linux远程连接工具
http://www.netsarang.com/download/software.html


Navicat
数据库连接工具
http://www.formysql.com/xiazai.html


PowerDesigner
数据库设计工具
http://powerdesigner.de/


Axure
原型设计工具
https://www.axure.com/


MindMaster
思维导图设计工具
http://www.edrawsoft.cn/mindmaster


ScreenToGif
gif录制工具
https://www.screentogif.com/


ProcessOn
流程图绘制工具
https://www.processon.com/


PicPick
图片处理工具
https://picpick.app/zh/


Snipaste
屏幕截图工具
https://www.snipaste.com/



开发环境



工具
版本号
下载




JDK
1.8
https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html


Mysql
5.7
https://www.mysql.com/


Redis
3.2
https://redis.io/download


Elasticsearch
6.2.2
https://www.elastic.co/downloads


MongoDb
3.2
https://www.mongodb.com/download-center


RabbitMq
3.7.14
http://www.rabbitmq.com/download.html


Nginx
1.10
http://nginx.org/en/download.html



搭建步骤

Windows环境部署


Windows环境搭建请参考：mall在Windows环境下的部署;
注意：只启动mall-admin,仅需安装mysql即可;
克隆mall-admin-web项目，并导入到IDEA中完成编译传送门;
mall-admin-web项目的安装及部署请参考：mall前端项目的安装与部署;
ELK日志收集系统的搭建请参考：SpringBoot应用整合ELK实现日志收集。


Docker环境部署


使用虚拟机安装CentOS7.6请参考：虚拟机安装及使用Linux，看这一篇就够了！；
Docker环境的安装请参考：开发者必备Docker命令；
本项目Docker镜像构建请参考：使用Maven插件为SpringBoot应用构建Docker镜像；
本项目在Docker容器下的部署请参考：mall在Linux环境下的部署（基于Docker容器）；
本项目使用Docker Compose请参考： mall在Linux环境下的部署（基于Docker Compose）。

参考资料

Spring实战（第4版）
Spring Boot实战
Spring Cloud微服务实战
Spring Cloud与Docker微服务架构实战
Spring Data实战
MyBatis从入门到精通
深入浅出MySQL
循序渐进Linux（第2版）
Elasticsearch 权威指南
Elasticsearch 技术解析与实战
MongoDB实战(第二版)
Kubernetes权威指南
Pro Git

公众号
mall项目全套学习教程连载中，关注公众号第一时间获取。

许可证
Apache License 2.0
Copyright (c) 2018-2019 macrozheng
"
78,Java,"

   
README of Chinese
About
AndroidUtilCode 🔥 is a powerful & easy to use library for Android. This library encapsulates the functions that commonly used in Android development which have complete demo and unit test. By using it's encapsulated APIs, you can greatly improve the development efficiency. The program mainly consists of two modules which is utilcode, which is commonly used in development, and subutil which is rarely used in development, but the utils can be beneficial to simplify the main module. 🔥
Documentation
utilcode

README of English
README of Chinese

subutil

README of English
README of Chinese

Donations
If this project helps you a lot and you want to support the project's development and maintenance of this project, feel free to scan the following QR code for donation. Your donation is highly appreciated. Thank you!

Contact
   
Change Log
打个小广告
欢迎加入我的知识星球「基你太美」，我会在星球中分享 AucFrame 框架、大厂面经、AndroidUtilCode 更详尽的说明...一切我所了解的知识，你可以通过支付进入我的星球「基你太美」进行体验，加入后优先观看星球中精华的部分，如果觉得星球的内容对自身没有收益，你可以自行申请退款退出星球，也没必要加我好友；如果你已确定要留在我的星球，可以通过扫描如下二维码（备注：基你太美）加我个人微信，发送给我你的星球 ID，方便我后续拉你进群(PS：进得越早价格越便宜)。

"
79,Java,"暂停维护此项目
首先感谢大家支持和反馈才使得proxyee-down能一直迭代到现在的版本，但由于本人精力有限，宣布暂时停止此项目的维护，并且关闭issue模块。
其次因为JAVA不太适合做客户端开发，打包后体积太大且内存占用太高，本人计划在空余时间用GO来重写一遍，目标是打造一个体积小、跨平台、内存低、可扩展、免费的下载器。
新项目目前托管在go-download，现处于初始化阶段，感兴趣的可以一起参与哦~

Proxyee Down






Proxyee Down 是一款开源的免费 HTTP 高速下载器，底层使用netty开发，支持自定义 HTTP 请求下载且支持扩展功能，可以通过安装扩展实现特殊的下载需求。

使用教程
点击查看教程
交流群
1 群11352304、2 群20236964、3 群20233754、4 群737991056
开发
本项目后端主要使用 java + spring + boot + netty，前端使用 vue.js + iview
环境
  
oracle jdk 1.8+或 openjfx(openjdk默认不包含javafx包)

编译
git clone https://github.com/proxyee-down-org/proxyee-down.git
cd proxyee-down/front
#build html
npm install
npm run build
cd ../main
mvn clean package -Pprd

运行
java -jar proxyee-down-main.jar

"
80,Java,"JADX






jadx - Dex to Java decompiler
Command line and GUI tools for produce Java source code from Android Dex and Apk files
Main features:

decompile Dalvik bytecode to java classes from APK, dex, aar and zip files
decode AndroidManifest.xml and other resources from resources.arsc
deobfuscator included

jadx-gui features:

view decompiled code with highlighted syntax
jump to declaration
find usage
full text search

See these features in action here: jadx-gui features overview

Download

latest unstable build:  
release from github: 
release from bintray:  

After download unpack zip file go to bin directory and run:

jadx - command line version
jadx-gui - UI version

On Windows run .bat files with double-click
Note: ensure you have installed Java 8 or later 64-bit version.
For windows you can download it from adoptopenjdk.net (select ""Install JRE"").
Install

Arch linux
    sudo pacman -S jadx

macOS
    brew install jadx


Build from source
JDK 8 or higher must be installed:
git clone https://github.com/skylot/jadx.git
cd jadx
./gradlew dist

(on Windows, use gradlew.bat instead of ./gradlew)
Scripts for run jadx will be placed in build/jadx/bin
and also packed to build/jadx-<version>.zip
Usage
jadx[-gui] [options] <input file> (.apk, .dex, .jar, .class, .smali, .zip, .aar, .arsc)
options:
  -d, --output-dir                    - output directory
  -ds, --output-dir-src               - output directory for sources
  -dr, --output-dir-res               - output directory for resources
  -r, --no-res                        - do not decode resources
  -s, --no-src                        - do not decompile source code
  --single-class                      - decompile a single class
  --output-format                     - can be 'java' or 'json', default: java
  -e, --export-gradle                 - save as android gradle project
  -j, --threads-count                 - processing threads count, default: 4
  --show-bad-code                     - show inconsistent code (incorrectly decompiled)
  --no-imports                        - disable use of imports, always write entire package name
  --no-debug-info                     - disable debug info
  --no-inline-anonymous               - disable anonymous classes inline
  --no-replace-consts                 - don't replace constant value with matching constant field
  --escape-unicode                    - escape non latin characters in strings (with \u)
  --respect-bytecode-access-modifiers - don't change original access modifiers
  --deobf                             - activate deobfuscation
  --deobf-min                         - min length of name, renamed if shorter, default: 3
  --deobf-max                         - max length of name, renamed if longer, default: 64
  --deobf-rewrite-cfg                 - force to save deobfuscation map
  --deobf-use-sourcename              - use source file name as class name alias
  --rename-flags                      - what to rename, comma-separated, 'case' for system case sensitivity, 'valid' for java identifiers, 'printable' characters, 'none' or 'all' (default)
  --fs-case-sensitive                 - treat filesystem as case sensitive, false by default
  --cfg                               - save methods control flow graph to dot file
  --raw-cfg                           - save methods control flow graph (use raw instructions)
  -f, --fallback                      - make simple dump (using goto instead of 'if', 'for', etc)
  -v, --verbose                       - verbose output (set --log-level to DEBUG)
  -q, --quiet                         - turn off output (set --log-level to QUIET)
  --log-level                         - set log level, values: QUIET, PROGRESS, ERROR, WARN, INFO, DEBUG, default: PROGRESS
  --version                           - print jadx version
  -h, --help                          - print this help
Example:
 jadx -d out classes.dex
 jadx --rename-flags ""none"" classes.dex
 jadx --rename-flags ""valid,printable"" classes.dex
 jadx --log-level error app.apk

These options also worked on jadx-gui running from command line and override options from preferences dialog
Troubleshooting
Please check wiki page Troubleshooting Q&A
Contributing
To support this project you can:

Post thoughts about new features/optimizations that important to you
Submit decompilation issues, please read before proceed: Open issue
Open pull request, please follow these rules: Pull Request Process

Related projects:

PyJadx - python binding for jadx by @romainthomas


Licensed under the Apache 2.0 License
Copyright 2019 by Skylot
"
81,Java,"Spring Boot 学习示例





Spring Boot 使用的各种示例，以最简单、最实用为标准，此开源项目中的每个示例都以最小依赖，最简单为标准，帮助初学者快速掌握 Spring Boot 各组件的使用。
Spring Boot 中文索引  |   Spring Cloud学习示例代码  |   Spring Boot 精品课程
English  |    Github地址  |   码云地址  |    Spring Boot 1.0

Spring Boot 2.0
Spring Boot 2.0 最全使用教程
Favorites-web：云收藏（Spring Boot 2.0 实战开源项目）
示例代码

spring-boot-hello：Spring Boot 2.0  Hello World 示例
spring-boot-banner：Spring Boot 定制 Banner 示例
spring-boot-docker：使用 Docker 部署 Spring Boot 示例
dockercompose-springboot-mysql-nginx ：Docker Compose + Spring Boot + Nginx + Mysql 示例
spring-boot-commandLineRunner ：Spring Boot 使用 commandLineRunner 实现项目启动时资源初始化示例
spring-boot-web-thymeleaf ：Spring Boot 使用 thymeleaf 实现布局、验参、增删改查示例
spring-boot-memcache-spymemcached ：Spring Boot 使用 spymemcached 集成  memcache 示例
spring-boot-webflux ：Spring Boot webflux 示例
spring-boot-elasticsearch ：Spring Boot elasticsearch 示例
spring-boot-swagger ：Spring Boot swagger2 示例
spring-boot-mybatis-plus ：Spring Boot 集成 MyBatis Plus 示例

参考文章

Spring Boot 2(一)：【重磅】Spring Boot 2.0权威发布
Spring Boot 2(二)：Spring Boot 2.0尝鲜-动态 Banner
Spring Boot 2(三)：Spring Boot 开源软件都有哪些？
Spring Boot 2(四)：使用 Docker 部署 Spring Boot
Spring Boot 2(五)：Docker Compose + Spring Boot + Nginx + Mysql 实践
Spring Boot 2(六)：使用 Docker 部署 Spring Boot 开源软件云收藏
Spring Boot 2(七)：Spring Boot 如何解决项目启动时初始化资源
Spring Boot 2(八)：Spring Boot 集成 Memcached
Spring Boot 2 (九)：【重磅】Spring Boot 2.1.0 权威发布
Spring Boot/Cloud 研发团队介绍
Spring Boot 2 (十)：Spring Boot 中的响应式编程和 WebFlux 入门

下方示例已经全部升级到 2.X，可关注下方公号查看。

另外关注后，回复：java 获取超过10000+人领取的 Java 知识体系/面试必看资料。
示例代码

spring-boot-helloWorld：Spring Boot 的 hello World 版本
spring-boot-web：Spring Boot Web 开发综合示例
spring-boot-redis：Spring Boot 集成 Redis 示例
spring-boot-jpa：Spring Boot 使用 Jpa 各种示例
spring-boot-mybaits-annotation：注解版本
spring-boot-mybaits-xml：Xml 配置版本
spring-boot-mybatis-xml-mulidatasource：Spring Boot + Mybatis (Xml 版） 多数据源最简解决方案
spring-boot-mybatis-annotation-mulidatasource：Spring Boot + Mybatis（注解版）多数据源最简解决方案
spring-boot-thymeleaf：Spring Boot 使用 Thymeleaf 详细示例
spring-boot-jpa-thymeleaf-curd：Spring Boot  + Jpa + Thymeleaf 增删改查示例
spring-boot-rabbitmq：Spring Boot 和 Rabbitmq 各种消息应用案例
spring-boot-scheduler：Spring Boot 和定时任务案例
spring-boot-mail：Spring Boot 和邮件服务
spring-boot-mongodb：Spring Boot 和 Mongodb 的使用
spring-boot-multi-mongodb：Spring Boot 和 Mongodb 多数据源的使用
spring-boot-package-war： Spring Boot 打包成 War 包示例
spring-boot-shiro：Spring Boot  整合 Shiro Rbac 示例
spring-boot-file-upload：使用 Spring Boot 上传文件示例
spring-boot-fastDFS：Spring Boot 整合 FastDFS 示例
spring-boot-actuator：Spring Boot Actuator 使用示例
spring-boot-admin-simple：Spring Boot Admin 的使用示例

参考文章

Spring Boot(一)：入门篇
Spring Boot(二)：Web 综合开发
Spring Boot(三)：Spring Boot 中 Redis 的使用
Spring Boot(四)：Thymeleaf 使用详解
Spring Boot(五)：Spring Data Jpa 的使用
Spring Boot(六)：如何优雅的使用 Mybatis
Spring Boot(七)：Spring Boot + Mybatis 多数据源最简解决方案
Spring Boot(八)：RabbitMQ 详解
Spring Boot(九)：定时任务
Spring Boot(十)：邮件服务
Spring Boot(十一)：Spring Boot 中 Mongodb 的使用
Spring Boot(十二)：Spring Boot 如何测试打包部署
Spring Boot(十三)：Spring Boot 小技巧
Spring Boot(十四)：Spring Boot 整合 Shiro-登录认证和权限管理
Spring Boot(十五)：Spring Boot + Jpa + Thymeleaf 增删改查示例
Spring Boot(十六)：使用 Jenkins 部署 Spring Boot
Spring Boot(十七)：使用 Spring Boot 上传文件
Spring Boot(十八)：使用 Spring Boot 集成 FastDFS
Spring Boot(十九)：使用 Spring Boot Actuator 监控应用
Spring Boot(二十)：使用 spring-boot-admin 对 Spring Boot 服务进行监控

Spring Boot 实战：我们的第一款开源项目


如果大家想了解关于 Spring Boot 的其它方面应用，也可以以issues的形式反馈给我，我后续来完善。

关注公众号：纯洁的微笑，回复""springboot""进群交流

"
82,Java,"Android智能下拉刷新框架-SmartRefreshLayout





English | 中文
SmartRefreshLayout以打造一个强大，稳定，成熟的下拉刷新框架为目标，并集成各种的炫酷、多样、实用、美观的Header和Footer。
正如名字所说，SmartRefreshLayout是一个“聪明”或者“智能”的下拉刷新布局，由于它的“智能”，它不只是支持所有的View，还支持多层嵌套的视图结构。
它继承自ViewGroup 而不是FrameLayout或LinearLayout，提高了性能。
也吸取了现在流行的各种刷新布局的优点，包括谷歌官方的 SwipeRefreshLayout，
其他第三方的 Ultra-Pull-To-Refresh、TwinklingRefreshLayout 。
还集成了各种炫酷的 Header 和 Footer。
注：本库也在开源中国上开源，如果有时候github出现下载缓慢问题可以转到开源中国下载或clone，记得star哦.
特点功能:

支持横向刷新
支持多点触摸
支持淘宝二楼和二级刷新
支持嵌套多层的视图结构 Layout (LinearLayout,FrameLayout...)
支持所有的 View（AbsListView、RecyclerView、WebView....View）
支持自定义并且已经集成了很多炫酷的 Header 和 Footer.
支持和 ListView 的无缝同步滚动 和 CoordinatorLayout 的嵌套滚动 .
支持自动刷新、自动上拉加载（自动检测列表惯性滚动到底部，而不用手动上拉）.
支持自定义回弹动画的插值器，实现各种炫酷的动画效果.
支持设置主题来适配任何场景的 App，不会出现炫酷但很尴尬的情况.
支持设多种滑动方式：平移、拉伸、背后固定、顶层固定、全屏
支持所有可滚动视图的越界回弹
支持 Header 和 Footer 交换混用
支持AndroidX

每天领红包
最近开通了支付宝商家，生成了个红包二维码，经常用支付宝的童鞋可有扫码领优惠红包，扫码只会拿红包，不会有任何损失，每天都可以扫码哦！


你也可以在支付宝中直接搜索 553866294 来获取红包。如果得到的是花呗红包，也不用失望。如果你经常使用信用卡的话那么使用花呗红包非常适合你，它也和信用卡一样先消费后还款，关键是每天都能扫红包省钱！

传送门

属性文档
常见问题
智能之处
更新日志
博客文章
源码下载
多点触摸
自定义Header

Demo
下载 APK-Demo

项目演示



个人首页
微博列表












餐饮美食
个人中心









样式演示 Style



Delivery
DropBox








Refresh-your-delivery
Dropbox-Refresh



上面这两个是我自己实现的，下面的是我把github上其它优秀的Header进行的整理和集合还有优化：



BezierRadar
BezierCircle








Pull To Refresh
Pull Down To Refresh






FlyRefresh
Classics








FlyRefresh
ClassicsHeader






Phoenix
Taurus








Yalantis/Phoenix
Yalantis/Taurus






BattleCity
HitBlock








FunGame/BattleCity
FunGame/HitBlock






WaveSwipe
Material








WaveSwipeRefreshLayout
MaterialHeader






StoreHouse
WaterDrop








CRefreshLayout
WaterDrop



看到这么多炫酷的Header，是不是觉得很棒？这时你或许会担心这么多的Header集成在一起，但是平时只会用到一个，是不是要引入很多无用的代码和资源？
请放心，我已经把刷新布局分成三个包啦，用到的时候自行引用就可以啦！

SmartRefreshLayout 刷新布局核心实现，自带ClassicsHeader（经典）、BezierRadarHeader（贝塞尔雷达）两个 Header.
SmartRefreshHeader 各种Header的集成，除了Layout自带的Header，其它都在这个包中.
SmartRefreshFooter 各种Footer的集成，除了Layout自带的Footer，其它都在这个包中.

简单用例
1.在 build.gradle 中添加依赖
【V2.0.0】 版本已经在开发，主要是对各个功能类进行分包，比如不用二级刷新就不依赖，避免代码冗余，欢迎大家来体验
implementation 'com.scwang.smartrefresh:SmartRefreshLayout:1.1.0'  //1.0.5及以前版本的老用户升级需谨慎，API改动过大
implementation 'com.scwang.smartrefresh:SmartRefreshHeader:1.1.0'  //没有使用特殊Header，可以不加这行


如果使用 AndroidX 在 gradle.properties 中添加
android.useAndroidX=true
android.enableJetifier=true


2.在XML布局文件中添加 SmartRefreshLayout
<?xml version=""1.0"" encoding=""utf-8""?>
<com.scwang.smartrefresh.layout.SmartRefreshLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    android:id=""@+id/refreshLayout""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent"">
    <android.support.v7.widget.RecyclerView
        android:id=""@+id/recyclerView""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:overScrollMode=""never""
        android:background=""#fff"" />
</com.scwang.smartrefresh.layout.SmartRefreshLayout>
3.在 Activity 或者 Fragment 中添加代码
RefreshLayout refreshLayout = (RefreshLayout)findViewById(R.id.refreshLayout);
refreshLayout.setOnRefreshListener(new OnRefreshListener() {
    @Override
    public void onRefresh(RefreshLayout refreshlayout) {
        refreshlayout.finishRefresh(2000/*,false*/);//传入false表示刷新失败
    }
});
refreshLayout.setOnLoadMoreListener(new OnLoadMoreListener() {
    @Override
    public void onLoadMore(RefreshLayout refreshlayout) {
        refreshlayout.finishLoadMore(2000/*,false*/);//传入false表示加载失败
    }
});
使用指定的 Header 和 Footer
1.方法一 全局设置
public class App extends Application {
    //static 代码段可以防止内存泄露
    static {
        //设置全局的Header构建器
        SmartRefreshLayout.setDefaultRefreshHeaderCreator(new DefaultRefreshHeaderCreator() {
                @Override
                public RefreshHeader createRefreshHeader(Context context, RefreshLayout layout) {
                    layout.setPrimaryColorsId(R.color.colorPrimary, android.R.color.white);//全局设置主题颜色
                    return new ClassicsHeader(context);//.setTimeFormat(new DynamicTimeFormat(""更新于 %s""));//指定为经典Header，默认是 贝塞尔雷达Header
                }
            });
        //设置全局的Footer构建器
        SmartRefreshLayout.setDefaultRefreshFooterCreator(new DefaultRefreshFooterCreator() {
                @Override
                public RefreshFooter createRefreshFooter(Context context, RefreshLayout layout) {
                    //指定为经典Footer，默认是 BallPulseFooter
                    return new ClassicsFooter(context).setDrawableSize(20);
                }
            });
    }
}
注意：方法一 设置的Header和Footer的优先级是最低的，如果同时还使用了方法二、三，将会被其它方法取代
2.方法二 XML布局文件指定
<com.scwang.smartrefresh.layout.SmartRefreshLayout
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    android:id=""@+id/refreshLayout""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:background=""#444444""
    app:srlPrimaryColor=""#444444""
    app:srlAccentColor=""@android:color/white""
    app:srlEnablePreviewInEditMode=""true"">
    <!--srlAccentColor srlPrimaryColor 将会改变 Header 和 Footer 的主题颜色-->
    <!--srlEnablePreviewInEditMode 可以开启和关闭预览功能-->
    <com.scwang.smartrefresh.layout.header.ClassicsHeader
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""/>
    <TextView
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:padding=""@dimen/dimenPaddingCommon""
        android:background=""@android:color/white""
        android:text=""@string/description_define_in_xml""/>
    <com.scwang.smartrefresh.layout.footer.ClassicsFooter
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""/>
</com.scwang.smartrefresh.layout.SmartRefreshLayout>
注意：方法二 XML设置的Header和Footer的优先级是中等的，会被方法三覆盖。而且使用本方法的时候，Android Studio 会有预览效果，如下图：

不过不用担心，只是预览效果，运行的时候只有下拉才会出现~
3.方法三 Java代码设置
final RefreshLayout refreshLayout = (RefreshLayout) findViewById(R.id.refreshLayout);
//设置 Header 为 贝塞尔雷达 样式
refreshLayout.setRefreshHeader(new BezierRadarHeader(this).setEnableHorizontalDrag(true));
//设置 Footer 为 球脉冲 样式
refreshLayout.setRefreshFooter(new BallPulseFooter(this).setSpinnerStyle(SpinnerStyle.Scale));
4.更多使用说明

属性文档
常见问题
自定义Header

混淆
SmartRefreshLayout 没有使用到：序列化、反序列化、JNI、反射，所以并不需要添加混淆过滤代码，并且已经混淆测试通过，如果你在项目的使用中混淆之后出现问题，请及时通知我。
赞赏
如果你喜欢 SmartRefreshLayout 的设计，感觉 SmartRefreshLayout 帮助到了你，可以点右上角 ""Star"" 支持一下 谢谢！ ^_^
你也还可以扫描下面的二维码~ 请作者喝一杯咖啡。
  

如果希望捐赠之后能获得相关的帮助，可以选择加入下面的付费群来取代普通捐赠，付费群可以直接获得作者的直接帮助，与问题反馈。

如果在捐赠留言中备注名称，将会被记录到列表中~ 如果你也是github开源作者，捐赠时可以留下github项目地址或者个人主页地址，链接将会被添加到列表中起到互相推广的作用
捐赠列表
友情链接
github/faith-hb/WidgetCase
github/Bamboy120315/Freedom
github/TommyLemon/APIJSON
github/dengyuhan
github/zrp2017
github/fly803/BaseProject
github/razerdp
github/SuperChenC/s-mvp
github/KingJA/LoadSir
github/jianshijiuyou
github/zxy198717
github/addappcn
github/RainliFu
github/sugarya
github/stormzhang
讨论
QQ解决群 - 602537182 （付费）
进群须知
自开群以来，还是有很多的朋友提出了很多问题，我也解决了很多问题，其中有大半问题是本库的Bug导致，也有些是使用者项目本
身的环境问题，这花费了我大量的时间，经过我的观察和测试，到目前为止，本库的bug已经越来越少，当然不能说完全没有，但是
已经能满足很大部分项目的需求。所以从现在起，我做出一个决定：把之前的讨论群改成解决群，并开启付费入群功能，专为解决大
家在使用本库时遇到的问题，不管是本库bug还是，特殊的项目环境导致（包含项目本身的bug）。
我也有自己的工作和娱乐时间，只有大家理解和支持我，我才能专心的为大家解决问题。不过用担心，我已经建立了另一个可以免费
进入的QQ讨论群。
QQ讨论群 - 914275312 （新） 477963933 （满）  538979188 （满）
进群须知
这个群，免费进入，大家可以相互讨论本库的相关使用和出现的问题，群主也会在里面解决问题，如果提出的问题，群成员不能帮助
解决，需要群主解决，但是要花费群主五分钟以上的时间（本库Bug除外），群主将不会解决这个问题，如果项目紧急，请付费进入解
决群解决（不过注意，付费群中群主会很认真很努力的解决问题，但也不能保证已经能完美解决）或者转换使用其他的刷新库。
温馨提示
加入群的答案在本文档中可以找到~
其他作品
MultiWaveHeader
诗和远方
感谢
SwipeRefreshLayout
Ultra-Pull-To-Refresh
TwinklingRefreshLayout
BeautifulRefreshLayout
License
Copyright 2017 scwang90

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
83,Java,"

BRVAH
http://www.recyclerview.org/
Powerful and flexible RecyclerAdapter,
Please feel free to use this. (Welcome to Star and Fork)
kotlin demo :BRVAH_kotlin
androidX stable version 
Document

English
中文1
中文2

UI
Demo

国内下载地址
proguard-rules.pro

此资源库自带混淆规则，并且会自动导入，正常情况下无需手动导入。
The library comes with proguard-rules.pro rules and is automatically imported. Normally no manual import is required.
You can also go here to view proguard-rules

Extension library
PinnedSectionItemDecoration
EasyRefreshLayout
EasySwipeMenuLayout
Thanks
JoanZapata / base-adapter-helper
License
Copyright 2016 陈宇明

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
84,Java,"
Hystrix: Latency and Fault Tolerance for Distributed Systems




Hystrix Status
Hystrix is no longer in active development, and is currently in maintenance mode.
Hystrix (at version 1.5.18) is stable enough to meet the needs of Netflix for our existing applications. Meanwhile, our focus has shifted towards more adaptive implementations that react to an application’s real time performance rather than pre-configured settings (for example, through adaptive concurrency limits). For the cases where something like Hystrix makes sense, we intend to continue using Hystrix for existing applications, and to leverage open and active projects like resilience4j for new internal projects. We are beginning to recommend others do the same.
Netflix Hystrix is now officially in maintenance mode, with the following expectations to the greater community:
Netflix will no longer actively review issues, merge pull-requests, and release new versions of Hystrix.
We have made a final release of Hystrix (1.5.18) per issue 1891 so that the latest version in Maven Central is aligned with the last known stable version used internally at Netflix (1.5.11).
If members of the community are interested in taking ownership of Hystrix and moving it back into active mode, please reach out to hystrixoss@googlegroups.com.
Hystrix has served Netflix and the community well over the years, and the transition to maintenance mode is in no way an indication that the concepts and ideas from Hystrix are no longer valuable. On the contrary, Hystrix has inspired many great ideas and projects. We thank everyone at Netflix, and in the greater community, for all the contributions made to Hystrix over the years.
Introduction
Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable.
Full Documentation
See the Wiki for full documentation, examples, operational details and other information.
See the Javadoc for the API.
Communication

Google Group: HystrixOSS
Twitter: @HystrixOSS
GitHub Issues

What does it do?
1) Latency and Fault Tolerance
Stop cascading failures. Fallbacks and graceful degradation. Fail fast and rapid recovery.
Thread and semaphore isolation with circuit breakers.
2) Realtime Operations
Realtime monitoring and configuration changes. Watch service and property changes take effect immediately as they spread across a fleet.
Be alerted, make decisions, affect change and see results in seconds.
3) Concurrency
Parallel execution. Concurrency aware request caching. Automated batching through request collapsing.
Hello World!
Code to be isolated is wrapped inside the run() method of a HystrixCommand similar to the following:
public class CommandHelloWorld extends HystrixCommand<String> {

    private final String name;

    public CommandHelloWorld(String name) {
        super(HystrixCommandGroupKey.Factory.asKey(""ExampleGroup""));
        this.name = name;
    }

    @Override
    protected String run() {
        return ""Hello "" + name + ""!"";
    }
}
This command could be used like this:
String s = new CommandHelloWorld(""Bob"").execute();
Future<String> s = new CommandHelloWorld(""Bob"").queue();
Observable<String> s = new CommandHelloWorld(""Bob"").observe();
More examples and information can be found in the How To Use section.
Example source code can be found in the hystrix-examples module.
Binaries
Binaries and dependency information for Maven, Ivy, Gradle and others can be found at http://search.maven.org.
Change history and version numbers => CHANGELOG.md
Example for Maven:
<dependency>
    <groupId>com.netflix.hystrix</groupId>
    <artifactId>hystrix-core</artifactId>
    <version>x.y.z</version>
</dependency>
and for Ivy:
<dependency org=""com.netflix.hystrix"" name=""hystrix-core"" rev=""x.y.z"" />
If you need to download the jars instead of using a build system, create a Maven pom file like this with the desired version:
<?xml version=""1.0""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>
	<groupId>com.netflix.hystrix.download</groupId>
	<artifactId>hystrix-download</artifactId>
	<version>1.0-SNAPSHOT</version>
	<name>Simple POM to download hystrix-core and dependencies</name>
	<url>http://github.com/Netflix/Hystrix</url>
	<dependencies>
		<dependency>
			<groupId>com.netflix.hystrix</groupId>
			<artifactId>hystrix-core</artifactId>
			<version>x.y.z</version>
			<scope/>
		</dependency>
	</dependencies>
</project>
Then execute:
mvn -f download-hystrix-pom.xml dependency:copy-dependencies

It will download hystrix-core-*.jar and its dependencies into ./target/dependency/.
You need Java 6 or later.
Build
To build:
$ git clone git@github.com:Netflix/Hystrix.git
$ cd Hystrix/
$ ./gradlew build

Futher details on building can be found on the Getting Started page of the wiki.
Run Demo
To run a demo app do the following:
$ git clone git@github.com:Netflix/Hystrix.git
$ cd Hystrix/
./gradlew runDemo

You will see output similar to the following:
Request => GetUserAccountCommand[SUCCESS][8ms], GetPaymentInformationCommand[SUCCESS][20ms], GetUserAccountCommand[SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][101ms], CreditCardCommand[SUCCESS][1075ms]
Request => GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS][2ms], GetPaymentInformationCommand[SUCCESS][22ms], GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][130ms], CreditCardCommand[SUCCESS][1050ms]
Request => GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS][4ms], GetPaymentInformationCommand[SUCCESS][19ms], GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][145ms], CreditCardCommand[SUCCESS][1301ms]
Request => GetUserAccountCommand[SUCCESS][4ms], GetPaymentInformationCommand[SUCCESS][11ms], GetUserAccountCommand[SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][93ms], CreditCardCommand[SUCCESS][1409ms]

#####################################################################################
# CreditCardCommand: Requests: 17 Errors: 0 (0%)   Mean: 1171 75th: 1391 90th: 1470 99th: 1486 
# GetOrderCommand: Requests: 21 Errors: 0 (0%)   Mean: 100 75th: 144 90th: 207 99th: 230 
# GetUserAccountCommand: Requests: 21 Errors: 4 (19%)   Mean: 8 75th: 11 90th: 46 99th: 51 
# GetPaymentInformationCommand: Requests: 21 Errors: 0 (0%)   Mean: 18 75th: 21 90th: 24 99th: 25 
#####################################################################################

Request => GetUserAccountCommand[SUCCESS][10ms], GetPaymentInformationCommand[SUCCESS][16ms], GetUserAccountCommand[SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][51ms], CreditCardCommand[SUCCESS][922ms]
Request => GetUserAccountCommand[SUCCESS][12ms], GetPaymentInformationCommand[SUCCESS][12ms], GetUserAccountCommand[SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][68ms], CreditCardCommand[SUCCESS][1257ms]
Request => GetUserAccountCommand[SUCCESS][10ms], GetPaymentInformationCommand[SUCCESS][11ms], GetUserAccountCommand[SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][78ms], CreditCardCommand[SUCCESS][1295ms]
Request => GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS][6ms], GetPaymentInformationCommand[SUCCESS][11ms], GetUserAccountCommand[FAILURE, FALLBACK_SUCCESS, RESPONSE_FROM_CACHE][0ms]x2, GetOrderCommand[SUCCESS][153ms], CreditCardCommand[SUCCESS][1321ms]

This demo simulates 4 different HystrixCommand implementations with failures, latency, timeouts and duplicate calls in a multi-threaded environment.
It logs the results of HystrixRequestLog and metrics from HystrixCommandMetrics.
Dashboard
The hystrix-dashboard component of this project has been deprecated and moved to Netflix-Skunkworks/hystrix-dashboard. Please see the README there for more details including important security considerations.
Bugs and Feedback
For bugs, questions and discussions please use the GitHub Issues.
LICENSE
Copyright 2013 Netflix, Inc.
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"
85,Java,"
Apollo - A reliable configuration management system









Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。
服务端基于Spring Boot和Spring Cloud开发，打包后可以直接运行，不需要额外安装Tomcat等应用容器。
Java客户端不依赖任何框架，能够运行于所有Java运行时环境，同时对Spring/Spring Boot环境也有较好的支持。
.Net客户端不依赖任何框架，能够运行于所有.Net运行时环境。
更多产品介绍参见Apollo配置中心介绍
本地快速部署请参见Quick Start
演示环境（Demo）:

106.54.227.205
账号/密码:apollo/admin


如访问github速度缓慢，可以访问gitee镜像，不定期同步

Screenshots

Features


统一管理不同环境、不同集群的配置

Apollo提供了一个统一界面集中式管理不同环境（environment）、不同集群（cluster）、不同命名空间（namespace）的配置。
同一份代码部署在不同的集群，可以有不同的配置，比如zk的地址等
通过命名空间（namespace）可以很方便的支持多个不同应用共享同一份配置，同时还允许应用对共享的配置进行覆盖
配置界面支持多语言（中文，English）



配置修改实时生效（热发布）

用户在Apollo修改完配置并发布后，客户端能实时（1秒）接收到最新的配置，并通知到应用程序。



版本发布管理

所有的配置发布都有版本概念，从而可以方便的支持配置的回滚。



灰度发布

支持配置的灰度发布，比如点了发布后，只对部分应用实例生效，等观察一段时间没问题后再推给所有应用实例。



权限管理、发布审核、操作审计

应用和配置的管理都有完善的权限管理机制，对配置的管理还分为了编辑和发布两个环节，从而减少人为的错误。
所有的操作都有审计日志，可以方便的追踪问题。



客户端配置信息监控

可以方便的看到配置在被哪些实例使用



提供Java和.Net原生客户端

提供了Java和.Net的原生客户端，方便应用集成
支持Spring Placeholder, Annotation和Spring Boot的ConfigurationProperties，方便应用使用（需要Spring 3.1.1+）
同时提供了Http接口，非Java和.Net应用也可以方便的使用



提供开放平台API

Apollo自身提供了比较完善的统一配置管理界面，支持多环境、多数据中心配置管理、权限、流程治理等特性。
不过Apollo出于通用性考虑，对配置的修改不会做过多限制，只要符合基本的格式就能够保存。
在我们的调研中发现，对于有些使用方，它们的配置可能会有比较复杂的格式，如xml, json，需要对格式做校验。
还有一些使用方如DAL，不仅有特定的格式，而且对输入的值也需要进行校验后方可保存，如检查数据库、用户名和密码是否匹配。
对于这类应用，Apollo支持应用方通过开放接口在Apollo进行配置的修改和发布，并且具备完善的授权和权限控制



部署简单

配置中心作为基础服务，可用性要求非常高，这就要求Apollo对外部依赖尽可能地少
目前唯一的外部依赖是MySQL，所以部署非常简单，只要安装好Java和MySQL就可以让Apollo跑起来
Apollo还提供了打包脚本，一键就可以生成所有需要的安装包，并且支持自定义运行时参数



Usage

Apollo使用指南
Java客户端使用指南
.Net客户端使用指南
其它语言客户端接入指南
Apollo开放平台接入指南
Apollo使用场景和示例代码

Design

Apollo配置中心设计
Apollo核心概念之“Namespace”
Apollo配置中心架构剖析
Apollo源码解析（据说Apollo非常适合作为初学者第一个通读源码学习的分布式中间件产品）

Development

Apollo开发指南
Code Styles

Eclipse Code Style
Intellij Code Style



Deployment

Quick Start
分布式部署指南

Release Notes

版本发布历史

FAQ

常见问题回答
部署&开发遇到的常见问题

Presentation

携程开源配置中心Apollo的设计与实现

Slides


配置中心，让微服务更『智能』

Slides



Publication

开源配置中心Apollo的设计与实现
配置中心，让微服务更『智能』

Support


Apollo技术支持⑤群群号：914839843（未满）
Apollo技术支持④群群号：516773934（已满）
Apollo技术支持③群群号：742035428（已满）
Apollo技术支持②群群号：904287263（已满）
Apollo技术支持①群群号：375526581（已满）











Contribution
Please make sure to read the Contributing Guide before making a pull request.
Thanks for all the people who contributed to Apollo!

License
The project is licensed under the Apache 2 license.
Known Users

按照登记顺序排序，更多接入公司，欢迎在https://github.com/ctripcorp/apollo/issues/451登记（仅供开源用户参考）























































































































































































































Awards

Stargazers over time

"
86,Java,"RxAndroid: Reactive Extensions for Android
Android specific bindings for RxJava 2.
This module adds the minimum classes to RxJava that make writing reactive components in Android
applications easy and hassle-free. More specifically, it provides a Scheduler that schedules on
the main thread or any given Looper.
Communication
Since RxAndroid is part of the RxJava family the communication channels are similar:

Google Group: RxJava
Twitter: @RxJava
StackOverflow: rx-android
GitHub Issues

Binaries
implementation 'io.reactivex.rxjava2:rxandroid:2.1.1'
// Because RxAndroid releases are few and far between, it is recommended you also
// explicitly depend on RxJava's latest version for bug fixes and new features.
// (see https://github.com/ReactiveX/RxJava/releases for latest 2.x.x version)
implementation 'io.reactivex.rxjava2:rxjava:2.x.x'

RxAndroid: 
RxJava: 

Additional binaries and dependency information for can be found at http://search.maven.org.
Build
To build:
$ git clone git@github.com:ReactiveX/RxAndroid.git
$ cd RxAndroid/
$ ./gradlew build
Further details on building can be found on the RxJava Getting Started page of the wiki.

Sample usage
A sample project which provides runnable code examples that demonstrate uses of the classes in this
project is available in the sample-app/ folder.
Observing on the main thread
One of the most common operations when dealing with asynchronous tasks on Android is to observe the task's
result or outcome on the main thread. Using vanilla Android, this would typically be accomplished with an
AsyncTask. With RxJava instead you would declare your Observable to be observed on the main thread:
Observable.just(""one"", ""two"", ""three"", ""four"", ""five"")
        .subscribeOn(Schedulers.newThread())
        .observeOn(AndroidSchedulers.mainThread())
        .subscribe(/* an Observer */);
This will execute the Observable on a new thread, and emit results through onNext on the main thread.
Observing on arbitrary loopers
The previous sample is merely a specialization of a more general concept: binding asynchronous
communication to an Android message loop, or Looper. In order to observe an Observable on an arbitrary
Looper, create an associated Scheduler by calling AndroidSchedulers.from:
Looper backgroundLooper = // ...
Observable.just(""one"", ""two"", ""three"", ""four"", ""five"")
        .observeOn(AndroidSchedulers.from(backgroundLooper))
        .subscribe(/* an Observer */)
This will execute the Observable on a new thread and emit results through onNext on whatever thread is
running backgroundLooper.
Bugs and Feedback
For bugs, feature requests, and discussion please use GitHub Issues.
For general usage questions please use the mailing list or StackOverflow.
LICENSE
Copyright 2015 The RxAndroid authors

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
87,Java,"Ghidra Software Reverse Engineering Framework
Ghidra is a software reverse engineering (SRE) framework created and maintained by the National Security Agency Research Directorate. This framework includes a suite of full-featured, high-end software analysis tools that enable users to analyze compiled code on a variety of platforms including Windows, macOS, and Linux. Capabilities include disassembly, assembly, decompilation, graphing, and scripting, along with hundreds of other features. Ghidra supports a wide variety of processor instruction sets and executable formats and can be run in both user-interactive and automated modes. Users may also develop their own Ghidra plug-in components and/or scripts using Java or Python.
In support of NSA's Cybersecurity mission, Ghidra was built to solve scaling and teaming problems on complex SRE efforts, and to provide a customizable and extensible SRE research platform. NSA has applied Ghidra SRE capabilities to a variety of problems that involve analyzing malicious code and generating deep insights for SRE analysts who seek a better understanding of potential vulnerabilities in networks and systems.
To start developing extensions and scripts, try out the GhidraDev plugin for Eclipse, which is part of the distribution package.  The full release build can be downloaded from our project homepage.
This repository contains the source for the core framework, features, and extensions.
If you would like to contribute, please take a look at our contributor guide to see how you can participate in this open source project.
If you are a U.S. citizen interested in projects like this, to develop Ghidra, and
other cybersecurity tools, for NSA to help protect our nation and its allies,
consider applying for a career with us.
"
88,Java,"Arthas







Arthas is a Java Diagnostic tool open sourced by Alibaba.
Arthas allows developers to troubleshoot production issues for Java applications without modifying code or restarting servers.
中文说明/Chinese Documentation
Background
Often times, the production system network is inaccessible from the local development environment. If issues are encountered in production systems, it is impossible to use IDEs to debug the application remotely. More importantly, debugging in production environment is unacceptable, as it will suspend all the threads, resulting in the suspension of business services.
Developers could always try to reproduce the same issue on the test/staging environment. However, this is tricky as some issues cannot be reproduced easily on a different environment, or even disappear once restarted.
And if you're thinking of adding some logs to your code to help troubleshoot the issue, you will have to go through the following lifecycle; test, staging, and then to production. Time is money! This approach is inefficient! Besides, the issue may not be reproducible once the JVM is restarted, as described above.
Arthas was built to solve these issues. A developer can troubleshoot your production issues on-the-fly. No JVM restart, no additional code changes. Arthas works as an observer, which will never suspend your existing threads.
Key features

Check whether a class is loaded, or where the class is being loaded. (Useful for troubleshooting jar file conflicts)
Decompile a class to ensure the code is running as expected.
View classloader statistics, e.g. the number of classloaders, the number of classes loaded per classloader, the classloader hierarchy, possible classloader leaks, etc.
View the method invocation details, e.g. method parameter, return object, thrown exception, and etc.
Check the stack trace of specified method invocation. This is useful when a developers wants to know the caller of the said method.
Trace the method invocation to find slow sub-invocations.
Monitor method invocation statistics, e.g. qps, rt, success rate and etc.
Monitor system metrics, thread states and cpu usage, gc statistics, and etc.
Supports command line interactive mode, with auto-complete feature enabled.
Supports telnet and websocket, which enables both local and remote diagnostics with command line and browsers.
Supports profiler/Flame Graph
Supports JDK 6+.
Supports Linux/Mac/Windows.

Online Tutorials(Recommend)

Arthas Basics
Arthas Advanced

Quick start
Use arthas-boot(Recommend)
Downloadarthas-boot.jar，Start with java command:
curl -O https://alibaba.github.io/arthas/arthas-boot.jar
java -jar arthas-boot.jar
Print usage:
java -jar arthas-boot.jar -h
Use as.sh
You can install Arthas with one single line command on Linux, Unix, and Mac. Copy the following command and paste it into the command line, then press Enter to run:
curl -L https://alibaba.github.io/arthas/install.sh | sh
The command above will download the bootstrap script as.sh to the current directory. You can move it the any other place you want, or put its location in $PATH.
You can enter its interactive interface by executing as.sh, or execute as.sh -h for more help information.
Documentation

Online Tutorials(Recommend)
User manual
Installation
Download
Quick start
Advanced usage
Commands
WebConsole
Docker
User cases
Questions and answers
Compile and debug/How to contribute
Release Notes

Feature Showcase
Dashboard

https://alibaba.github.io/arthas/en/dashboard


Thread

https://alibaba.github.io/arthas/en/thread

See what is eating your cpu (ranked by top cpu usage) and what is going on there in one glance:
$ thread -n 3
""as-command-execute-daemon"" Id=29 cpuUsage=75% RUNNABLE
    at sun.management.ThreadImpl.dumpThreads0(Native Method)
    at sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:440)
    at com.taobao.arthas.core.command.monitor200.ThreadCommand$1.action(ThreadCommand.java:58)
    at com.taobao.arthas.core.command.handler.AbstractCommandHandler.execute(AbstractCommandHandler.java:238)
    at com.taobao.arthas.core.command.handler.DefaultCommandHandler.handleCommand(DefaultCommandHandler.java:67)
    at com.taobao.arthas.core.server.ArthasServer$4.run(ArthasServer.java:276)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@6cd0b6f8

""as-session-expire-daemon"" Id=25 cpuUsage=24% TIMED_WAITING
    at java.lang.Thread.sleep(Native Method)
    at com.taobao.arthas.core.server.DefaultSessionManager$2.run(DefaultSessionManager.java:85)

""Reference Handler"" Id=2 cpuUsage=0% WAITING on java.lang.ref.Reference$Lock@69ba0f27
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.Reference$Lock@69ba0f27
    at java.lang.Object.wait(Object.java:503)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
jad

https://alibaba.github.io/arthas/en/jad

Decompile your class with one shot:
$ jad javax.servlet.Servlet

ClassLoader:
+-java.net.URLClassLoader@6108b2d7
  +-sun.misc.Launcher$AppClassLoader@18b4aac2
    +-sun.misc.Launcher$ExtClassLoader@1ddf84b8

Location:
/Users/xxx/work/test/lib/servlet-api.jar

/*
 * Decompiled with CFR 0_122.
 */
package javax.servlet;

import java.io.IOException;
import javax.servlet.ServletConfig;
import javax.servlet.ServletException;
import javax.servlet.ServletRequest;
import javax.servlet.ServletResponse;

public interface Servlet {
    public void init(ServletConfig var1) throws ServletException;

    public ServletConfig getServletConfig();

    public void service(ServletRequest var1, ServletResponse var2) throws ServletException, IOException;

    public String getServletInfo();

    public void destroy();
}
mc

https://alibaba.github.io/arthas/en/mc

Memory compiler, compiles .java files into .class files in memory.
mc /tmp/Test.java
redefine

https://alibaba.github.io/arthas/en/redefine

Load the external *.class files to re-define the loaded classes in JVM.
redefine /tmp/Test.class
redefine -c 327a647b /tmp/Test.class /tmp/Test\$Inner.class
sc

https://alibaba.github.io/arthas/en/sc

Search any loaded class with detailed information.
$ sc -d org.springframework.web.context.support.XmlWebApplicationContext
 class-info        org.springframework.web.context.support.XmlWebApplicationContext
 code-source       /Users/xxx/work/test/WEB-INF/lib/spring-web-3.2.11.RELEASE.jar
 name              org.springframework.web.context.support.XmlWebApplicationContext
 isInterface       false
 isAnnotation      false
 isEnum            false
 isAnonymousClass  false
 isArray           false
 isLocalClass      false
 isMemberClass     false
 isPrimitive       false
 isSynthetic       false
 simple-name       XmlWebApplicationContext
 modifier          public
 annotation
 interfaces
 super-class       +-org.springframework.web.context.support.AbstractRefreshableWebApplicationContext
                     +-org.springframework.context.support.AbstractRefreshableConfigApplicationContext
                       +-org.springframework.context.support.AbstractRefreshableApplicationContext
                         +-org.springframework.context.support.AbstractApplicationContext
                           +-org.springframework.core.io.DefaultResourceLoader
                             +-java.lang.Object
 class-loader      +-org.apache.catalina.loader.ParallelWebappClassLoader
                     +-java.net.URLClassLoader@6108b2d7
                       +-sun.misc.Launcher$AppClassLoader@18b4aac2
                         +-sun.misc.Launcher$ExtClassLoader@1ddf84b8
 classLoaderHash   25131501

stack

https://alibaba.github.io/arthas/en/stack

View the call stack of test.arthas.TestStack#doGet:
$ stack test.arthas.TestStack doGet
Press Ctrl+C to abort.
Affect(class-cnt:1 , method-cnt:1) cost in 286 ms.
ts=2018-09-18 10:11:45;thread_name=http-bio-8080-exec-10;id=d9;is_daemon=true;priority=5;TCCL=org.apache.catalina.loader.ParallelWebappClassLoader@25131501
    @test.arthas.TestStack.doGet()
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:624)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:731)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110)
        ...
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:451)
        at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1121)
        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)
        at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
        at java.lang.Thread.run(Thread.java:745)
Trace

https://alibaba.github.io/arthas/en/trace

See what is slowing down your method invocation with trace command:

Watch

https://alibaba.github.io/arthas/en/watch

Watch the first parameter and thrown exception of test.arthas.TestWatch#doGet only if it throws exception.
$ watch test.arthas.TestWatch doGet {params[0], throwExp} -e
Press Ctrl+C to abort.
Affect(class-cnt:1 , method-cnt:1) cost in 65 ms.
ts=2018-09-18 10:26:28;result=@ArrayList[
    @RequestFacade[org.apache.catalina.connector.RequestFacade@79f922b2],
    @NullPointerException[java.lang.NullPointerException],
]
Monitor

https://alibaba.github.io/arthas/en/monitor

Monitor a specific method invocation statistics, including total number of invocations, average response time, success rate, and every 5 seconds:
$ monitor -c 5 org.apache.dubbo.demo.provider.DemoServiceImpl sayHello
Press Ctrl+C to abort.
Affect(class-cnt:1 , method-cnt:1) cost in 109 ms.
 timestamp            class                                           method    total  success  fail  avg-rt(ms)  fail-rate
----------------------------------------------------------------------------------------------------------------------------
 2018-09-20 09:45:32  org.apache.dubbo.demo.provider.DemoServiceImpl  sayHello  5      5        0     0.67        0.00%

 timestamp            class                                           method    total  success  fail  avg-rt(ms)  fail-rate
----------------------------------------------------------------------------------------------------------------------------
 2018-09-20 09:45:37  org.apache.dubbo.demo.provider.DemoServiceImpl  sayHello  5      5        0     1.00        0.00%

 timestamp            class                                           method    total  success  fail  avg-rt(ms)  fail-rate
----------------------------------------------------------------------------------------------------------------------------
 2018-09-20 09:45:42  org.apache.dubbo.demo.provider.DemoServiceImpl  sayHello  5      5        0     0.43        0.00%
Time Tunnel(tt)

https://alibaba.github.io/arthas/en/tt

Record method invocation data, so that you can check the method invocation parameters, returned value, and thrown exceptions later. It works as if you could come back and replay the past method invocation via time tunnel.
$ tt -t org.apache.dubbo.demo.provider.DemoServiceImpl sayHello
Press Ctrl+C to abort.
Affect(class-cnt:1 , method-cnt:1) cost in 75 ms.
 INDEX   TIMESTAMP            COST(ms)  IS-RET  IS-EXP   OBJECT         CLASS                          METHOD
-------------------------------------------------------------------------------------------------------------------------------------
 1000    2018-09-20 09:54:10  1.971195  true    false    0x55965cca     DemoServiceImpl                sayHello
 1001    2018-09-20 09:54:11  0.215685  true    false    0x55965cca     DemoServiceImpl                sayHello
 1002    2018-09-20 09:54:12  0.236303  true    false    0x55965cca     DemoServiceImpl                sayHello
 1003    2018-09-20 09:54:13  0.159598  true    false    0x55965cca     DemoServiceImpl                sayHello
 1004    2018-09-20 09:54:14  0.201982  true    false    0x55965cca     DemoServiceImpl                sayHello
 1005    2018-09-20 09:54:15  0.214205  true    false    0x55965cca     DemoServiceImpl                sayHello
 1006    2018-09-20 09:54:16  0.241863  true    false    0x55965cca     DemoServiceImpl                sayHello
 1007    2018-09-20 09:54:17  0.305747  true    false    0x55965cca     DemoServiceImpl                sayHello
 1008    2018-09-20 09:54:18  0.18468   true    false    0x55965cca     DemoServiceImpl                sayHello
Classloader

https://alibaba.github.io/arthas/en/classloader

$ classloader
 name                                                  numberOfInstances  loadedCountTotal
 BootstrapClassLoader                                  1                  3346
 com.taobao.arthas.agent.ArthasClassloader             1                  1262
 java.net.URLClassLoader                               2                  1033
 org.apache.catalina.loader.ParallelWebappClassLoader  1                  628
 sun.reflect.DelegatingClassLoader                     166                166
 sun.misc.Launcher$AppClassLoader                      1                  31
 com.alibaba.fastjson.util.ASMClassLoader              6                  15
 sun.misc.Launcher$ExtClassLoader                      1                  7
 org.jvnet.hk2.internal.DelegatingClassLoader          2                  2
 sun.reflect.misc.MethodUtil                           1                  1
Web Console

https://alibaba.github.io/arthas/en/web-console


Profiler/FlameGraph

https://alibaba.github.io/arthas/en/profiler

$ profiler start
Started [cpu] profiling
$ profiler stop
profiler output file: /tmp/demo/arthas-output/20191125-135546.svg
OK

View profiler results under arthas-output via browser:

Known Users
Welcome to register the company name in this issue: https://github.com/alibaba/arthas/issues/111 (in order of registration)






























































































Derivative Projects

Bistoury: A project that integrates Arthas
A fork of arthas using MVEL

Credit
Contributors
This project exists thanks to all the people who contribute.

Projects

greys-anatomy: The Arthas code base has derived from Greys, we thank for the excellent work done by Greys.
termd: Arthas's terminal implementation is based on termd, an open source library for writing terminal applications in Java.
crash: Arthas's text based user interface rendering is based on codes extracted from here
cli: Arthas's command line interface implementation is based on cli, open sourced by vert.x
compiler Arthas's memory compiler.
Apache Commons Net Arthas's telnet client.
async-profiler Arthas's profielr command.

"
89,Java,"HanLP: Han Language Processing
汉语言处理包





HanLP是一系列模型与算法组成的NLP工具包，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。
HanLP提供下列功能：

中文分词

HMM-Bigram（速度与精度最佳平衡；一百兆内存）

最短路分词、N-最短路分词


由字构词（侧重精度，全世界最大语料库，可识别新词；适合NLP任务）

感知机分词、CRF分词


词典分词（侧重速度，每秒数千万字符；省内存）

极速词典分词


所有分词器都支持：

索引全切分模式
用户自定义词典
兼容繁体中文
训练用户自己的领域模型




词性标注

HMM词性标注（速度快）
感知机词性标注、CRF词性标注（精度高）


命名实体识别

基于HMM角色标注的命名实体识别 （速度快）

中国人名识别、音译人名识别、日本人名识别、地名识别、实体机构名识别


基于线性模型的命名实体识别（精度高）

感知机命名实体识别、CRF命名实体识别




关键词提取

TextRank关键词提取


自动摘要

TextRank自动摘要


短语提取

基于互信息和左右信息熵的短语提取


拼音转换

多音字、声母、韵母、声调


简繁转换

简繁分歧词（简体、繁体、臺灣正體、香港繁體）


文本推荐

语义推荐、拼音推荐、字词推荐


依存句法分析

基于神经网络的高性能依存句法分析器
基于ArcEager转移系统的柱搜索依存句法分析器


文本分类

情感分析


文本聚类

KMeans、Repeated Bisection、自动推断聚类数目k


word2vec

词向量训练、加载、词语相似度计算、语义运算、查询、KMeans聚类
文档语义相似度计算


语料库工具

部分默认模型训练自小型语料库，鼓励用户自行训练。所有模块提供训练接口，语料可参考98年人民日报语料库。



在提供丰富功能的同时，HanLP内部模块坚持低耦合、模型坚持惰性加载、服务坚持静态提供、词典坚持明文发布，使用非常方便。默认模型训练自全世界最大规模的中文语料库，同时自带一些语料处理工具，帮助用户训练自己的模型。

项目主页
《自然语言处理入门》🔥、随书代码、在线演示、Python调用、Solr及Lucene插件、论坛、论文引用、更多信息。

下载与配置
方式一、Maven
为了方便用户，特提供内置了数据包的Portable版，只需在pom.xml加入：
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.7.5</version>
</dependency>
零配置，即可使用基本功能（除由字构词、依存句法分析外的全部功能）。如果用户有自定义的需求，可以参考方式二，使用hanlp.properties进行配置（Portable版同样支持hanlp.properties）。
方式二、下载jar、data、hanlp.properties
HanLP将数据与程序分离，给予用户自定义的自由。
1、下载：data.zip
下载后解压到任意目录，接下来通过配置文件告诉HanLP数据包的位置。
HanLP中的数据分为词典和模型，其中词典是词法分析必需的，模型是句法分析必需的。
data
│
├─dictionary
└─model

用户可以自行增删替换，如果不需要句法分析等功能的话，随时可以删除model文件夹。

模型跟词典没有绝对的区别，隐马模型被做成人人都可以编辑的词典形式，不代表它不是模型。
GitHub代码库中已经包含了data.zip中的词典，直接编译运行自动缓存即可；模型则需要额外下载。

2、下载jar和配置文件：hanlp-release.zip
配置文件的作用是告诉HanLP数据包的位置，只需修改第一行
root=D:/JavaProjects/HanLP/

为data的父目录即可，比如data目录是/Users/hankcs/Documents/data，那么root=/Users/hankcs/Documents/ 。
最后将hanlp.properties放入classpath即可，对于多数项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。除了配置文件外，还可以使用环境变量HANLP_ROOT来设置root。安卓项目请参考demo。
如果放置不当，HanLP会提示当前环境下的合适路径，并且尝试从项目根目录读取数据集。
调用方法
HanLP几乎所有的功能都可以通过工具类HanLP快捷调用，当你想不起来调用方法时，只需键入HanLP.，IDE应当会给出提示，并展示HanLP完善的文档。
所有Demo都位于com.hankcs.demo下，比文档覆盖了更多细节，更新更及时，强烈建议运行一遍。此处仅列举部分常用接口。
1. 第一个Demo
System.out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！""));

内存要求

内存120MB以上（-Xms120m -Xmx120m -Xmn64m），标准数据包（35万核心词库+默认用户词典），分词测试正常。全部词典和模型都是惰性加载的，不使用的模型相当于不存在，可以自由删除。
HanLP对词典的数据结构进行了长期的优化，可以应对绝大多数场景。哪怕HanLP的词典上百兆也无需担心，因为在内存中被精心压缩过。如果内存非常有限，请使用小词典。HanLP默认使用大词典，同时提供小词典，请参考配置文件章节。


写给正在编译HanLP的开发者

如果你正在编译运行从Github检出的HanLP代码，并且没有下载data缓存，那么首次加载词典/模型会发生一个自动缓存的过程。
自动缓存的目的是为了加速词典载入速度，在下次载入时，缓存的词典文件会带来毫秒级的加载速度。由于词典体积很大，自动缓存会耗费一些时间，请耐心等待。
自动缓存缓存的不是明文词典，而是双数组Trie树、DAWG、AhoCorasickDoubleArrayTrie等数据结构。



2. 标准分词
List<Term> termList = StandardTokenizer.segment(""商品和服务"");
System.out.println(termList);

说明

HanLP中有一系列“开箱即用”的静态分词器，以Tokenizer结尾，在接下来的例子中会继续介绍。
HanLP.segment其实是对StandardTokenizer.segment的包装。
分词结果包含词性，每个词性的意思请查阅《HanLP词性标注集》。


算法详解

《词图的生成》



3. NLP分词
System.out.println(NLPTokenizer.segment(""我新造一个词叫幻想乡你能识别并标注正确词性吗？""));
// 注意观察下面两个“希望”的词性、两个“晚霞”的词性
System.out.println(NLPTokenizer.analyze(""我的希望是希望张晚霞的背影被晚霞映红"").translateLabels());
System.out.println(NLPTokenizer.analyze(""支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。""));

说明

NLP分词NLPTokenizer会执行词性标注和命名实体识别，由结构化感知机序列标注框架支撑。
默认模型训练自9970万字的大型综合语料库，是已知范围内全世界最大的中文分词语料库。语料库规模决定实际效果，面向生产环境的语料库应当在千万字量级。欢迎用户在自己的语料上训练新模型以适应新领域、识别新的命名实体。



4. 索引分词
List<Term> termList = IndexTokenizer.segment(""主副食品"");
for (Term term : termList)
{
    System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
}

说明

索引分词IndexTokenizer是面向搜索引擎的分词器，能够对长词全切分，另外通过term.offset可以获取单词在文本中的偏移量。
任何分词器都可以通过基类Segment的enableIndexMode方法激活索引模式。



5. N-最短路径分词
Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
Segment shortestSegment = new DijkstraSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
String[] testCase = new String[]{
        ""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。"",
        ""刘喜杰石国祥会见吴亚琴先进事迹报告团成员"",
        };
for (String sentence : testCase)
{
    System.out.println(""N-最短分词："" + nShortSegment.seg(sentence) + ""\n最短路分词："" + shortestSegment.seg(sentence));
}

说明

N最短路分词器NShortSegment比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强。
一般场景下最短路分词的精度已经足够，而且速度比N最短路分词器快几倍，请酌情选择。


算法详解

《N最短路径的Java实现与分词应用》



6. CRF分词
        CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
        String[] tests = new String[]{
            ""商品和服务"",
            ""上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观"",
            ""微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。"" // 支持繁体中文
        };
        for (String sentence : tests)
        {
            System.out.println(analyzer.analyze(sentence));
        }

说明

CRF对新词有很好的识别能力，但是开销较大。


算法详解

《CRF中文分词、词性标注与命名实体识别》



7. 极速词典分词
/**
 * 演示极速分词，基于AhoCorasickDoubleArrayTrie实现的词典分词，适用于“高吞吐量”“精度一般”的场合
 * @author hankcs
 */
public class DemoHighSpeedSegment
{
    public static void main(String[] args)
    {
        String text = ""江西鄱阳湖干枯，中国最大淡水湖变成大草原"";
        System.out.println(SpeedTokenizer.segment(text));
        long start = System.currentTimeMillis();
        int pressure = 1000000;
        for (int i = 0; i < pressure; ++i)
        {
            SpeedTokenizer.segment(text);
        }
        double costTime = (System.currentTimeMillis() - start) / (double)1000;
        System.out.printf(""分词速度：%.2f字每秒"", text.length() * pressure / costTime);
    }
}

说明

极速分词是词典最长分词，速度极其快，精度一般。
在i7-6700K上跑出了4500万字每秒的速度。


算法详解

《Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配》



8. 用户自定义词典
/**
 * 演示用户词典的动态增删
 *
 * @author hankcs
 */
public class DemoCustomDictionary
{
    public static void main(String[] args)
    {
        // 动态增加
        CustomDictionary.add(""攻城狮"");
        // 强行插入
        CustomDictionary.insert(""白富美"", ""nz 1024"");
        // 删除词语（注释掉试试）
//        CustomDictionary.remove(""攻城狮"");
        System.out.println(CustomDictionary.add(""单身狗"", ""nz 1024 n 1""));
        System.out.println(CustomDictionary.get(""单身狗""));

        String text = ""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰"";  // 怎么可能噗哈哈！

        // AhoCorasickDoubleArrayTrie自动机扫描文本中出现的自定义词语
        final char[] charArray = text.toCharArray();
        CustomDictionary.parseText(charArray, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
        {
            @Override
            public void hit(int begin, int end, CoreDictionary.Attribute value)
            {
                System.out.printf(""[%d:%d]=%s %s\n"", begin, end, new String(charArray, begin, end - begin), value);
            }
        });

        // 自定义词典在所有分词器中都有效
        System.out.println(HanLP.segment(text));
    }
}

说明

CustomDictionary是一份全局的用户自定义词典，可以随时增删，影响全部分词器。另外可以在任何分词器中关闭它。通过代码动态增删不会保存到词典文件。
中文分词≠词典，词典无法解决中文分词，Segment提供高低优先级应对不同场景，请参考FAQ。


追加词典

CustomDictionary主词典文本路径是data/dictionary/custom/CustomDictionary.txt，用户可以在此增加自己的词语（不推荐）；也可以单独新建一个文本文件，通过配置文件CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;来追加词典（推荐）。
始终建议将相同词性的词语放到同一个词典文件里，便于维护和分享。


词典格式

每一行代表一个单词，格式遵从[单词] [词性A] [A的频次] [词性B] [B的频次] ... 如果不填词性则表示采用词典的默认词性。
词典的默认词性默认是名词n，可以通过配置文件修改：全国地名大全.txt ns;如果词典路径后面空格紧接着词性，则该词典默认是该词性。
在统计分词中，并不保证自定义词典中的词一定被切分出来。用户可在理解后果的情况下通过Segment#enableCustomDictionaryForcing强制生效。
关于用户词典的更多信息请参考词典说明一章。


算法详解

《Trie树分词》
《Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配》



9. 中国人名识别
String[] testCase = new String[]{
        ""签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。"",
        ""王国强、高峰、汪洋、张朝阳光着头、韩寒、小四"",
        ""张浩和胡健康复员回家了"",
        ""王总和小丽结婚了"",
        ""编剧邵钧林和稽道青说"",
        ""这里有关天培的有关事迹"",
        ""龚学平等领导,邓颖超生前"",
        };
Segment segment = HanLP.newSegment().enableNameRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}

说明

目前分词器基本上都默认开启了中国人名识别，比如HanLP.segment()接口中使用的分词器等等，用户不必手动开启；上面的代码只是为了强调。
有一定的误命中率，比如误命中关键年，则可以通过在data/dictionary/person/nr.txt加入一条关键年 A 1来排除关键年作为人名的可能性，也可以将关键年作为新词登记到自定义词典中。
如果你通过上述办法解决了问题，欢迎向我提交pull request，词典也是宝贵的财富。
建议NLP用户使用感知机或CRF词法分析器，精度更高。


算法详解

《实战HMM-Viterbi角色标注中国人名识别》



10. 音译人名识别
String[] testCase = new String[]{
                ""一桶冰水当头倒下，微软的比尔盖茨、Facebook的扎克伯格跟桑德博格、亚马逊的贝索斯、苹果的库克全都不惜湿身入镜，这些硅谷的科技人，飞蛾扑火似地牺牲演出，其实全为了慈善。"",
                ""世界上最长的姓名是简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿。"",
        };
Segment segment = HanLP.newSegment().enableTranslatedNameRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}

说明

目前分词器基本上都默认开启了音译人名识别，用户不必手动开启；上面的代码只是为了强调。


算法详解

《层叠隐马模型下的音译人名和日本人名识别》



11. 日本人名识别
String[] testCase = new String[]{
        ""北川景子参演了林诣彬导演的《速度与激情3》"",
        ""林志玲亮相网友:确定不是波多野结衣？"",
};
Segment segment = HanLP.newSegment().enableJapaneseNameRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}

说明

目前标准分词器默认关闭了日本人名识别，用户需要手动开启；这是因为日本人名的出现频率较低，但是又消耗性能。


算法详解

《层叠隐马模型下的音译人名和日本人名识别》



12. 地名识别
String[] testCase = new String[]{
        ""武胜县新学乡政府大楼门前锣鼓喧天"",
        ""蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机"",
};
Segment segment = HanLP.newSegment().enablePlaceRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}

说明

目前标准分词器都默认关闭了地名识别，用户需要手动开启；这是因为消耗性能，其实多数地名都收录在核心词典和用户自定义词典中。
在生产环境中，能靠词典解决的问题就靠词典解决，这是最高效稳定的方法。
建议对命名实体识别要求较高的用户使用感知机词法分析器。


算法详解

《实战HMM-Viterbi角色标注地名识别》



13. 机构名识别
String[] testCase = new String[]{
    ""我在上海林原科技有限公司兼职工作，"",
    ""我经常在台川喜宴餐厅吃饭，"",
    ""偶尔去地中海影城看电影。"",
};
Segment segment = HanLP.newSegment().enableOrganizationRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}

说明

目前分词器默认关闭了机构名识别，用户需要手动开启；这是因为消耗性能，其实常用机构名都收录在核心词典和用户自定义词典中。
HanLP的目的不是演示动态识别，在生产环境中，能靠词典解决的问题就靠词典解决，这是最高效稳定的方法。
建议对命名实体识别要求较高的用户使用感知机词法分析器。


算法详解

《层叠HMM-Viterbi角色标注模型下的机构名识别》



14. 关键词提取
String content = ""程序员(英文Programmer)是从事程序开发、维护的专业人员。一般将程序员分为程序设计人员和程序编码人员，但两者的界限并不非常清楚，特别是在中国。软件从业人员分为初级程序员、高级程序员、系统分析员和项目经理四大类。"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);

说明

内部采用TextRankKeyword实现，用户可以直接调用TextRankKeyword.getKeywordList(document, size)


算法详解

《TextRank算法提取关键词的Java实现》



15. 自动摘要
String document = ""算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。\n"" +
        ""算法可以宽泛的分为三类，\n"" +
        ""一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。\n"" +
        ""二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。\n"" +
        ""三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。"";
List<String> sentenceList = HanLP.extractSummary(document, 3);
System.out.println(sentenceList);

说明

内部采用TextRankSentence实现，用户可以直接调用TextRankSentence.getTopSentenceList(document, size)。


算法详解

《TextRank算法自动摘要的Java实现》



16. 短语提取
String text = ""算法工程师\n"" +
                ""算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\n"" +
                ""\n"" +
                ""1职位简介\n"" +
                ""算法工程师是一个非常高端的职位；\n"" +
                ""专业要求：计算机、电子、通信、数学等相关专业；\n"" +
                ""学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\n"" +
                ""语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\n"" +
                ""必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\n"" +
                ""\n"" +
                ""2研究方向\n"" +
                ""视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\n"" +
                ""\n"" +
                ""3目前国内外状况\n"" +
                ""目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\n"" +
                ""在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\n"" +
                ""在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\n"" +
                ""另外数据挖掘、互联网搜索算法也成为当今的热门方向。\n"" +
                ""算法工程师逐渐往人工智能方向发展。"";
List<String> phraseList = HanLP.extractPhrase(text, 10);
System.out.println(phraseList);

说明

内部采用MutualInformationEntropyPhraseExtractor实现，用户可以直接调用MutualInformationEntropyPhraseExtractor.extractPhrase(text, size)。


算法详解

《基于互信息和左右信息熵的短语提取识别》



17. 拼音转换
/**
 * 汉字转拼音
 * @author hankcs
 */
public class DemoPinyin
{
    public static void main(String[] args)
    {
        String text = ""重载不是重任"";
        List<Pinyin> pinyinList = HanLP.convertToPinyinList(text);
        System.out.print(""原文,"");
        for (char c : text.toCharArray())
        {
            System.out.printf(""%c,"", c);
        }
        System.out.println();

        System.out.print(""拼音（数字音调）,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin);
        }
        System.out.println();

        System.out.print(""拼音（符号音调）,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getPinyinWithToneMark());
        }
        System.out.println();

        System.out.print(""拼音（无音调）,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getPinyinWithoutTone());
        }
        System.out.println();

        System.out.print(""声调,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getTone());
        }
        System.out.println();

        System.out.print(""声母,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getShengmu());
        }
        System.out.println();

        System.out.print(""韵母,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getYunmu());
        }
        System.out.println();

        System.out.print(""输入法头,"");
        for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getHead());
        }
        System.out.println();
    }
}

说明

HanLP不仅支持基础的汉字转拼音，还支持声母、韵母、音调、音标和输入法首字母首声母功能。
HanLP能够识别多音字，也能给繁体中文注拼音。
最重要的是，HanLP采用的模式匹配升级到AhoCorasickDoubleArrayTrie，性能大幅提升，能够提供毫秒级的响应速度！


算法详解

《汉字转拼音与简繁转换的Java实现》



18. 简繁转换
/**
 * 简繁转换
 * @author hankcs
 */
public class DemoTraditionalChinese2SimplifiedChinese
{
    public static void main(String[] args)
    {
        System.out.println(HanLP.convertToTraditionalChinese(""用笔记本电脑写程序""));
        System.out.println(HanLP.convertToSimplifiedChinese(""「以後等妳當上皇后，就能買士多啤梨慶祝了」""));
    }
}

说明

HanLP能够识别简繁分歧词，比如打印机=印表機。许多简繁转换工具不能区分“以后”“皇后”中的两个“后”字，HanLP可以。


算法详解

《汉字转拼音与简繁转换的Java实现》



19. 文本推荐
/**
 * 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)
 * @author hankcs
 */
public class DemoSuggester
{
    public static void main(String[] args)
    {
        Suggester suggester = new Suggester();
        String[] titleArray =
        (
                ""威廉王子发表演说 呼吁保护野生动物\n"" +
                ""《时代》年度人物最终入围名单出炉 普京马云入选\n"" +
                ""“黑格比”横扫菲：菲吸取“海燕”经验及早疏散\n"" +
                ""日本保密法将正式生效 日媒指其损害国民知情权\n"" +
                ""英报告说空气污染带来“公共健康危机”""
        ).split(""\\n"");
        for (String title : titleArray)
        {
            suggester.addSentence(title);
        }

        System.out.println(suggester.suggest(""发言"", 1));       // 语义
        System.out.println(suggester.suggest(""危机公共"", 1));   // 字符
        System.out.println(suggester.suggest(""mayun"", 1));      // 拼音
    }
}

说明

在搜索引擎的输入框中，用户输入一个词，搜索引擎会联想出最合适的搜索词，HanLP实现了类似的功能。
可以动态调节每种识别器的权重



20. 语义距离
/**
 * 演示词向量的训练与应用
 *
 * @author hankcs
 */
public class DemoWord2Vec
{
    public static void main(String[] args) throws IOException
    {
        WordVectorModel wordVectorModel = trainOrLoadModel();
        printNearest(""中国"", wordVectorModel);
        printNearest(""美丽"", wordVectorModel);
        printNearest(""购买"", wordVectorModel);

        // 文档向量
        DocVectorModel docVectorModel = new DocVectorModel(wordVectorModel);
        String[] documents = new String[]{
            ""山东苹果丰收"",
            ""农民在江苏种水稻"",
            ""奥运会女排夺冠"",
            ""世界锦标赛胜出"",
            ""中国足球失败"",
        };

        System.out.println(docVectorModel.similarity(documents[0], documents[1]));
        System.out.println(docVectorModel.similarity(documents[0], documents[4]));

        for (int i = 0; i < documents.length; i++)
        {
            docVectorModel.addDocument(i, documents[i]);
        }

        printNearestDocument(""体育"", documents, docVectorModel);
        printNearestDocument(""农业"", documents, docVectorModel);
        printNearestDocument(""我要看比赛"", documents, docVectorModel);
        printNearestDocument(""要不做饭吧"", documents, docVectorModel);
    }
}

说明

word2vec文档
《word2vec原理推导与代码分析》



21. 依存句法分析
/**
 * 依存句法分析（MaxEnt和神经网络句法模型需要-Xms1g -Xmx1g -Xmn512m）
 * @author hankcs
 */
public class DemoDependencyParser
{
    public static void main(String[] args)
    {
        CoNLLSentence sentence = HanLP.parseDependency(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"");
        System.out.println(sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence)
        {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
        // 也可以直接拿到数组，任意顺序或逆序遍历
        CoNLLWord[] wordArray = sentence.getWordArray();
        for (int i = wordArray.length - 1; i >= 0; i--)
        {
            CoNLLWord word = wordArray[i];
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
        // 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根
        CoNLLWord head = wordArray[12];
        while ((head = head.HEAD) != null)
        {
            if (head == CoNLLWord.ROOT) System.out.println(head.LEMMA);
            else System.out.printf(""%s --(%s)--> "", head.LEMMA, head.DEPREL);
        }
    }
}

说明

内部采用NeuralNetworkDependencyParser实现，用户可以直接调用NeuralNetworkDependencyParser.compute(sentence)
也可以调用基于ArcEager转移系统的柱搜索依存句法分析器KBeamArcEagerDependencyParser


算法详解

《基于神经网络分类模型与转移系统的判决式依存句法分析器》



词典说明
本章详细介绍HanLP中的词典格式，满足用户自定义的需要。HanLP中有许多词典，它们的格式都是相似的，形式都是文本文档，随时可以修改。
基本格式
词典分为词频词性词典和词频词典。

词频词性词典（如CoreNatureDictionary.txt）

每一行代表一个单词，格式遵从[单词] [词性A] [A的频次] [词性B] [B的频次] ...。
支持省略词性和频次，直接一行一个单词。
.txt词典文件的分隔符为空格或制表符，所以不支持含有空格的词语。如果需要支持空格，请使用英文逗号,分割的纯文本.csv文件。在使用Excel等富文本编辑器时，则请注意保存为纯文本形式。


词频词典（如CoreNatureDictionary.ngram.txt）

每一行代表一个单词或条目，格式遵从[单词] [单词的频次]。
每一行的分隔符为空格或制表符。



少数词典有自己的专用格式，比如同义词词典兼容《同义词词林扩展版》的文本格式，而转移矩阵词典则是一个csv表格。
下文主要介绍通用词典，如不注明，词典特指通用词典。
数据结构
Trie树（字典树）是HanLP中使用最多的数据结构，为此，我实现了通用的Trie树，支持泛型、遍历、储存、载入。
用户自定义词典采用AhoCorasickDoubleArrayTrie和二分Trie树储存，其他词典采用基于双数组Trie树(DoubleArrayTrie)实现的AC自动机AhoCorasickDoubleArrayTrie。关于一些常用数据结构的性能评估，请参考wiki。
储存形式
词典有两个形态：文本文件(filename.txt)和缓存文件(filename.txt.bin或filename.txt.trie.dat和filename.txt.trie.value)。

文本文件

采用明文储存，UTF-8编码，CRLF换行符。


缓存文件

就是一些二进制文件，通常在文本文件的文件名后面加上.bin表示。有时候是.trie.dat和.trie.value。后者是历史遗留产物，分别代表trie树的数组和值。
如果你修改了任何词典，只有删除缓存才能生效。



修改方法
HanLP的核心词典训练自人民日报2014语料，语料不是完美的，总会存在一些错误。这些错误可能会导致分词出现奇怪的结果，这时请打开调试模式排查问题：
HanLP.Config.enableDebug();

核心词性词频词典

比如你在data/dictionary/CoreNatureDictionary.txt中发现了一个不是词的词，或者词性标注得明显不对，那么你可以修改它，然后删除缓存文件使其生效。
目前CoreNatureDictionary.ngram.txt的缓存依赖于CoreNatureDictionary.txt的缓存，修改了后者之后必须同步删除前者的缓存，否则可能出错


核心二元文法词典

二元文法词典data/dictionary/CoreNatureDictionary.ngram.txt储存的是两个词的接续，如果你发现不可能存在这种接续时，删掉即可。
你也可以添加你认为合理的接续，但是这两个词必须同时在核心词典中才会生效。


命名实体识别词典

基于角色标注的命名实体识别比较依赖词典，所以词典的质量大幅影响识别质量。
这些词典的格式与原理都是类似的，请阅读相应的文章或代码修改它。



若还有疑问，请参考《自然语言处理入门》相应章节。如果问题解决了，欢迎向我提交一个pull request，这是我在代码库中保留明文词典的原因，众人拾柴火焰高！

《自然语言处理入门》

一本配套HanLP的NLP入门书，基础理论与生产代码并重，Python与Java双实现。从基本概念出发，逐步介绍中文分词、词性标注、命名实体识别、信息抽取、文本聚类、文本分类、句法分析这几个热门问题的算法原理与工程实现。书中通过对多种算法的讲解，比较了它们的优缺点和适用场景，同时详细演示生产级成熟代码，助你真正将自然语言处理应用在生产环境中。
《自然语言处理入门》由南方科技大学数学系创系主任夏志宏、微软亚洲研究院副院长周明、字节跳动人工智能实验室总监李航、华为诺亚方舟实验室语音语义首席科学家刘群、小米人工智能实验室主任兼NLP首席科学家王斌、中国科学院自动化研究所研究员宗成庆、清华大学副教授刘知远、北京理工大学副教授张华平和52nlp作序推荐。感谢各位前辈老师，希望这个项目和这本书能成为大家工程和学习上的“蝴蝶效应”，帮助大家在NLP之路上蜕变成蝶。
版权
Apache License Version 2.0

如不特殊注明，所有模块都以此协议授权使用。
任何使用了HanLP的全部或部分功能、词典、模型的项目、产品或文章等形式的成果必须显式注明HanLP及此项目主页。

青岛大快搜索计算技术股份有限公司

HanLP从v1.3版本起至v1.6正式由大快搜索主导开发，并拥有1.3-1.6版本的版权。这些版本继续完全开源，唯一官网为：http://hanlp.com/ 。

上海林原信息科技有限公司

HanLP 早期得到了上海林原公司的大力支持，并拥有1.2及前序版本的版权，相关版本也曾在上海林源公司网站发布。

其他版权方

实施上由个人维护，欢迎任何人与任何公司向本项目开源模块。
充分尊重所有版权方的贡献，本项目不占有用户贡献模块的版权。

鸣谢
感谢下列优秀开源项目：

darts-clone-java
SharpICTCLAS
snownlp
ansj_seg
nlp-lang

感谢NLP界各位学者老师的著作：

《基于角色标注的中国人名自动识别研究》张华平 刘群
《基于层叠隐马尔可夫模型的中文命名实体识别》俞鸿魁 张华平 刘群 吕学强 施水才
《基于角色标注的中文机构名识别》俞鸿魁 张华平 刘群
《基于最大熵的依存句法分析》 辛霄 范士喜 王轩 王晓龙
An Efficient Implementation of Trie Structures, JUN-ICHI AOE AND KATSUSHI MORIMOTO
TextRank: Bringing Order into Texts, Rada Mihalcea and Paul Tarau

感谢诸位用户的关注和使用，HanLP并不完善，未来还恳求各位NLP爱好者多多关照，提出宝贵意见。
作者 @hankcs
2016年9月16日
"
